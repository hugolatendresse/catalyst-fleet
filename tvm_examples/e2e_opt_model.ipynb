{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haFCuUiQYQc-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Taken directly from https://tvm.apache.org/docs/how_to/tutorials/e2e_opt_model.html\n",
        "Model Type: CNN (Resnet)\n",
        "Model Definition: PyTorch\n",
        "Model Export: torch.export\n",
        "Model Ingestion: tvm.relax.frontend.torch.from_exported_program\n",
        "Target: CUDA\n",
        "Compile and Test Result: FAIL:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/ssd1/htalendr/tvm/python:\n",
            "TVM successfully imported!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Add TVM path\n",
        "os.environ['PYTHONPATH'] = \"/ssd1/htalendr/tvm/python:\" + os.environ.get('PYTHONPATH', '')\n",
        "\n",
        "# Verify it's set\n",
        "print(os.environ['PYTHONPATH'])\n",
        "\n",
        "# Reload sys.path\n",
        "sys.path.append(\"/ssd1/htalendr/tvm/python\")\n",
        "\n",
        "# Test import\n",
        "import tvm\n",
        "from tvm import relax\n",
        "print(\"TVM successfully imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esFSRTkrYQdA"
      },
      "source": [
        "\n",
        "\n",
        "# End-to-End Optimize Model\n",
        "This tutorial demonstrates how to optimize a machine learning model using Apache TVM. We will\n",
        "use a pre-trained ResNet-18 model from PyTorch and end-to-end optimize it using TVM's Relax API.\n",
        "Please note that default end-to-end optimization may not suit complex models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7MaT_FJYQdB"
      },
      "source": [
        "## Preparation\n",
        "First, we prepare the model and input information. We use a pre-trained ResNet-18 model from\n",
        "PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XLjva_NMYQdB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.export import export\n",
        "from torchvision.models.resnet import ResNet18_Weights, resnet18\n",
        "\n",
        "torch_model = resnet18(weights=ResNet18_Weights.DEFAULT).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdlXOGo3YQdC"
      },
      "source": [
        "## Review Overall Flow\n",
        "The overall flow consists of the following steps:\n",
        "\n",
        "- **Construct or Import a Model**: Construct a neural network model or import a pre-trained\n",
        "  model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains\n",
        "  all the information needed for compilation, including high-level Relax functions for\n",
        "  computational graph, and low-level TensorIR functions for tensor program.\n",
        "- **Perform Composable Optimizations**: Perform a series of optimization transformations,\n",
        "  such as graph optimizations, tensor program optimizations, and library dispatching.\n",
        "- **Build and Universal Deployment**: Build the optimized model to a deployable module to the\n",
        "  universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEPviL7UYQdD"
      },
      "source": [
        "### Convert the model to IRModule\n",
        "Next step, we convert the model to an IRModule using the Relax frontend for PyTorch for further\n",
        "optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OoP5rHXgYQdD"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Unsupported function type batch_norm.default",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     exported_program \u001b[38;5;241m=\u001b[39m export(torch_model, example_args)\n\u001b[0;32m---> 11\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_exported_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexported_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_params_as_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m mod, params \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mfrontend\u001b[38;5;241m.\u001b[39mdetach_params(mod)\n\u001b[1;32m     14\u001b[0m mod\u001b[38;5;241m.\u001b[39mshow()\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/frontend/torch/exported_program_translator.py:454\u001b[0m, in \u001b[0;36mfrom_exported_program\u001b[0;34m(exported_program, keep_params_as_input, unwrap_unit_return_tuple, no_bind_return_tuple)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# decompose into Core ATen operators\u001b[39;00m\n\u001b[1;32m    452\u001b[0m exported_program\u001b[38;5;241m.\u001b[39mrun_decompositions()\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExportedProgramImporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_exported_program\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexported_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_params_as_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43munwrap_unit_return_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_bind_return_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/frontend/torch/exported_program_translator.py:350\u001b[0m, in \u001b[0;36mExportedProgramImporter.from_exported_program\u001b[0;34m(self, exported_program, keep_params_as_input, unwrap_unit_return_tuple, no_bind_return_tuple)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    349\u001b[0m     func_name \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    351\u001b[0m         func_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_map\n\u001b[1;32m    352\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported function type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv[node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_map[func_name](node)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mAssertionError\u001b[0m: Unsupported function type batch_norm.default"
          ]
        }
      ],
      "source": [
        "import tvm\n",
        "from tvm import relax\n",
        "from tvm.relax.frontend.torch import from_exported_program\n",
        "\n",
        "# Give an example argument to torch.export\n",
        "example_args = (torch.randn(1, 3, 224, 224, dtype=torch.float32),)\n",
        "\n",
        "# Convert the model to IRModule\n",
        "with torch.no_grad():\n",
        "    exported_program = export(torch_model, example_args)\n",
        "    mod = from_exported_program(exported_program, keep_params_as_input=True)\n",
        "\n",
        "mod, params = relax.frontend.detach_params(mod)\n",
        "mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkaCH3V_YQdE"
      },
      "source": [
        "## IRModule Optimization\n",
        "Apache TVM Unity provides a flexible way to optimize the IRModule. Everything centered\n",
        "around IRModule optimization can be composed with existing pipelines. Note that each\n",
        "transformation can be combined as an optimization pipeline via ``tvm.ir.transform.Sequential``.\n",
        "\n",
        "In this tutorial, we focus on the end-to-end optimization of the model via auto-tuning. We\n",
        "leverage MetaSchedule to tune the model and store the tuning logs to the database. We also\n",
        "apply the database to the model to get the best performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4rx1HsKYQdE"
      },
      "outputs": [],
      "source": [
        "TOTAL_TRIALS = 8000  # Change to 20000 for better performance if needed\n",
        "target = tvm.target.Target(\"nvidia/geforce-rtx-3090-ti\")  # Change to your target device\n",
        "work_dir = \"tuning_logs\"\n",
        "\n",
        "# Skip running in CI environment\n",
        "IS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\n",
        "if not IS_IN_CI:\n",
        "    mod = relax.get_pipeline(\"static_shape_tuning\", target=target, total_trials=TOTAL_TRIALS)(mod)\n",
        "\n",
        "    # Only show the main function\n",
        "    mod[\"main\"].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etxE3uDnYQdF"
      },
      "source": [
        "## Build and Deploy\n",
        "Finally, we build the optimized model and deploy it to the target device.\n",
        "We skip this step in the CI environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78C1aAv7YQdF"
      },
      "outputs": [],
      "source": [
        "if not IS_IN_CI:\n",
        "    ex = relax.build(mod, target=\"cuda\")\n",
        "    dev = tvm.device(\"cuda\", 0)\n",
        "    vm = relax.VirtualMachine(ex, dev)\n",
        "    # Need to allocate data and params on GPU device\n",
        "    gpu_data = tvm.nd.array(np.random.rand(1, 3, 224, 224).astype(\"float32\"), dev)\n",
        "    gpu_params = [tvm.nd.array(p, dev) for p in params[\"main\"]]\n",
        "    gpu_out = vm[\"main\"](gpu_data, *gpu_params).numpy()\n",
        "\n",
        "    print(gpu_out.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
