{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Taken directly from https://tvm.apache.org/docs/how_to/tutorials/e2e_opt_model.html\n",
        "Model Type: CNN\n",
        "Model Definition: PyTorch\n",
        "Model Export: torch.export\n",
        "Model Ingestion: tvm.relax.frontend.torch.from_exported_program\n",
        "Target: CUDA\n",
        "Result: FAIL\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/ssd1/htalendr/tvm/python:\n",
            "TVM successfully imported!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Add TVM path\n",
        "os.environ['PYTHONPATH'] = \"/ssd1/htalendr/tvm/python:\" + os.environ.get('PYTHONPATH', '')\n",
        "\n",
        "# Verify it's set\n",
        "print(os.environ['PYTHONPATH'])\n",
        "\n",
        "# Reload sys.path\n",
        "sys.path.append(\"/ssd1/htalendr/tvm/python\")\n",
        "\n",
        "# Test import\n",
        "import tvm\n",
        "from tvm import relax\n",
        "print(\"TVM successfully imported!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Feb  9 18:45:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
            "|  0%   34C    P8             34W /  450W |      31MiB /  24564MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  NVIDIA GeForce RTX 2070        Off |   00000000:4A:00.0 Off |                  N/A |\n",
            "|  0%   32C    P8             19W /  185W |       8MiB /   8192MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A   1110368      G   /usr/lib/xorg/Xorg                              8MiB |\n",
            "|    0   N/A  N/A   1110572      G   /usr/bin/gnome-shell                            8MiB |\n",
            "|    1   N/A  N/A   1110368      G   /usr/lib/xorg/Xorg                              4MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUeEKl5iKJmd"
      },
      "source": [
        "\n",
        "\n",
        "# End-to-End Optimize Model\n",
        "This tutorial demonstrates how to optimize a machine learning model using Apache TVM. We will\n",
        "use a pre-trained ResNet-18 model from PyTorch and end-to-end optimize it using TVM's Relax API.\n",
        "Please note that default end-to-end optimization may not suit complex models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qab2Suw_KJme"
      },
      "source": [
        "## Preparation\n",
        "First, we prepare the model and input information. We use a pre-trained ResNet-18 model from\n",
        "PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M6nuVDUgKJmf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.export import export\n",
        "# from torchvision.models.resnet import ResNet18_Weights, resnet18\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.export import export\n",
        "from tvm.relax.frontend.torch import from_exported_program\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy model\n",
        "class PyTorchCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(PyTorchCNN, self).__init__()\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        # self.drop = nn.Dropout2d(p=0.2) # TODO retrain without dropout?\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # # Ensure input is in the correct format (assumes already in NCHW if using PyTorch DataLoader)\n",
        "        # if not isinstance(x, torch.Tensor):\n",
        "        #     x = self.transformation(x).float() # Converts HWC -> CHW\n",
        "        #     x = x.unsqueeze(0)  # Converts CHW -> NCHW\n",
        "        #     x = Variable(x)\n",
        "\n",
        "        # Forward pass through CNN layers\n",
        "        x = F.relu(self.pool(self.conv1(x)))\n",
        "        x = F.relu(self.pool(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x)) # used to be: x = F.relu(self.drop(self.conv3(x)))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        \n",
        "        # Flatten the tensor before passing to the fully connected layer\n",
        "        x = x.view(x.size(0), -1)  # Use x.size(0) to handle batch size dynamically\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        # Return log probabilities for classification\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "torch_model = PyTorchCNN().eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXI6FJu5KJmf"
      },
      "source": [
        "## Review Overall Flow\n",
        "The overall flow consists of the following steps:\n",
        "\n",
        "- **Construct or Import a Model**: Construct a neural network model or import a pre-trained\n",
        "  model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains\n",
        "  all the information needed for compilation, including high-level Relax functions for\n",
        "  computational graph, and low-level TensorIR functions for tensor program.\n",
        "- **Perform Composable Optimizations**: Perform a series of optimization transformations,\n",
        "  such as graph optimizations, tensor program optimizations, and library dispatching.\n",
        "- **Build and Universal Deployment**: Build the optimized model to a deployable module to the\n",
        "  universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y1vG-mrKJmf"
      },
      "source": [
        "### Convert the model to IRModule\n",
        "Next step, we convert the model to an IRModule using the Relax frontend for PyTorch for further\n",
        "optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zglhE3y8KJmg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv3_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv3_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">24</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">24576</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
              "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(x, p_conv1_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(p_conv1_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
              "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv, lv1)\n",
              "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>max_pool2d(lv2, pool_size<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], ceil_mode<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, count_include_pad<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>)\n",
              "            lv4: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv3)\n",
              "            lv5: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(lv4, p_conv2_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv6: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(p_conv2_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
              "            lv7: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv5, lv6)\n",
              "            lv8: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>max_pool2d(lv7, pool_size<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], ceil_mode<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, count_include_pad<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>)\n",
              "            lv9: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv8)\n",
              "            lv10: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(lv9, p_conv3_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv11: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(p_conv3_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
              "            lv12: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv10, lv11)\n",
              "            lv13: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv12)\n",
              "            lv14: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24576</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(lv13, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24576</span>]))\n",
              "            lv15: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">24576</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(p_fc_weight, axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv16: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(lv14, lv15, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv17: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv16, p_fc_bias)\n",
              "            lv18: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>log_softmax(lv17, axis<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>)\n",
              "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">=</span> (lv18,)\n",
              "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tvm\n",
        "from tvm import relax\n",
        "from tvm.relax.frontend.torch import from_exported_program\n",
        "\n",
        "# Give an example argument to torch.export\n",
        "example_args = (torch.randn(1, 3, 128, 128, dtype=torch.float32),)\n",
        "\n",
        "# Convert the model to IRModule\n",
        "with torch.no_grad():\n",
        "    exported_program = export(torch_model, example_args)\n",
        "    mod = from_exported_program(exported_program, keep_params_as_input=True)\n",
        "\n",
        "mod, params = relax.frontend.detach_params(mod)\n",
        "mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbhXpm4gKJmg"
      },
      "source": [
        "## IRModule Optimization\n",
        "Apache TVM Unity provides a flexible way to optimize the IRModule. Everything centered\n",
        "around IRModule optimization can be composed with existing pipelines. Note that each\n",
        "transformation can be combined as an optimization pipeline via ``tvm.ir.transform.Sequential``.\n",
        "\n",
        "In this tutorial, we focus on the end-to-end optimization of the model via auto-tuning. We\n",
        "leverage MetaSchedule to tune the model and store the tuning logs to the database. We also\n",
        "apply the database to the model to get the best performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjaU_NK9KJmh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IS_IN_CI: False\n",
            "2025-02-09 18:45:00 [INFO] Logging directory: tuning_logs/logs\n",
            "2025-02-09 18:45:10 [INFO] LocalBuilder: max_workers = 32\n",
            "2025-02-09 18:45:10 [INFO] LocalRunner: max_workers = 1\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #0: \"reshape2\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #1: \"fused_conv2d2_add2_relu2\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #2: \"transpose\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #3: \"reshape1\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #4: \"fused_matmul_add3\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #5: \"fused_max_pool2d1_relu1\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #6: \"fused_conv2d1_add1\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[18:45:12] /ssd1/htalendr/tvm/src/meta_schedule/schedule_rule/apply_custom_rule.cc:56: Warning: Unknown schedule rule \"meta_schedule.pool_max\" for target keys \"[\"cuda\", \"gpu\"]\". Checked PackedFuncs:\n",
            "  meta_schedule.cuda.meta_schedule.pool_max\n",
            "  meta_schedule.gpu.meta_schedule.pool_max\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #7: \"log_softmax\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #8: \"fused_max_pool2d_relu\"\n",
            "2025-02-09 18:45:12 [INFO] [task_scheduler.cc:159] Initializing Task #9: \"fused_conv2d_add\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[18:45:12] /ssd1/htalendr/tvm/src/meta_schedule/schedule_rule/apply_custom_rule.cc:56: Warning: Unknown schedule rule \"meta_schedule.pool_max\" for target keys \"[\"cuda\", \"gpu\"]\". Checked PackedFuncs:\n",
            "  meta_schedule.cuda.meta_schedule.pool_max\n",
            "  meta_schedule.gpu.meta_schedule.pool_max\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-09 18:45:13 [INFO] [task_scheduler.cc:159] Initializing Task #10: \"reshape\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>FLOP</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Speed (GFLOPS)</th>\n",
              "      <th>Latency (us)</th>\n",
              "      <th>Weighted Latency (us)</th>\n",
              "      <th>Trials</th>\n",
              "      <th>Done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reshape2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fused_conv2d2_add2_relu2</td>\n",
              "      <td>5357568</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>transpose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>reshape1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fused_matmul_add3</td>\n",
              "      <td>147459</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>fused_max_pool2d1_relu1</td>\n",
              "      <td>61440</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>fused_conv2d1_add1</td>\n",
              "      <td>10665984</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>log_softmax</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fused_max_pool2d_relu</td>\n",
              "      <td>245760</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>fused_conv2d_add</td>\n",
              "      <td>10813440</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>reshape</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Name        FLOP    Weight    Speed (GFLOPS)   \\\n",
              "0                    reshape2           1         1               N/A    \n",
              "1    fused_conv2d2_add2_relu2     5357568         1               N/A    \n",
              "2                   transpose           1         1               N/A    \n",
              "3                    reshape1           1         1               N/A    \n",
              "4           fused_matmul_add3      147459         1               N/A    \n",
              "5     fused_max_pool2d1_relu1       61440         1               N/A    \n",
              "6          fused_conv2d1_add1    10665984         1               N/A    \n",
              "7                 log_softmax          15         1               N/A    \n",
              "8       fused_max_pool2d_relu      245760         1               N/A    \n",
              "9            fused_conv2d_add    10813440         1               N/A    \n",
              "10                    reshape           1         2               N/A    \n",
              "\n",
              "     Latency (us)    Weighted Latency (us)    Trials    Done   \n",
              "0             N/A                      N/A         0           \n",
              "1             N/A                      N/A         0           \n",
              "2             N/A                      N/A         0           \n",
              "3             N/A                      N/A         0           \n",
              "4             N/A                      N/A         0           \n",
              "5             N/A                      N/A         0           \n",
              "6             N/A                      N/A         0           \n",
              "7             N/A                      N/A         0           \n",
              "8             N/A                      N/A         0           \n",
              "9             N/A                      N/A         0           \n",
              "10            N/A                      N/A         0           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-09 18:45:13 [DEBUG] [task_scheduler.cc:318] \n",
            " ID |                     Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "  0 |                 reshape2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  1 | fused_conv2d2_add2_relu2 |  5357568 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  2 |                transpose |        1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  3 |                 reshape1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  4 |        fused_matmul_add3 |   147459 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  5 |  fused_max_pool2d1_relu1 |    61440 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  6 |       fused_conv2d1_add1 | 10665984 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  7 |              log_softmax |       15 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  8 |    fused_max_pool2d_relu |   245760 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  9 |         fused_conv2d_add | 10813440 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            " 10 |                  reshape |        1 |      2 |            N/A |          N/A |                   N/A |      0 |      \n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 0\n",
            "Total latency (us): 0\n",
            "\n",
            "\n",
            "Total trials: 0\n",
            "Total latency (us): 0\n",
            "\n",
            "2025-02-09 18:45:13 [INFO] [task_scheduler.cc:180] TaskScheduler picks Task #0: \"reshape2\"\n",
            "2025-02-09 18:45:13 [INFO] [task_scheduler.cc:193] Sending 6 sample(s) to builder\n",
            "2025-02-09 18:45:14 [INFO] [task_scheduler.cc:195] Sending 6 sample(s) to runner\n",
            "2025-02-09 18:45:14 [INFO] [task_scheduler.cc:180] TaskScheduler picks Task #1: \"fused_conv2d2_add2_relu2\"\n"
          ]
        }
      ],
      "source": [
        "TOTAL_TRIALS = 8  # Change to 20000 for better performance if needed\n",
        "target = tvm.target.Target(\"nvidia/geforce-rtx-4090\")  # Change to your target device\n",
        "work_dir = \"tuning_logs\"\n",
        "\n",
        "# Skip running in CI environment\n",
        "IS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\n",
        "print(\"IS_IN_CI:\", IS_IN_CI)\n",
        "if not IS_IN_CI:\n",
        "    mod = relax.get_pipeline(\"static_shape_tuning\", target=target, total_trials=TOTAL_TRIALS)(mod)\n",
        "\n",
        "    # Only show the main function\n",
        "    mod[\"main\"].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBEPef4hKJmh"
      },
      "source": [
        "## Build and Deploy\n",
        "Finally, we build the optimized model and deploy it to the target device.\n",
        "We skip this step in the CI environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FeR5l2EtKJmh"
      },
      "outputs": [
        {
          "ename": "TVMError",
          "evalue": "Traceback (most recent call last):\n  4: operator()\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:531\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:492\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:418\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:291\n  0: operator()\n        at /ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `p_fc_bias` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `lv16` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `T_add` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef add3(lv16: T.Buffer((T.int64(1), T.int64(3)), \"float32\"), p_fc_bias: T.Buffer((T.int64(3),), \"float32\"), T_add: T.Buffer((T.int64(1), T.int64(3)), \"float32\")):\n    T.func_attr({\"target\": T.target({\"arch\": \"sm_89\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"mtriple\": \"x86_64-conda-linux-gnu\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    for ax1 in range(3):\n        T_add_1 = T.Buffer((T.int64(3),), data=T_add.data)\n        lv16_1 = T.Buffer((T.int64(3),), data=lv16.data)\n        p_fc_bias_1 = T.Buffer((T.int64(3),), data=p_fc_bias.data)\n        T_add_1[ax1] = lv16_1[ax1] + p_fc_bias_1[ax1]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if not IS_IN_CI:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ex \u001b[38;5;241m=\u001b[39m \u001b[43mrelax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dev \u001b[38;5;241m=\u001b[39m tvm\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m vm \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mVirtualMachine(ex, dev)\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py:353\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(mod, target, params, pipeline, exec_mode, system_lib)\u001b[0m\n\u001b[1;32m    351\u001b[0m builder \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mExecBuilder()\n\u001b[1;32m    352\u001b[0m mod \u001b[38;5;241m=\u001b[39m _vmcodegen(builder, mod, exec_mode)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vmlink\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtir_mod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_tir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mext_libs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext_libs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_lib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py:249\u001b[0m, in \u001b[0;36m_vmlink\u001b[0;34m(builder, target, tir_mod, ext_libs, params, system_lib)\u001b[0m\n\u001b[1;32m    247\u001b[0m tir_ext_libs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tir_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tir_mod\u001b[38;5;241m.\u001b[39mget_global_vars()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mtvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtir_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_autodetect_system_lib_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_lib\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ext_mod \u001b[38;5;129;01min\u001b[39;00m ext_libs:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext_mod\u001b[38;5;241m.\u001b[39mis_device_module:\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/driver/build_module.py:297\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    293\u001b[0m     target_host \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tvm\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39menabled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstackvm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 297\u001b[0m rt_mod_host \u001b[38;5;241m=\u001b[39m \u001b[43m_driver_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtir_to_runtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_mods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_host, Target):\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/_ffi/_ctypes/packed_func.py:245\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    233\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    235\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    244\u001b[0m ):\n\u001b[0;32m--> 245\u001b[0m     \u001b[43mraise_last_ffi_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[1;32m    247\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/_ffi/base.py:481\u001b[0m, in \u001b[0;36mraise_last_ffi_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# The exception PyObject may contain a large amount of state,\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# including all stack frames that may be inspected in a later\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# PDB post-mortem.  Therefore, we must make sure to remove the\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# underlying PyObject* from the C++ side after we retrieve it.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m _LIB\u001b[38;5;241m.\u001b[39mTVMDropLastPythonError()\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m py_err\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:531\u001b[0m, in \u001b[0;36moperator()\u001b[0;34m()\u001b[0m\n\u001b[1;32m    529\u001b[0m TVM_REGISTER_GLOBAL(\"driver.tir_to_runtime\")\n\u001b[1;32m    530\u001b[0m     .set_body_typed([](const Map<Target, IRModule>& inputs_arg, Target host_target) {\n\u001b[0;32m--> 531\u001b[0m       return TIRToRuntime(inputs_arg, host_target);\n\u001b[1;32m    532\u001b[0m     });\n\u001b[1;32m    533\u001b[0m \n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:492\u001b[0m, in \u001b[0;36mtvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    490\u001b[0m const Target& target = it.first;\n\u001b[1;32m    491\u001b[0m const IRModule& ir_module = it.second;\n\u001b[0;32m--> 492\u001b[0m auto pair = SplitMixedModule(ir_module, target, target_host);\n\u001b[1;32m    493\u001b[0m auto& host_mod = pair.first;\n\u001b[1;32m    494\u001b[0m auto& device_mod = pair.second;\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:418\u001b[0m, in \u001b[0;36mtvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m ICHECK(mod_mixed.defined()) << \"This module must be defined\";\n\u001b[1;32m    417\u001b[0m \n\u001b[0;32m--> 418\u001b[0m mod_mixed = ApplyPasses(mod_mixed, MixedModulePassManager(mod_mixed, target));\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m IRModule host_mod = ApplyPasses(mod_mixed, HostModulePassManager(mod_mixed, target_host));\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:291\u001b[0m, in \u001b[0;36mtvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \n\u001b[1;32m    290\u001b[0m IRModule ApplyPasses(IRModule mod, transform::Sequential seq) {\n\u001b[0;32m--> 291\u001b[0m   mod = seq(std::move(mod));\n\u001b[1;32m    292\u001b[0m   return mod;\n\u001b[1;32m    293\u001b[0m }\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\u001b[0m, in \u001b[0;36moperator()\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m   s << \"    \" << err << \"\\n\";\n\u001b[1;32m    204\u001b[0m }\n\u001b[0;32m--> 205\u001b[0m LOG(FATAL) << \"RuntimeError: Memory verification failed with the following errors:\\n\"\n\u001b[1;32m    206\u001b[0m            << s.str() << \"  Did you forget to bind?\\n\"\n\u001b[1;32m    207\u001b[0m            << func;\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  4: operator()\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:531\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:492\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:418\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:291\n  0: operator()\n        at /ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `p_fc_bias` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `lv16` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `T_add` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef add3(lv16: T.Buffer((T.int64(1), T.int64(3)), \"float32\"), p_fc_bias: T.Buffer((T.int64(3),), \"float32\"), T_add: T.Buffer((T.int64(1), T.int64(3)), \"float32\")):\n    T.func_attr({\"target\": T.target({\"arch\": \"sm_89\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"mtriple\": \"x86_64-conda-linux-gnu\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    for ax1 in range(3):\n        T_add_1 = T.Buffer((T.int64(3),), data=T_add.data)\n        lv16_1 = T.Buffer((T.int64(3),), data=lv16.data)\n        p_fc_bias_1 = T.Buffer((T.int64(3),), data=p_fc_bias.data)\n        T_add_1[ax1] = lv16_1[ax1] + p_fc_bias_1[ax1]"
          ]
        }
      ],
      "source": [
        "# if not IS_IN_CI:\n",
        "ex = relax.build(mod, target=\"cuda\")\n",
        "dev = tvm.device(\"cuda\", 0)\n",
        "vm = relax.VirtualMachine(ex, dev)\n",
        "# Need to allocate data and params on GPU device\n",
        "gpu_data = tvm.nd.array(np.random.rand(1, 3, 224, 224).astype(\"float32\"), dev)\n",
        "gpu_params = [tvm.nd.array(p, dev) for p in params[\"main\"]]\n",
        "gpu_out = vm[\"main\"](gpu_data, *gpu_params).numpy()\n",
        "\n",
        "print(gpu_out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
