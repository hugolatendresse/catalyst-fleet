{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haFCuUiQYQc-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nTaken directly from https://tvm.apache.org/docs/how_to/tutorials/e2e_opt_model.html\\nModel Type: CNN\\nModel Definition: PyTorch\\nModel Export: torch.export\\nModel Ingestion: tvm.relax.frontend.torch.from_exported_program\\nTarget: CUDA\\nCompile and Test Result: FAIL:\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Taken directly from https://tvm.apache.org/docs/how_to/tutorials/e2e_opt_model.html\n",
        "Model Type: CNN\n",
        "Model Definition: PyTorch\n",
        "Model Export: torch.export\n",
        "Model Ingestion: tvm.relax.frontend.torch.from_exported_program\n",
        "Target: CUDA\n",
        "Compile and Test Result: FAIL: Did you forget to bind?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/ssd1/htalendr/tvm/python:\n",
            "TVM successfully imported!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Add TVM path\n",
        "os.environ['PYTHONPATH'] = \"/ssd1/htalendr/tvm/python:\" + os.environ.get('PYTHONPATH', '')\n",
        "\n",
        "# Verify it's set\n",
        "print(os.environ['PYTHONPATH'])\n",
        "\n",
        "# Reload sys.path\n",
        "sys.path.append(\"/ssd1/htalendr/tvm/python\")\n",
        "\n",
        "# Test import\n",
        "import tvm\n",
        "from tvm import relax\n",
        "print(\"TVM successfully imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esFSRTkrYQdA"
      },
      "source": [
        "\n",
        "\n",
        "# End-to-End Optimize Model\n",
        "This tutorial demonstrates how to optimize a machine learning model using Apache TVM. We will\n",
        "use a pre-trained ResNet-18 model from PyTorch and end-to-end optimize it using TVM's Relax API.\n",
        "Please note that default end-to-end optimization may not suit complex models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7MaT_FJYQdB"
      },
      "source": [
        "## Preparation\n",
        "First, we prepare the model and input information. We use a pre-trained ResNet-18 model from\n",
        "PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XLjva_NMYQdB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.export import export\n",
        "from torchvision.models.resnet import ResNet18_Weights, resnet18\n",
        "\n",
        "class TorchModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "torch_model = TorchModel().eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdlXOGo3YQdC"
      },
      "source": [
        "## Review Overall Flow\n",
        "The overall flow consists of the following steps:\n",
        "\n",
        "- **Construct or Import a Model**: Construct a neural network model or import a pre-trained\n",
        "  model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains\n",
        "  all the information needed for compilation, including high-level Relax functions for\n",
        "  computational graph, and low-level TensorIR functions for tensor program.\n",
        "- **Perform Composable Optimizations**: Perform a series of optimization transformations,\n",
        "  such as graph optimizations, tensor program optimizations, and library dispatching.\n",
        "- **Build and Universal Deployment**: Build the optimized model to a deployable module to the\n",
        "  universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEPviL7UYQdD"
      },
      "source": [
        "### Convert the model to IRModule\n",
        "Next step, we convert the model to an IRModule using the Relax frontend for PyTorch for further\n",
        "optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OoP5rHXgYQdD"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
              "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(p_fc1_weight, axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(x, lv, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv1, p_fc1_bias)\n",
              "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv2)\n",
              "            lv4: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(p_fc2_weight, axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv5: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(lv3, lv4, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv6: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv5, p_fc2_bias)\n",
              "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">=</span> (lv6,)\n",
              "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tvm\n",
        "from tvm import relax\n",
        "from tvm.relax.frontend.torch import from_exported_program\n",
        "\n",
        "# Give an example argument to torch.export\n",
        "example_args = (torch.randn(10, 784, dtype=torch.float32),)\n",
        "\n",
        "# Convert the model to IRModule\n",
        "with torch.no_grad():\n",
        "    exported_program = export(torch_model, example_args)\n",
        "    mod = from_exported_program(exported_program, keep_params_as_input=True)\n",
        "\n",
        "mod, params = relax.frontend.detach_params(mod)\n",
        "mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkaCH3V_YQdE"
      },
      "source": [
        "## IRModule Optimization\n",
        "Apache TVM Unity provides a flexible way to optimize the IRModule. Everything centered\n",
        "around IRModule optimization can be composed with existing pipelines. Note that each\n",
        "transformation can be combined as an optimization pipeline via ``tvm.ir.transform.Sequential``.\n",
        "\n",
        "In this tutorial, we focus on the end-to-end optimization of the model via auto-tuning. We\n",
        "leverage MetaSchedule to tune the model and store the tuning logs to the database. We also\n",
        "apply the database to the model to get the best performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D4rx1HsKYQdE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-10 10:10:21 [INFO] Logging directory: tuning_logs/logs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-10 10:10:35 [INFO] LocalBuilder: max_workers = 32\n",
            "2025-02-10 10:10:36 [INFO] LocalRunner: max_workers = 1\n",
            "2025-02-10 10:10:37 [INFO] [task_scheduler.cc:159] Initializing Task #0: \"fused_matmul1_add1\"\n",
            "2025-02-10 10:10:37 [INFO] [task_scheduler.cc:159] Initializing Task #1: \"transpose1\"\n",
            "2025-02-10 10:10:37 [INFO] [task_scheduler.cc:159] Initializing Task #2: \"fused_matmul_add_relu\"\n",
            "2025-02-10 10:10:37 [INFO] [task_scheduler.cc:159] Initializing Task #3: \"transpose\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>FLOP</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Speed (GFLOPS)</th>\n",
              "      <th>Latency (us)</th>\n",
              "      <th>Weighted Latency (us)</th>\n",
              "      <th>Trials</th>\n",
              "      <th>Done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fused_matmul1_add1</td>\n",
              "      <td>51300</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>transpose1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fused_matmul_add_relu</td>\n",
              "      <td>4019200</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transpose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Name       FLOP    Weight    Speed (GFLOPS)   \\\n",
              "0      fused_matmul1_add1      51300         1               N/A    \n",
              "1              transpose1          1         1               N/A    \n",
              "2   fused_matmul_add_relu    4019200         1               N/A    \n",
              "3               transpose          1         1               N/A    \n",
              "\n",
              "    Latency (us)    Weighted Latency (us)    Trials    Done   \n",
              "0            N/A                      N/A         0           \n",
              "1            N/A                      N/A         0           \n",
              "2            N/A                      N/A         0           \n",
              "3            N/A                      N/A         0           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-10 10:10:37 [DEBUG] [task_scheduler.cc:318] \n",
            " ID |                  Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "  0 |    fused_matmul1_add1 |   51300 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  1 |            transpose1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  2 | fused_matmul_add_relu | 4019200 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  3 |             transpose |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 0\n",
            "Total latency (us): 0\n",
            "\n",
            "\n",
            "Total trials: 0\n",
            "Total latency (us): 0\n",
            "\n",
            "2025-02-10 10:10:37 [INFO] [task_scheduler.cc:180] TaskScheduler picks Task #0: \"fused_matmul1_add1\"\n",
            "2025-02-10 10:10:46 [INFO] [task_scheduler.cc:193] Sending 2 sample(s) to builder\n",
            "2025-02-10 10:10:48 [INFO] [task_scheduler.cc:195] Sending 2 sample(s) to runner\n",
            "2025-02-10 10:10:48 [DEBUG] XGB iter   0: tr-p-rmse: 0.289988\ttr-a-peak@32: 1.000000\ttr-rmse: 0.675326\ttr-rmse: 0.675326\n",
            "2025-02-10 10:10:48 [DEBUG] XGB iter  25: tr-p-rmse: 0.002928\ttr-a-peak@32: 1.000000\ttr-rmse: 0.747816\ttr-rmse: 0.747816\n",
            "2025-02-10 10:10:48 [DEBUG] XGB iter  50: tr-p-rmse: 0.002928\ttr-a-peak@32: 1.000000\ttr-rmse: 0.747816\ttr-rmse: 0.747816\n",
            "2025-02-10 10:10:48 [DEBUG] XGB stopped. Best iteration: [7] tr-p-rmse:0.00293\ttr-a-peak@32:1.00000\ttr-rmse:0.74780\ttr-rmse:0.74780 \n",
            "2025-02-10 10:10:48 [INFO] [task_scheduler.cc:237] [Updated] Task #0: \"fused_matmul1_add1\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>FLOP</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Speed (GFLOPS)</th>\n",
              "      <th>Latency (us)</th>\n",
              "      <th>Weighted Latency (us)</th>\n",
              "      <th>Trials</th>\n",
              "      <th>Done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fused_matmul1_add1</td>\n",
              "      <td>51300</td>\n",
              "      <td>1</td>\n",
              "      <td>17.5788</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>transpose1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fused_matmul_add_relu</td>\n",
              "      <td>4019200</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transpose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Name       FLOP    Weight    Speed (GFLOPS)   \\\n",
              "0      fused_matmul1_add1      51300         1           17.5788    \n",
              "1              transpose1          1         1               N/A    \n",
              "2   fused_matmul_add_relu    4019200         1               N/A    \n",
              "3               transpose          1         1               N/A    \n",
              "\n",
              "    Latency (us)    Weighted Latency (us)    Trials    Done   \n",
              "0         2.9183                   2.9183         2           \n",
              "1            N/A                      N/A         0           \n",
              "2            N/A                      N/A         0           \n",
              "3            N/A                      N/A         0           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "2025-02-10 10:10:48 [DEBUG] [task_scheduler.cc:318] \n",
            " ID |                  Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "  0 |    fused_matmul1_add1 |   51300 |      1 |        17.5788 |       2.9183 |                2.9183 |      2 |      \n",
            "  1 |            transpose1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  2 | fused_matmul_add_relu | 4019200 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  3 |             transpose |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "2025-02-10 10:10:48 [INFO] [task_scheduler.cc:260] Task #0 has finished. Remaining task(s): 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>FLOP</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Speed (GFLOPS)</th>\n",
              "      <th>Latency (us)</th>\n",
              "      <th>Weighted Latency (us)</th>\n",
              "      <th>Trials</th>\n",
              "      <th>Done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fused_matmul1_add1</td>\n",
              "      <td>51300</td>\n",
              "      <td>1</td>\n",
              "      <td>17.5788</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>transpose1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fused_matmul_add_relu</td>\n",
              "      <td>4019200</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transpose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Name       FLOP    Weight    Speed (GFLOPS)   \\\n",
              "0      fused_matmul1_add1      51300         1           17.5788    \n",
              "1              transpose1          1         1               N/A    \n",
              "2   fused_matmul_add_relu    4019200         1               N/A    \n",
              "3               transpose          1         1               N/A    \n",
              "\n",
              "    Latency (us)    Weighted Latency (us)    Trials    Done   \n",
              "0         2.9183                   2.9183         2       Y   \n",
              "1            N/A                      N/A         0           \n",
              "2            N/A                      N/A         0           \n",
              "3            N/A                      N/A         0           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-10 10:10:49 [DEBUG] [task_scheduler.cc:318] \n",
            " ID |                  Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "  0 |    fused_matmul1_add1 |   51300 |      1 |        17.5788 |       2.9183 |                2.9183 |      2 |    Y \n",
            "  1 |            transpose1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  2 | fused_matmul_add_relu | 4019200 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  3 |             transpose |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "2025-02-10 10:10:49 [INFO] [task_scheduler.cc:260] Task #1 has finished. Remaining task(s): 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>FLOP</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Speed (GFLOPS)</th>\n",
              "      <th>Latency (us)</th>\n",
              "      <th>Weighted Latency (us)</th>\n",
              "      <th>Trials</th>\n",
              "      <th>Done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fused_matmul1_add1</td>\n",
              "      <td>51300</td>\n",
              "      <td>1</td>\n",
              "      <td>17.5788</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>transpose1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fused_matmul_add_relu</td>\n",
              "      <td>4019200</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transpose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Name       FLOP    Weight    Speed (GFLOPS)   \\\n",
              "0      fused_matmul1_add1      51300         1           17.5788    \n",
              "1              transpose1          1         1               N/A    \n",
              "2   fused_matmul_add_relu    4019200         1               N/A    \n",
              "3               transpose          1         1               N/A    \n",
              "\n",
              "    Latency (us)    Weighted Latency (us)    Trials    Done   \n",
              "0         2.9183                   2.9183         2       Y   \n",
              "1            N/A                      N/A         0       Y   \n",
              "2            N/A                      N/A         0           \n",
              "3            N/A                      N/A         0           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "2025-02-10 10:10:49 [DEBUG] [task_scheduler.cc:318] \n",
            " ID |                  Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "  0 |    fused_matmul1_add1 |   51300 |      1 |        17.5788 |       2.9183 |                2.9183 |      2 |    Y \n",
            "  1 |            transpose1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |    Y \n",
            "  2 | fused_matmul_add_relu | 4019200 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "  3 |             transpose |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "2025-02-10 10:10:49 [INFO] [task_scheduler.cc:260] Task #2 has finished. Remaining task(s): 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>FLOP</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Speed (GFLOPS)</th>\n",
              "      <th>Latency (us)</th>\n",
              "      <th>Weighted Latency (us)</th>\n",
              "      <th>Trials</th>\n",
              "      <th>Done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fused_matmul1_add1</td>\n",
              "      <td>51300</td>\n",
              "      <td>1</td>\n",
              "      <td>17.5788</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>transpose1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fused_matmul_add_relu</td>\n",
              "      <td>4019200</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transpose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Name       FLOP    Weight    Speed (GFLOPS)   \\\n",
              "0      fused_matmul1_add1      51300         1           17.5788    \n",
              "1              transpose1          1         1               N/A    \n",
              "2   fused_matmul_add_relu    4019200         1               N/A    \n",
              "3               transpose          1         1               N/A    \n",
              "\n",
              "    Latency (us)    Weighted Latency (us)    Trials    Done   \n",
              "0         2.9183                   2.9183         2       Y   \n",
              "1            N/A                      N/A         0       Y   \n",
              "2            N/A                      N/A         0       Y   \n",
              "3            N/A                      N/A         0           "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-02-10 10:10:49 [DEBUG] [task_scheduler.cc:318] \n",
            " ID |                  Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "  0 |    fused_matmul1_add1 |   51300 |      1 |        17.5788 |       2.9183 |                2.9183 |      2 |    Y \n",
            "  1 |            transpose1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |    Y \n",
            "  2 | fused_matmul_add_relu | 4019200 |      1 |            N/A |          N/A |                   N/A |      0 |    Y \n",
            "  3 |             transpose |       1 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "2025-02-10 10:10:49 [INFO] [task_scheduler.cc:260] Task #3 has finished. Remaining task(s): 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>FLOP</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Speed (GFLOPS)</th>\n",
              "      <th>Latency (us)</th>\n",
              "      <th>Weighted Latency (us)</th>\n",
              "      <th>Trials</th>\n",
              "      <th>Done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fused_matmul1_add1</td>\n",
              "      <td>51300</td>\n",
              "      <td>1</td>\n",
              "      <td>17.5788</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2.9183</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>transpose1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fused_matmul_add_relu</td>\n",
              "      <td>4019200</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>transpose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0</td>\n",
              "      <td>Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Name       FLOP    Weight    Speed (GFLOPS)   \\\n",
              "0      fused_matmul1_add1      51300         1           17.5788    \n",
              "1              transpose1          1         1               N/A    \n",
              "2   fused_matmul_add_relu    4019200         1               N/A    \n",
              "3               transpose          1         1               N/A    \n",
              "\n",
              "    Latency (us)    Weighted Latency (us)    Trials    Done   \n",
              "0         2.9183                   2.9183         2       Y   \n",
              "1            N/A                      N/A         0       Y   \n",
              "2            N/A                      N/A         0       Y   \n",
              "3            N/A                      N/A         0       Y   "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n",
            "2025-02-10 10:10:49 [DEBUG] [task_scheduler.cc:318] \n",
            " ID |                  Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "  0 |    fused_matmul1_add1 |   51300 |      1 |        17.5788 |       2.9183 |                2.9183 |      2 |    Y \n",
            "  1 |            transpose1 |       1 |      1 |            N/A |          N/A |                   N/A |      0 |    Y \n",
            "  2 | fused_matmul_add_relu | 4019200 |      1 |            N/A |          N/A |                   N/A |      0 |    Y \n",
            "  3 |             transpose |       1 |      1 |            N/A |          N/A |                   N/A |      0 |    Y \n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 2\n",
            "Total latency (us): 2.91828\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[10:10:49] /ssd1/htalendr/tvm/src/relax/transform/meta_schedule.cc:119: Warning: Creating JSONDatabase. Workload at: tuning_logs/database_workload.json, Tuning records at: tuning_logs/database_tuning_record.json\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
              "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "    R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
              "        lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(transpose, (p_fc1_weight,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "        lv_1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(fused_matmul_add_relu, (x, lv, p_fc1_bias), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">256</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "        lv4 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(transpose1, (p_fc2_weight,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">256</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "        lv1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(fused_matmul1_add1, (lv_1, lv4, p_fc2_bias), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "        gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">=</span> (lv1,)\n",
              "        R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
              "    <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "TOTAL_TRIALS = 2  # Change to 20000 for better performance if needed\n",
        "target = tvm.target.Target(\"nvidia/geforce-rtx-3090-ti\")  # Change to your target device\n",
        "work_dir = \"tuning_logs\"\n",
        "\n",
        "# Skip running in CI environment\n",
        "IS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\n",
        "if not IS_IN_CI:\n",
        "    mod = relax.get_pipeline(\"static_shape_tuning\", target=target, total_trials=TOTAL_TRIALS)(mod)\n",
        "\n",
        "    # Only show the main function\n",
        "    mod[\"main\"].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etxE3uDnYQdF"
      },
      "source": [
        "## Build and Deploy\n",
        "Finally, we build the optimized model and deploy it to the target device.\n",
        "We skip this step in the CI environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "78C1aAv7YQdF"
      },
      "outputs": [
        {
          "ename": "TVMError",
          "evalue": "Traceback (most recent call last):\n  4: operator()\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:531\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:492\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:418\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:291\n  0: operator()\n        at /ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `p_fc2_weight` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `T_transpose` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef transpose1(p_fc2_weight: T.Buffer((T.int64(10), T.int64(256)), \"float32\"), T_transpose: T.Buffer((T.int64(256), T.int64(10)), \"float32\")):\n    T.func_attr({\"op_pattern\": 2, \"target\": T.target({\"arch\": \"sm_89\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"mtriple\": \"x86_64-conda-linux-gnu\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    for ax0, ax1 in T.grid(256, 10):\n        T_transpose_1 = T.Buffer((T.int64(2560),), data=T_transpose.data)\n        p_fc2_weight_1 = T.Buffer((T.int64(2560),), data=p_fc2_weight.data)\n        T_transpose_1[ax0 * 10 + ax1] = p_fc2_weight_1[ax1 * 256 + ax0]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IS_IN_CI:\n\u001b[0;32m----> 2\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[43mrelax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     dev \u001b[38;5;241m=\u001b[39m tvm\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m     vm \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mVirtualMachine(ex, dev)\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py:353\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(mod, target, params, pipeline, exec_mode, system_lib)\u001b[0m\n\u001b[1;32m    351\u001b[0m builder \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mExecBuilder()\n\u001b[1;32m    352\u001b[0m mod \u001b[38;5;241m=\u001b[39m _vmcodegen(builder, mod, exec_mode)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vmlink\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtir_mod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_tir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mext_libs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext_libs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_lib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py:249\u001b[0m, in \u001b[0;36m_vmlink\u001b[0;34m(builder, target, tir_mod, ext_libs, params, system_lib)\u001b[0m\n\u001b[1;32m    247\u001b[0m tir_ext_libs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tir_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tir_mod\u001b[38;5;241m.\u001b[39mget_global_vars()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mtvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtir_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_autodetect_system_lib_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_lib\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ext_mod \u001b[38;5;129;01min\u001b[39;00m ext_libs:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext_mod\u001b[38;5;241m.\u001b[39mis_device_module:\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/driver/build_module.py:297\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    293\u001b[0m     target_host \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tvm\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39menabled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstackvm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 297\u001b[0m rt_mod_host \u001b[38;5;241m=\u001b[39m \u001b[43m_driver_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtir_to_runtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_mods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_host, Target):\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/_ffi/_ctypes/packed_func.py:245\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    233\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    235\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    244\u001b[0m ):\n\u001b[0;32m--> 245\u001b[0m     \u001b[43mraise_last_ffi_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[1;32m    247\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/_ffi/base.py:481\u001b[0m, in \u001b[0;36mraise_last_ffi_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# The exception PyObject may contain a large amount of state,\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# including all stack frames that may be inspected in a later\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# PDB post-mortem.  Therefore, we must make sure to remove the\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# underlying PyObject* from the C++ side after we retrieve it.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m _LIB\u001b[38;5;241m.\u001b[39mTVMDropLastPythonError()\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m py_err\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:531\u001b[0m, in \u001b[0;36moperator()\u001b[0;34m()\u001b[0m\n\u001b[1;32m    529\u001b[0m TVM_REGISTER_GLOBAL(\"driver.tir_to_runtime\")\n\u001b[1;32m    530\u001b[0m     .set_body_typed([](const Map<Target, IRModule>& inputs_arg, Target host_target) {\n\u001b[0;32m--> 531\u001b[0m       return TIRToRuntime(inputs_arg, host_target);\n\u001b[1;32m    532\u001b[0m     });\n\u001b[1;32m    533\u001b[0m \n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:492\u001b[0m, in \u001b[0;36mtvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    490\u001b[0m const Target& target = it.first;\n\u001b[1;32m    491\u001b[0m const IRModule& ir_module = it.second;\n\u001b[0;32m--> 492\u001b[0m auto pair = SplitMixedModule(ir_module, target, target_host);\n\u001b[1;32m    493\u001b[0m auto& host_mod = pair.first;\n\u001b[1;32m    494\u001b[0m auto& device_mod = pair.second;\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:418\u001b[0m, in \u001b[0;36mtvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m ICHECK(mod_mixed.defined()) << \"This module must be defined\";\n\u001b[1;32m    417\u001b[0m \n\u001b[0;32m--> 418\u001b[0m mod_mixed = ApplyPasses(mod_mixed, MixedModulePassManager(mod_mixed, target));\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m IRModule host_mod = ApplyPasses(mod_mixed, HostModulePassManager(mod_mixed, target_host));\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:291\u001b[0m, in \u001b[0;36mtvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \n\u001b[1;32m    290\u001b[0m IRModule ApplyPasses(IRModule mod, transform::Sequential seq) {\n\u001b[0;32m--> 291\u001b[0m   mod = seq(std::move(mod));\n\u001b[1;32m    292\u001b[0m   return mod;\n\u001b[1;32m    293\u001b[0m }\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\u001b[0m, in \u001b[0;36moperator()\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m   s << \"    \" << err << \"\\n\";\n\u001b[1;32m    204\u001b[0m }\n\u001b[0;32m--> 205\u001b[0m LOG(FATAL) << \"RuntimeError: Memory verification failed with the following errors:\\n\"\n\u001b[1;32m    206\u001b[0m            << s.str() << \"  Did you forget to bind?\\n\"\n\u001b[1;32m    207\u001b[0m            << func;\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  4: operator()\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:531\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:492\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:418\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:291\n  0: operator()\n        at /ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `p_fc2_weight` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `T_transpose` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef transpose1(p_fc2_weight: T.Buffer((T.int64(10), T.int64(256)), \"float32\"), T_transpose: T.Buffer((T.int64(256), T.int64(10)), \"float32\")):\n    T.func_attr({\"op_pattern\": 2, \"target\": T.target({\"arch\": \"sm_89\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"mtriple\": \"x86_64-conda-linux-gnu\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    for ax0, ax1 in T.grid(256, 10):\n        T_transpose_1 = T.Buffer((T.int64(2560),), data=T_transpose.data)\n        p_fc2_weight_1 = T.Buffer((T.int64(2560),), data=p_fc2_weight.data)\n        T_transpose_1[ax0 * 10 + ax1] = p_fc2_weight_1[ax1 * 256 + ax0]"
          ]
        }
      ],
      "source": [
        "if not IS_IN_CI:\n",
        "    ex = relax.build(mod, target=\"cuda\")\n",
        "    dev = tvm.device(\"cuda\", 0)\n",
        "    vm = relax.VirtualMachine(ex, dev)\n",
        "    # Need to allocate data and params on GPU device\n",
        "    gpu_data = tvm.nd.array(np.random.rand(1, 3, 224, 224).astype(\"float32\"), dev)\n",
        "    gpu_params = [tvm.nd.array(p, dev) for p in params[\"main\"]]\n",
        "    gpu_out = vm[\"main\"](gpu_data, *gpu_params).numpy()\n",
        "\n",
        "    print(gpu_out.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
