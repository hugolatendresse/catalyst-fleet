{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nTaken directly from https://tvm.apache.org/docs/how_to/tutorials/e2e_opt_model.html\\nModel Type: CNN\\nModel Definition: PyTorch\\nModel Export: torch.export\\nModel Ingestion: tvm.relax.frontend.torch.from_exported_program\\nTarget: CUDA\\nCompile and Test Result: FAIL: Did you forget to bind?\\n    Variable `p_conv3_weight` is directly accessed by host memory\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Taken directly from https://tvm.apache.org/docs/how_to/tutorials/e2e_opt_model.html\n",
        "Model Type: CNN\n",
        "Model Definition: PyTorch\n",
        "Model Export: torch.export\n",
        "Model Ingestion: tvm.relax.frontend.torch.from_exported_program\n",
        "Target: CUDA\n",
        "Compile and Test Result: FAIL: Did you forget to bind?\n",
        "    Variable `p_conv3_weight` is directly accessed by host memory\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/ssd1/htalendr/tvm/python:\n",
            "TVM successfully imported!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Add TVM path\n",
        "os.environ['PYTHONPATH'] = \"/ssd1/htalendr/tvm/python:\" + os.environ.get('PYTHONPATH', '')\n",
        "\n",
        "# Verify it's set\n",
        "print(os.environ['PYTHONPATH'])\n",
        "\n",
        "# Reload sys.path\n",
        "sys.path.append(\"/ssd1/htalendr/tvm/python\")\n",
        "\n",
        "# Test import\n",
        "import tvm\n",
        "from tvm import relax\n",
        "print(\"TVM successfully imported!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Feb 10 10:18:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
            "|  0%   35C    P8             36W /  450W |     419MiB /  24564MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  NVIDIA GeForce RTX 2070        Off |   00000000:4A:00.0 Off |                  N/A |\n",
            "|  0%   32C    P8             20W /  185W |       8MiB /   8192MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A    107921      C   ...ndr/miniconda3/envs/env1/bin/python        384MiB |\n",
            "|    0   N/A  N/A   1110368      G   /usr/lib/xorg/Xorg                              8MiB |\n",
            "|    0   N/A  N/A   1110572      G   /usr/bin/gnome-shell                            8MiB |\n",
            "|    1   N/A  N/A   1110368      G   /usr/lib/xorg/Xorg                              4MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUeEKl5iKJmd"
      },
      "source": [
        "\n",
        "\n",
        "# End-to-End Optimize Model\n",
        "This tutorial demonstrates how to optimize a machine learning model using Apache TVM. We will\n",
        "use a pre-trained ResNet-18 model from PyTorch and end-to-end optimize it using TVM's Relax API.\n",
        "Please note that default end-to-end optimization may not suit complex models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qab2Suw_KJme"
      },
      "source": [
        "## Preparation\n",
        "First, we prepare the model and input information. We use a pre-trained ResNet-18 model from\n",
        "PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M6nuVDUgKJmf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.export import export\n",
        "# from torchvision.models.resnet import ResNet18_Weights, resnet18\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.export import export\n",
        "from tvm.relax.frontend.torch import from_exported_program\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy model\n",
        "class PyTorchCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(PyTorchCNN, self).__init__()\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        # self.drop = nn.Dropout2d(p=0.2) # TODO retrain without dropout?\n",
        "        \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # # Ensure input is in the correct format (assumes already in NCHW if using PyTorch DataLoader)\n",
        "        # if not isinstance(x, torch.Tensor):\n",
        "        #     x = self.transformation(x).float() # Converts HWC -> CHW\n",
        "        #     x = x.unsqueeze(0)  # Converts CHW -> NCHW\n",
        "        #     x = Variable(x)\n",
        "\n",
        "        # Forward pass through CNN layers\n",
        "        x = F.relu(self.pool(self.conv1(x)))\n",
        "        x = F.relu(self.pool(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x)) # used to be: x = F.relu(self.drop(self.conv3(x)))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        \n",
        "        # Flatten the tensor before passing to the fully connected layer\n",
        "        x = x.view(x.size(0), -1)  # Use x.size(0) to handle batch size dynamically\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        # Return log probabilities for classification\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "torch_model = PyTorchCNN().eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXI6FJu5KJmf"
      },
      "source": [
        "## Review Overall Flow\n",
        "The overall flow consists of the following steps:\n",
        "\n",
        "- **Construct or Import a Model**: Construct a neural network model or import a pre-trained\n",
        "  model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains\n",
        "  all the information needed for compilation, including high-level Relax functions for\n",
        "  computational graph, and low-level TensorIR functions for tensor program.\n",
        "- **Perform Composable Optimizations**: Perform a series of optimization transformations,\n",
        "  such as graph optimizations, tensor program optimizations, and library dispatching.\n",
        "- **Build and Universal Deployment**: Build the optimized model to a deployable module to the\n",
        "  universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y1vG-mrKJmf"
      },
      "source": [
        "### Convert the model to IRModule\n",
        "Next step, we convert the model to an IRModule using the Relax frontend for PyTorch for further\n",
        "optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zglhE3y8KJmg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv1_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv1_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv2_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv2_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">12</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv3_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_conv3_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">24</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc_weight: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">24576</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), p_fc_bias: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
              "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(x, p_conv1_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(p_conv1_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
              "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv, lv1)\n",
              "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>max_pool2d(lv2, pool_size<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], ceil_mode<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, count_include_pad<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>)\n",
              "            lv4: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv3)\n",
              "            lv5: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(lv4, p_conv2_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv6: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(p_conv2_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
              "            lv7: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">64</span>, <span style=\"color: #008000\">64</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv5, lv6)\n",
              "            lv8: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>max_pool2d(lv7, pool_size<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], ceil_mode<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, count_include_pad<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">False</span>, layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>)\n",
              "            lv9: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">12</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv8)\n",
              "            lv10: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>conv2d(lv9, p_conv3_weight, strides<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], padding<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], dilation<span style=\"color: #A2F; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>], groups<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, kernel_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIHW&quot;</span>, out_layout<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCHW&quot;</span>, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv11: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(p_conv3_bias, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>]))\n",
              "            lv12: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv10, lv11)\n",
              "            lv13: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv12)\n",
              "            lv14: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24576</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>reshape(lv13, R<span style=\"color: #A2F; font-weight: bold\">.</span>shape([<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">24576</span>]))\n",
              "            lv15: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">24576</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(p_fc_weight, axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv16: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(lv14, lv15, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)\n",
              "            lv17: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv16, p_fc_bias)\n",
              "            lv18: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>log_softmax(lv17, axis<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>)\n",
              "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tuple(R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">=</span> (lv18,)\n",
              "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tvm\n",
        "from tvm import relax\n",
        "from tvm.relax.frontend.torch import from_exported_program\n",
        "\n",
        "# Give an example argument to torch.export\n",
        "example_args = (torch.randn(1, 3, 128, 128, dtype=torch.float32),)\n",
        "\n",
        "# Convert the model to IRModule\n",
        "with torch.no_grad():\n",
        "    exported_program = export(torch_model, example_args)\n",
        "    mod = from_exported_program(exported_program, keep_params_as_input=True)\n",
        "\n",
        "mod, params = relax.frontend.detach_params(mod)\n",
        "mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbhXpm4gKJmg"
      },
      "source": [
        "## IRModule Optimization\n",
        "Apache TVM Unity provides a flexible way to optimize the IRModule. Everything centered\n",
        "around IRModule optimization can be composed with existing pipelines. Note that each\n",
        "transformation can be combined as an optimization pipeline via ``tvm.ir.transform.Sequential``.\n",
        "\n",
        "In this tutorial, we focus on the end-to-end optimization of the model via auto-tuning. We\n",
        "leverage MetaSchedule to tune the model and store the tuning logs to the database. We also\n",
        "apply the database to the model to get the best performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WjaU_NK9KJmh"
      },
      "outputs": [],
      "source": [
        "# Skipping the optimization\n",
        "\n",
        "# TOTAL_TRIALS = 8  # Change to 20000 for better performance if needed\n",
        "# target = tvm.target.Target(\"nvidia/geforce-rtx-4090\")  # Change to your target device\n",
        "# work_dir = \"tuning_logs\"\n",
        "\n",
        "# # Skip running in CI environment\n",
        "# IS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\n",
        "# print(\"IS_IN_CI:\", IS_IN_CI)\n",
        "# if not IS_IN_CI:\n",
        "#     mod = relax.get_pipeline(\"static_shape_tuning\", target=target, total_trials=TOTAL_TRIALS)(mod)\n",
        "\n",
        "#     # Only show the main function\n",
        "#     mod[\"main\"].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBEPef4hKJmh"
      },
      "source": [
        "## Build and Deploy\n",
        "Finally, we build the optimized model and deploy it to the target device.\n",
        "We skip this step in the CI environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FeR5l2EtKJmh"
      },
      "outputs": [
        {
          "ename": "TVMError",
          "evalue": "Traceback (most recent call last):\n  4: operator()\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:531\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:492\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:418\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:291\n  0: operator()\n        at /ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `lv3` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `compute` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef relu(lv3: T.Buffer((T.int64(1), T.int64(12), T.int64(64), T.int64(64)), \"float32\"), compute: T.Buffer((T.int64(1), T.int64(12), T.int64(64), T.int64(64)), \"float32\")):\n    T.func_attr({\"target\": T.target({\"arch\": \"sm_89\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"mtriple\": \"x86_64-conda-linux-gnu\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    for i1, i2, i3 in T.grid(12, 64, 64):\n        cse_var_1: T.int32 = i1 * 4096 + i2 * 64 + i3\n        compute_1 = T.Buffer((T.int64(49152),), data=compute.data)\n        lv3_1 = T.Buffer((T.int64(49152),), data=lv3.data)\n        compute_1[cse_var_1] = T.max(lv3_1[cse_var_1], T.float32(0.0))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m      4\u001b[0m     gpu_mod \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mApplyDefaultSchedule(\n\u001b[1;32m      5\u001b[0m         dl\u001b[38;5;241m.\u001b[39mgpu\u001b[38;5;241m.\u001b[39mGEMV(),\n\u001b[1;32m      6\u001b[0m         dl\u001b[38;5;241m.\u001b[39mgpu\u001b[38;5;241m.\u001b[39mLowBatchGEMV(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         dl\u001b[38;5;241m.\u001b[39mgpu\u001b[38;5;241m.\u001b[39mRMSNorm(),\n\u001b[1;32m     13\u001b[0m     )(mod)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# if not IS_IN_CI:\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m ex \u001b[38;5;241m=\u001b[39m \u001b[43mrelax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m dev \u001b[38;5;241m=\u001b[39m tvm\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m vm \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mVirtualMachine(ex, dev)\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py:353\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(mod, target, params, pipeline, exec_mode, system_lib)\u001b[0m\n\u001b[1;32m    351\u001b[0m builder \u001b[38;5;241m=\u001b[39m relax\u001b[38;5;241m.\u001b[39mExecBuilder()\n\u001b[1;32m    352\u001b[0m mod \u001b[38;5;241m=\u001b[39m _vmcodegen(builder, mod, exec_mode)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vmlink\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtir_mod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_tir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mext_libs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext_libs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_lib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py:249\u001b[0m, in \u001b[0;36m_vmlink\u001b[0;34m(builder, target, tir_mod, ext_libs, params, system_lib)\u001b[0m\n\u001b[1;32m    247\u001b[0m tir_ext_libs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tir_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tir_mod\u001b[38;5;241m.\u001b[39mget_global_vars()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mtvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtir_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_autodetect_system_lib_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_lib\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ext_mod \u001b[38;5;129;01min\u001b[39;00m ext_libs:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext_mod\u001b[38;5;241m.\u001b[39mis_device_module:\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/driver/build_module.py:297\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    293\u001b[0m     target_host \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tvm\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39menabled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstackvm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 297\u001b[0m rt_mod_host \u001b[38;5;241m=\u001b[39m \u001b[43m_driver_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtir_to_runtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_mods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_host, Target):\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/_ffi/_ctypes/packed_func.py:245\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    233\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    235\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    244\u001b[0m ):\n\u001b[0;32m--> 245\u001b[0m     \u001b[43mraise_last_ffi_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[1;32m    247\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/python/tvm/_ffi/base.py:481\u001b[0m, in \u001b[0;36mraise_last_ffi_error\u001b[0;34m()\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# The exception PyObject may contain a large amount of state,\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# including all stack frames that may be inspected in a later\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# PDB post-mortem.  Therefore, we must make sure to remove the\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# underlying PyObject* from the C++ side after we retrieve it.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m _LIB\u001b[38;5;241m.\u001b[39mTVMDropLastPythonError()\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m py_err\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:531\u001b[0m, in \u001b[0;36moperator()\u001b[0;34m()\u001b[0m\n\u001b[1;32m    529\u001b[0m TVM_REGISTER_GLOBAL(\"driver.tir_to_runtime\")\n\u001b[1;32m    530\u001b[0m     .set_body_typed([](const Map<Target, IRModule>& inputs_arg, Target host_target) {\n\u001b[0;32m--> 531\u001b[0m       return TIRToRuntime(inputs_arg, host_target);\n\u001b[1;32m    532\u001b[0m     });\n\u001b[1;32m    533\u001b[0m \n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:492\u001b[0m, in \u001b[0;36mtvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    490\u001b[0m const Target& target = it.first;\n\u001b[1;32m    491\u001b[0m const IRModule& ir_module = it.second;\n\u001b[0;32m--> 492\u001b[0m auto pair = SplitMixedModule(ir_module, target, target_host);\n\u001b[1;32m    493\u001b[0m auto& host_mod = pair.first;\n\u001b[1;32m    494\u001b[0m auto& device_mod = pair.second;\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:418\u001b[0m, in \u001b[0;36mtvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m ICHECK(mod_mixed.defined()) << \"This module must be defined\";\n\u001b[1;32m    417\u001b[0m \n\u001b[0;32m--> 418\u001b[0m mod_mixed = ApplyPasses(mod_mixed, MixedModulePassManager(mod_mixed, target));\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m IRModule host_mod = ApplyPasses(mod_mixed, HostModulePassManager(mod_mixed, target_host));\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/driver/driver_api.cc:291\u001b[0m, in \u001b[0;36mtvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \n\u001b[1;32m    290\u001b[0m IRModule ApplyPasses(IRModule mod, transform::Sequential seq) {\n\u001b[0;32m--> 291\u001b[0m   mod = seq(std::move(mod));\n\u001b[1;32m    292\u001b[0m   return mod;\n\u001b[1;32m    293\u001b[0m }\n",
            "File \u001b[0;32m/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\u001b[0m, in \u001b[0;36moperator()\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m   s << \"    \" << err << \"\\n\";\n\u001b[1;32m    204\u001b[0m }\n\u001b[0;32m--> 205\u001b[0m LOG(FATAL) << \"RuntimeError: Memory verification failed with the following errors:\\n\"\n\u001b[1;32m    206\u001b[0m            << s.str() << \"  Did you forget to bind?\\n\"\n\u001b[1;32m    207\u001b[0m            << func;\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  4: operator()\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:531\n  3: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:492\n  2: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:418\n  1: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /ssd1/htalendr/tvm/src/driver/driver_api.cc:291\n  0: operator()\n        at /ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc:205\n  Did you forget to bind?\n    Variable `lv3` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `compute` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/ssd1/htalendr/tvm/src/tir/analysis/verify_memory.cc\", line 205\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef relu(lv3: T.Buffer((T.int64(1), T.int64(12), T.int64(64), T.int64(64)), \"float32\"), compute: T.Buffer((T.int64(1), T.int64(12), T.int64(64), T.int64(64)), \"float32\")):\n    T.func_attr({\"target\": T.target({\"arch\": \"sm_89\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"mtriple\": \"x86_64-conda-linux-gnu\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    for i1, i2, i3 in T.grid(12, 64, 64):\n        cse_var_1: T.int32 = i1 * 4096 + i2 * 64 + i3\n        compute_1 = T.Buffer((T.int64(49152),), data=compute.data)\n        lv3_1 = T.Buffer((T.int64(49152),), data=lv3.data)\n        compute_1[cse_var_1] = T.max(lv3_1[cse_var_1], T.float32(0.0))"
          ]
        }
      ],
      "source": [
        "from tvm import dlight as dl\n",
        "\n",
        "with tvm.target.Target(\"cuda\"):\n",
        "    gpu_mod = dl.ApplyDefaultSchedule(\n",
        "        dl.gpu.GEMV(),\n",
        "        dl.gpu.LowBatchGEMV(),\n",
        "        dl.gpu.Fallback(),\n",
        "        dl.gpu.Matmul(),\n",
        "        dl.gpu.Reduction(),\n",
        "        dl.gpu.Transpose(),\n",
        "        dl.gpu.GeneralReduction(),\n",
        "        dl.gpu.RMSNorm(),\n",
        "    )(mod)\n",
        "\n",
        "\n",
        "# if not IS_IN_CI:\n",
        "ex = relax.build(gpu_mod, target=\"cuda\")\n",
        "dev = tvm.device(\"cuda\", 0)\n",
        "vm = relax.VirtualMachine(ex, dev)\n",
        "# Need to allocate data and params on GPU device\n",
        "gpu_data = tvm.nd.array(np.random.rand(1, 3, 224, 224).astype(\"float32\"), dev)\n",
        "gpu_params = [tvm.nd.array(p, dev) for p in params[\"main\"]]\n",
        "gpu_out = vm[\"main\"](gpu_data, *gpu_params).numpy()\n",
        "\n",
        "print(gpu_out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
