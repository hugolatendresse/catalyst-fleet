/ssd1/htalendr/miniconda3/envs/env1/lib/python3.12/site-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
  getattr_node = gm.graph.get_attr(lifted_node)
/ssd1/htalendr/miniconda3/envs/env1/lib/python3.12/site-packages/torch/fx/graph.py:1801: UserWarning: Node input_ids target input_ids input_ids of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(
found function conv1d.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 5
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, list) of length 1
isinstance(node, list) of length 1
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function conv1d.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 5
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, list) of length 1
isinstance(node, list) of length 1
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function permute.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 2
found function embedding.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function arange.start !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function unsqueeze.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function repeat.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 2
found function index.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 1
isinstance(node, fx.Node)
node: fx.Node passed to index_tensor:
len of args 2
type of args[0] <class 'tvm.relax.expr.Var'>
args[0] p_decoder_embed_positions_weight
type of args[1] <class 'list'>
args[1] [lv158]
type of indices <class 'tvm.relax.expr.DataflowVar'>
found function to.dtype_layout !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default [16:57:19] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def tir_function(A: T.Buffer((T.int64(1), T.int64(1)), "int64"), T_reshape: T.Buffer((T.int64(1), T.int64(1)), "int64")):
    T.func_attr({"op_pattern": 1, "target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}), "tir.noalias": T.bool(True)})
    T_reshape_1 = T.Buffer((T.int64(1),), "int64", data=T_reshape.data)
    A_1 = T.Buffer((T.int64(1),), "int64", data=A.data)
    T_reshape_1[0] = A_1[0]
[16:57:19] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def tir_function(lv151: T.Buffer((T.int64(1), T.int64(1)), "int64"), compute: T.Buffer((T.int64(1), T.int64(1)), "int32")):
    T.func_attr({"op_pattern": 0, "target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}), "tir.noalias": T.bool(True)})
    compute_1 = T.Buffer((T.int64(1),), "int32", data=compute.data)
    lv151_1 = T.Buffer((T.int64(1),), "int64", data=lv151.data)
    compute_1[0] = T.Cast("int32", lv151_1[0])
[16:57:19] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def tir_function(lv152: T.Buffer((T.int64(1), T.int64(1)), "int32"), T_reshape: T.Buffer((T.int64(1),), "int32")):
    T.func_attr({"op_pattern": 1, "target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}), "tir.noalias": T.bool(True)})
    T_reshape_1 = T.Buffer((T.int64(1),), "int32", data=T_reshape.data)
    lv152_1 = T.Buffer((T.int64(1),), "int32", data=lv152.data)
    T_reshape_1[0] = lv152_1[0]
[16:57:19] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def tir_function(lv156: T.Buffer((T.int64(1),), "int64"), expand_dims: T.Buffer((T.int64(1), T.int64(1)), "int64")):
    T.func_attr({"op_pattern": 1, "target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}), "tir.noalias": T.bool(True)})
    expand_dims_1 = T.Buffer((T.int64(1),), "int64", data=expand_dims.data)
    lv156_1 = T.Buffer((T.int64(1),), "int64", data=lv156.data)
    expand_dims_1[0] = lv156_1[0]
[16:57:19] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def tir_function(A: T.Buffer((T.int64(1), T.int64(1)), "int64"), T_reshape: T.Buffer((T.int64(1), T.int64(1)), "int64")):
    T.func_attr({"op_pattern": 1, "target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}), "tir.noalias": T.bool(True)})
    T_reshape_1 = T.Buffer((T.int64(1),), "int64", data=T_reshape.data)
    A_1 = T.Buffer((T.int64(1),), "int64", data=A.data)
    T_reshape_1[0] = A_1[0]
[16:57:19] /ssd1/htalendr/tvm/src/relax/transform/fuse_tir.cc:1250: # from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def add(lv: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(3000)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv[v_ax0, v_ax1, v_ax2], lv1[v_ax0, v_ax1, T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv[v_ax0, v_ax1, v_ax2] + lv1[v_ax0, v_ax1, T.int64(0)]

    @T.prim_func(private=True)
    def add1(lv4: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32"), lv5: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(1500)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv4[v_ax0, v_ax1, v_ax2], lv5[v_ax0, v_ax1, T.int64(0)])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv4[v_ax0, v_ax1, v_ax2] + lv5[v_ax0, v_ax1, T.int64(0)]

    @T.prim_func(private=True)
    def add2(lv8: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), p_encoder_embed_positions_weight: T.Buffer((T.int64(1500), T.int64(384)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(384)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv8[v_ax0, v_ax1, v_ax2], p_encoder_embed_positions_weight[v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv8[v_ax0, v_ax1, v_ax2] + p_encoder_embed_positions_weight[v_ax1, v_ax2]

    @T.prim_func(private=True)
    def add3(lv12: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), p_encoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(384),), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(384)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv12[v_ax0, v_ax1, v_ax2], p_encoder_layers_0_self_attn_q_proj_bias[v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv12[v_ax0, v_ax1, v_ax2] + p_encoder_layers_0_self_attn_q_proj_bias[v_ax2]

    @T.prim_func(private=True)
    def add4(lv9: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), lv34: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(384)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv9[v_ax0, v_ax1, v_ax2], lv34[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv9[v_ax0, v_ax1, v_ax2] + lv34[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def add5(lv38: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32"), p_encoder_layers_0_fc1_bias: T.Buffer((T.int64(1536),), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(1536)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv38[v_ax0, v_ax1, v_ax2], p_encoder_layers_0_fc1_bias[v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv38[v_ax0, v_ax1, v_ax2] + p_encoder_layers_0_fc1_bias[v_ax2]

    @T.prim_func(private=True)
    def add6(lv155: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), lv160: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv155[v_ax0, v_ax1, v_ax2], lv160[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv155[v_ax0, v_ax1, v_ax2] + lv160[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def add7(lv164: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), p_decoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(384),), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv164[v_ax0, v_ax1, v_ax2], p_decoder_layers_0_self_attn_q_proj_bias[v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv164[v_ax0, v_ax1, v_ax2] + p_decoder_layers_0_self_attn_q_proj_bias[v_ax2]

    @T.prim_func(private=True)
    def add8(lv216: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32"), p_decoder_layers_0_fc1_bias: T.Buffer((T.int64(1536),), "float32"), T_add: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1536)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv216[v_ax0, v_ax1, v_ax2], p_decoder_layers_0_fc1_bias[v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = lv216[v_ax0, v_ax1, v_ax2] + p_decoder_layers_0_fc1_bias[v_ax2]

    @T.prim_func(private=True)
    def attention(lv25: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), lv26: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), lv27: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_transpose_1 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)))
        T_reshape = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(64)))
        T_transpose_2 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)))
        T_reshape_1 = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(64)))
        T_batch_matmul_NT = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(1500)))
        T_divide = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(1500)))
        T_softmax_maxelem = T.alloc_buffer((T.int64(6), T.int64(1500)))
        T_softmax_exp = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(1500)))
        T_softmax_expsum = T.alloc_buffer((T.int64(6), T.int64(1500)))
        T_softmax_norm = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(1500)))
        T_transpose_3 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)))
        T_reshape_2 = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(64)))
        T_batch_matmul_NN = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(64)))
        T_reshape_3 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv25[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv25[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_1[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = T_transpose_1[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv26[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_2[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_2[v_ax0, v_ax1, v_ax2, v_ax3] = lv26[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_2[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)])
                T.writes(T_reshape_1[v_ax0, v_ax1, v_ax2])
                T_reshape_1[v_ax0, v_ax1, v_ax2] = T_transpose_2[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)]
        for b, i, j, k in T.grid(T.int64(6), T.int64(1500), T.int64(1500), T.int64(64)):
            with T.block("T_batch_matmul_NT"):
                v_b, v_i, v_j, v_k = T.axis.remap("SSSR", [b, i, j, k])
                T.reads(T_reshape[v_b, v_i, v_k], T_reshape_1[v_b, v_j, v_k])
                T.writes(T_batch_matmul_NT[v_b, v_i, v_j])
                T.block_attr({"layout_free_placeholders": [T_reshape_1]})
                with T.init():
                    T_batch_matmul_NT[v_b, v_i, v_j] = T.float32(0.0)
                T_batch_matmul_NT[v_b, v_i, v_j] = T_batch_matmul_NT[v_b, v_i, v_j] + T_reshape[v_b, v_i, v_k] * T_reshape_1[v_b, v_j, v_k]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1500), T.int64(1500)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_batch_matmul_NT[v_ax0, v_ax1, v_ax2])
                T.writes(T_divide[v_ax0, v_ax1, v_ax2])
                T_divide[v_ax0, v_ax1, v_ax2] = T_batch_matmul_NT[v_ax0, v_ax1, v_ax2] / T.sqrt(T.float32(64.0))
        for i0, i1, k in T.grid(T.int64(6), T.int64(1500), T.int64(1500)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_divide[v_i0, v_i1, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1] = T.float32(-340282346638528859811704183484516925440.0)
                T_softmax_maxelem[v_i0, v_i1] = T.max(T_softmax_maxelem[v_i0, v_i1], T_divide[v_i0, v_i1, v_k])
        for i0, i1, i2 in T.grid(T.int64(6), T.int64(1500), T.int64(1500)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_divide[v_i0, v_i1, v_i2], T_softmax_maxelem[v_i0, v_i1])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2])
                T_softmax_exp[v_i0, v_i1, v_i2] = T.exp(T_divide[v_i0, v_i1, v_i2] - T_softmax_maxelem[v_i0, v_i1])
        for i0, i1, k in T.grid(T.int64(6), T.int64(1500), T.int64(1500)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1] = T.float32(0.0)
                T_softmax_expsum[v_i0, v_i1] = T_softmax_expsum[v_i0, v_i1] + T_softmax_exp[v_i0, v_i1, v_k]
        for i0, i1, i2 in T.grid(T.int64(6), T.int64(1500), T.int64(1500)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2], T_softmax_expsum[v_i0, v_i1])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                T.block_attr({"axis": 2})
                T_softmax_norm[v_i0, v_i1, v_i2] = T_softmax_exp[v_i0, v_i1, v_i2] / T_softmax_expsum[v_i0, v_i1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_transpose_2"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv27[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_3[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_3[v_ax0, v_ax1, v_ax2, v_ax3] = lv27[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_3[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)])
                T.writes(T_reshape_2[v_ax0, v_ax1, v_ax2])
                T_reshape_2[v_ax0, v_ax1, v_ax2] = T_transpose_3[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)]
        for b, i, j, k in T.grid(T.int64(6), T.int64(1500), T.int64(64), T.int64(1500)):
            with T.block("T_batch_matmul_NN"):
                v_b, v_i, v_j, v_k = T.axis.remap("SSSR", [b, i, j, k])
                T.reads(T_softmax_norm[v_b, v_i, v_k], T_reshape_2[v_b, v_k, v_j])
                T.writes(T_batch_matmul_NN[v_b, v_i, v_j])
                T.block_attr({"layout_free_placeholders": [T_reshape_2]})
                with T.init():
                    T_batch_matmul_NN[v_b, v_i, v_j] = T.float32(0.0)
                T_batch_matmul_NN[v_b, v_i, v_j] = T_batch_matmul_NN[v_b, v_i, v_j] + T_softmax_norm[v_b, v_i, v_k] * T_reshape_2[v_b, v_k, v_j]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_batch_matmul_NN[((v_ax3 // T.int64(64) + v_ax2) // T.int64(1500) + v_ax1) % T.int64(6), (v_ax3 // T.int64(64) + v_ax2) % T.int64(1500), v_ax3 % T.int64(64)])
                T.writes(T_reshape_3[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_3[v_ax0, v_ax1, v_ax2, v_ax3] = T_batch_matmul_NN[((v_ax3 // T.int64(64) + v_ax2) // T.int64(1500) + v_ax1) % T.int64(6), (v_ax3 // T.int64(64) + v_ax2) % T.int64(1500), v_ax3 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1500), T.int64(6), T.int64(64)):
            with T.block("T_transpose_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_reshape_3[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = T_reshape_3[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def attention1(lv177: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), lv178: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), lv179: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_transpose_1 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)))
        T_reshape = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(64)))
        T_transpose_2 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)))
        T_reshape_1 = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(64)))
        T_batch_matmul_NT = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1)))
        T_divide = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1)))
        T_softmax_maxelem = T.alloc_buffer((T.int64(6), T.int64(1)))
        T_softmax_exp = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1)))
        T_softmax_expsum = T.alloc_buffer((T.int64(6), T.int64(1)))
        T_softmax_norm = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1)))
        T_transpose_3 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)))
        T_reshape_2 = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(64)))
        T_batch_matmul_NN = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(64)))
        T_reshape_3 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv177[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv177[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_1[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = T_transpose_1[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv178[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_2[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_2[v_ax0, v_ax1, v_ax2, v_ax3] = lv178[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_2[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)])
                T.writes(T_reshape_1[v_ax0, v_ax1, v_ax2])
                T_reshape_1[v_ax0, v_ax1, v_ax2] = T_transpose_2[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)]
        for b, i, j, k in T.grid(T.int64(6), T.int64(1), T.int64(1), T.int64(64)):
            with T.block("T_batch_matmul_NT"):
                v_b, v_i, v_j, v_k = T.axis.remap("SSSR", [b, i, j, k])
                T.reads(T_reshape[v_b, v_i, v_k], T_reshape_1[v_b, v_j, v_k])
                T.writes(T_batch_matmul_NT[v_b, v_i, v_j])
                T.block_attr({"layout_free_placeholders": [T_reshape_1]})
                with T.init():
                    T_batch_matmul_NT[v_b, v_i, v_j] = T.float32(0.0)
                T_batch_matmul_NT[v_b, v_i, v_j] = T_batch_matmul_NT[v_b, v_i, v_j] + T_reshape[v_b, v_i, v_k] * T_reshape_1[v_b, v_j, v_k]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1), T.int64(1)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_batch_matmul_NT[v_ax0, v_ax1, v_ax2])
                T.writes(T_divide[v_ax0, v_ax1, v_ax2])
                T_divide[v_ax0, v_ax1, v_ax2] = T_batch_matmul_NT[v_ax0, v_ax1, v_ax2] / T.sqrt(T.float32(64.0))
        for i0, i1, k in T.grid(T.int64(6), T.int64(1), T.int64(1)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_divide[v_i0, v_i1, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1] = T.float32(-340282346638528859811704183484516925440.0)
                T_softmax_maxelem[v_i0, v_i1] = T.max(T_softmax_maxelem[v_i0, v_i1], T_divide[v_i0, v_i1, v_k])
        for i0, i1, i2 in T.grid(T.int64(6), T.int64(1), T.int64(1)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_divide[v_i0, v_i1, v_i2], T_softmax_maxelem[v_i0, v_i1])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2])
                T_softmax_exp[v_i0, v_i1, v_i2] = T.exp(T_divide[v_i0, v_i1, v_i2] - T_softmax_maxelem[v_i0, v_i1])
        for i0, i1, k in T.grid(T.int64(6), T.int64(1), T.int64(1)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1] = T.float32(0.0)
                T_softmax_expsum[v_i0, v_i1] = T_softmax_expsum[v_i0, v_i1] + T_softmax_exp[v_i0, v_i1, v_k]
        for i0, i1, i2 in T.grid(T.int64(6), T.int64(1), T.int64(1)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2], T_softmax_expsum[v_i0, v_i1])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                T.block_attr({"axis": 2})
                T_softmax_norm[v_i0, v_i1, v_i2] = T_softmax_exp[v_i0, v_i1, v_i2] / T_softmax_expsum[v_i0, v_i1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_transpose_2"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv179[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_3[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_3[v_ax0, v_ax1, v_ax2, v_ax3] = lv179[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_3[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)])
                T.writes(T_reshape_2[v_ax0, v_ax1, v_ax2])
                T_reshape_2[v_ax0, v_ax1, v_ax2] = T_transpose_3[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)]
        for b, i, j, k in T.grid(T.int64(6), T.int64(1), T.int64(64), T.int64(1)):
            with T.block("T_batch_matmul_NN"):
                v_b, v_i, v_j, v_k = T.axis.remap("SSSR", [b, i, j, k])
                T.reads(T_softmax_norm[v_b, v_i, v_k], T_reshape_2[v_b, v_k, v_j])
                T.writes(T_batch_matmul_NN[v_b, v_i, v_j])
                T.block_attr({"layout_free_placeholders": [T_reshape_2]})
                with T.init():
                    T_batch_matmul_NN[v_b, v_i, v_j] = T.float32(0.0)
                T_batch_matmul_NN[v_b, v_i, v_j] = T_batch_matmul_NN[v_b, v_i, v_j] + T_softmax_norm[v_b, v_i, v_k] * T_reshape_2[v_b, v_k, v_j]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_batch_matmul_NN[(v_ax3 // T.int64(64) + v_ax1 + v_ax2) % T.int64(6), T.int64(0), v_ax3 % T.int64(64)])
                T.writes(T_reshape_3[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_3[v_ax0, v_ax1, v_ax2, v_ax3] = T_batch_matmul_NN[(v_ax3 // T.int64(64) + v_ax1 + v_ax2) % T.int64(6), T.int64(0), v_ax3 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(6), T.int64(64)):
            with T.block("T_transpose_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_reshape_3[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = T_reshape_3[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def attention2(lv203: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), lv204: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), lv205: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_transpose_1 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)))
        T_reshape = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(64)))
        T_transpose_2 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)))
        T_reshape_1 = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(64)))
        T_batch_matmul_NT = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1500)))
        T_divide = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1500)))
        T_softmax_maxelem = T.alloc_buffer((T.int64(6), T.int64(1)))
        T_softmax_exp = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1500)))
        T_softmax_expsum = T.alloc_buffer((T.int64(6), T.int64(1)))
        T_softmax_norm = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(1500)))
        T_transpose_3 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)))
        T_reshape_2 = T.alloc_buffer((T.int64(6), T.int64(1500), T.int64(64)))
        T_batch_matmul_NN = T.alloc_buffer((T.int64(6), T.int64(1), T.int64(64)))
        T_reshape_3 = T.alloc_buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)))
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv203[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_1[v_ax0, v_ax1, v_ax2, v_ax3] = lv203[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_1[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = T_transpose_1[T.int64(0), (v_ax2 // T.int64(64) + v_ax0 + v_ax1) % T.int64(6), T.int64(0), v_ax2 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_transpose_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv204[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_2[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_2[v_ax0, v_ax1, v_ax2, v_ax3] = lv204[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_2[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)])
                T.writes(T_reshape_1[v_ax0, v_ax1, v_ax2])
                T_reshape_1[v_ax0, v_ax1, v_ax2] = T_transpose_2[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)]
        for b, i, j, k in T.grid(T.int64(6), T.int64(1), T.int64(1500), T.int64(64)):
            with T.block("T_batch_matmul_NT"):
                v_b, v_i, v_j, v_k = T.axis.remap("SSSR", [b, i, j, k])
                T.reads(T_reshape[v_b, v_i, v_k], T_reshape_1[v_b, v_j, v_k])
                T.writes(T_batch_matmul_NT[v_b, v_i, v_j])
                T.block_attr({"layout_free_placeholders": [T_reshape_1]})
                with T.init():
                    T_batch_matmul_NT[v_b, v_i, v_j] = T.float32(0.0)
                T_batch_matmul_NT[v_b, v_i, v_j] = T_batch_matmul_NT[v_b, v_i, v_j] + T_reshape[v_b, v_i, v_k] * T_reshape_1[v_b, v_j, v_k]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1), T.int64(1500)):
            with T.block("T_divide"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_batch_matmul_NT[v_ax0, v_ax1, v_ax2])
                T.writes(T_divide[v_ax0, v_ax1, v_ax2])
                T_divide[v_ax0, v_ax1, v_ax2] = T_batch_matmul_NT[v_ax0, v_ax1, v_ax2] / T.sqrt(T.float32(64.0))
        for i0, i1, k in T.grid(T.int64(6), T.int64(1), T.int64(1500)):
            with T.block("T_softmax_maxelem"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_divide[v_i0, v_i1, v_k])
                T.writes(T_softmax_maxelem[v_i0, v_i1])
                with T.init():
                    T_softmax_maxelem[v_i0, v_i1] = T.float32(-340282346638528859811704183484516925440.0)
                T_softmax_maxelem[v_i0, v_i1] = T.max(T_softmax_maxelem[v_i0, v_i1], T_divide[v_i0, v_i1, v_k])
        for i0, i1, i2 in T.grid(T.int64(6), T.int64(1), T.int64(1500)):
            with T.block("T_softmax_exp"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_divide[v_i0, v_i1, v_i2], T_softmax_maxelem[v_i0, v_i1])
                T.writes(T_softmax_exp[v_i0, v_i1, v_i2])
                T_softmax_exp[v_i0, v_i1, v_i2] = T.exp(T_divide[v_i0, v_i1, v_i2] - T_softmax_maxelem[v_i0, v_i1])
        for i0, i1, k in T.grid(T.int64(6), T.int64(1), T.int64(1500)):
            with T.block("T_softmax_expsum"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(T_softmax_exp[v_i0, v_i1, v_k])
                T.writes(T_softmax_expsum[v_i0, v_i1])
                with T.init():
                    T_softmax_expsum[v_i0, v_i1] = T.float32(0.0)
                T_softmax_expsum[v_i0, v_i1] = T_softmax_expsum[v_i0, v_i1] + T_softmax_exp[v_i0, v_i1, v_k]
        for i0, i1, i2 in T.grid(T.int64(6), T.int64(1), T.int64(1500)):
            with T.block("T_softmax_norm"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_softmax_exp[v_i0, v_i1, v_i2], T_softmax_expsum[v_i0, v_i1])
                T.writes(T_softmax_norm[v_i0, v_i1, v_i2])
                T.block_attr({"axis": 2})
                T_softmax_norm[v_i0, v_i1, v_i2] = T_softmax_exp[v_i0, v_i1, v_i2] / T_softmax_expsum[v_i0, v_i1]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_transpose_2"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv205[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose_3[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose_3[v_ax0, v_ax1, v_ax2, v_ax3] = lv205[v_ax0, v_ax2, v_ax1, v_ax3]
        for ax0, ax1, ax2 in T.grid(T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_reshape_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_transpose_3[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)])
                T.writes(T_reshape_2[v_ax0, v_ax1, v_ax2])
                T_reshape_2[v_ax0, v_ax1, v_ax2] = T_transpose_3[T.int64(0), ((v_ax2 // T.int64(64) + v_ax1) // T.int64(1500) + v_ax0) % T.int64(6), (v_ax2 // T.int64(64) + v_ax1) % T.int64(1500), v_ax2 % T.int64(64)]
        for b, i, j, k in T.grid(T.int64(6), T.int64(1), T.int64(64), T.int64(1500)):
            with T.block("T_batch_matmul_NN"):
                v_b, v_i, v_j, v_k = T.axis.remap("SSSR", [b, i, j, k])
                T.reads(T_softmax_norm[v_b, v_i, v_k], T_reshape_2[v_b, v_k, v_j])
                T.writes(T_batch_matmul_NN[v_b, v_i, v_j])
                T.block_attr({"layout_free_placeholders": [T_reshape_2]})
                with T.init():
                    T_batch_matmul_NN[v_b, v_i, v_j] = T.float32(0.0)
                T_batch_matmul_NN[v_b, v_i, v_j] = T_batch_matmul_NN[v_b, v_i, v_j] + T_softmax_norm[v_b, v_i, v_k] * T_reshape_2[v_b, v_k, v_j]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_reshape_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_batch_matmul_NN[(v_ax3 // T.int64(64) + v_ax1 + v_ax2) % T.int64(6), T.int64(0), v_ax3 % T.int64(64)])
                T.writes(T_reshape_3[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_3[v_ax0, v_ax1, v_ax2, v_ax3] = T_batch_matmul_NN[(v_ax3 // T.int64(64) + v_ax1 + v_ax2) % T.int64(6), T.int64(0), v_ax3 % T.int64(64)]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(6), T.int64(64)):
            with T.block("T_transpose_3"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_reshape_3[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = T_reshape_3[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def cast1(lv159: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), compute: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(lv159[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = lv159[v_i0, v_i1, v_i2]

    @T.prim_func(private=True)
    def conv1d(input_features: T.Buffer((T.int64(1), T.int64(80), T.int64(3000)), "float32"), p_encoder_conv1_weight: T.Buffer((T.int64(384), T.int64(80), T.int64(3)), "float32"), conv1d_ncw: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(80), T.int64(3002)))
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(80), T.int64(3002)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(input_features[v_i0, v_i1, v_i2 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2])
                pad_temp[v_i0, v_i1, v_i2] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(3001), input_features[v_i0, v_i1, v_i2 - T.int64(1)], T.float32(0.0))
        for nn, ff, yy, rc, ry in T.grid(T.int64(1), T.int64(384), T.int64(3000), T.int64(80), T.int64(3)):
            with T.block("conv1d_ncw"):
                v_nn, v_ff, v_yy, v_rc, v_ry = T.axis.remap("SSSRR", [nn, ff, yy, rc, ry])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry], p_encoder_conv1_weight[v_ff, v_rc, v_ry])
                T.writes(conv1d_ncw[v_nn, v_ff, v_yy])
                with T.init():
                    conv1d_ncw[v_nn, v_ff, v_yy] = T.float32(0.0)
                conv1d_ncw[v_nn, v_ff, v_yy] = conv1d_ncw[v_nn, v_ff, v_yy] + pad_temp[v_nn, v_rc, v_yy + v_ry] * p_encoder_conv1_weight[v_ff, v_rc, v_ry]

    @T.prim_func(private=True)
    def conv1d1(lv3: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32"), p_encoder_conv2_weight: T.Buffer((T.int64(384), T.int64(384), T.int64(3)), "float32"), conv1d_ncw: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(3002)))
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(384), T.int64(3002)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(lv3[v_i0, v_i1, v_i2 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2])
                pad_temp[v_i0, v_i1, v_i2] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(3001), lv3[v_i0, v_i1, v_i2 - T.int64(1)], T.float32(0.0))
        for nn, ff, yy, rc, ry in T.grid(T.int64(1), T.int64(384), T.int64(1500), T.int64(384), T.int64(3)):
            with T.block("conv1d_ncw"):
                v_nn, v_ff, v_yy, v_rc, v_ry = T.axis.remap("SSSRR", [nn, ff, yy, rc, ry])
                T.reads(pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry], p_encoder_conv2_weight[v_ff, v_rc, v_ry])
                T.writes(conv1d_ncw[v_nn, v_ff, v_yy])
                with T.init():
                    conv1d_ncw[v_nn, v_ff, v_yy] = T.float32(0.0)
                conv1d_ncw[v_nn, v_ff, v_yy] = conv1d_ncw[v_nn, v_ff, v_yy] + pad_temp[v_nn, v_rc, v_yy * T.int64(2) + v_ry] * p_encoder_conv2_weight[v_ff, v_rc, v_ry]

    @T.prim_func(private=True)
    def gelu(lv2: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(3000)))
        compute = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(3000)))
        T_multiply_2 = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(3000)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(3000)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(3000)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv2[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = lv2[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(384), T.int64(3000)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply_1[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.erf(T_multiply_1[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(3000)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T_multiply_2[v_ax0, v_ax1, v_ax2] = compute[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(3000)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_2[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(3000)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv2[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = lv2[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def gelu1(lv6: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(1500)))
        compute = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(1500)))
        T_multiply_2 = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(1500)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(1500)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(1500)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv6[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = lv6[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(384), T.int64(1500)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply_1[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.erf(T_multiply_1[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(1500)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T_multiply_2[v_ax0, v_ax1, v_ax2] = compute[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(1500)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_2[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(1500)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv6[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = lv6[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def gelu2(lv39: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(1500), T.int64(1536)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1500), T.int64(1536)))
        T_multiply_2 = T.alloc_buffer((T.int64(1), T.int64(1500), T.int64(1536)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(1500), T.int64(1536)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(1536)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv39[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = lv39[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1500), T.int64(1536)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply_1[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.erf(T_multiply_1[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(1536)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T_multiply_2[v_ax0, v_ax1, v_ax2] = compute[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(1536)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_2[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(1536)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv39[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = lv39[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def gelu3(lv217: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32"), T_multiply: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32")):
        T.func_attr({"op_pattern": 0, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_multiply_1 = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(1536)))
        compute = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(1536)))
        T_multiply_2 = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(1536)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(1536)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1536)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv217[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_1[v_ax0, v_ax1, v_ax2])
                T_multiply_1[v_ax0, v_ax1, v_ax2] = lv217[v_ax0, v_ax1, v_ax2] * T.float32(0.70710678118654757)
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(1536)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply_1[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.erf(T_multiply_1[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1536)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T_multiply_2[v_ax0, v_ax1, v_ax2] = compute[v_ax0, v_ax1, v_ax2] * T.float32(0.5)
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1536)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_2[v_ax0, v_ax1, v_ax2])
                T.writes(T_add[v_ax0, v_ax1, v_ax2])
                T_add[v_ax0, v_ax1, v_ax2] = T.float32(0.5) + T_multiply_2[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1536)):
            with T.block("T_multiply_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv217[v_ax0, v_ax1, v_ax2], T_add[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = lv217[v_ax0, v_ax1, v_ax2] * T_add[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def index_tensor(p_decoder_embed_positions_weight: T.Buffer((T.int64(448), T.int64(384)), "float32"), lv158: T.Buffer((T.int64(1), T.int64(1)), "int64"), T_take: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 8, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("T_take"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(p_decoder_embed_positions_weight[T.min(T.max(T.int64(0), lv158[v_ax0, v_ax1]), T.int64(447)), v_ax2], lv158[v_ax0, v_ax1])
                T.writes(T_take[v_ax0, v_ax1, v_ax2])
                T_take[v_ax0, v_ax1, v_ax2] = p_decoder_embed_positions_weight[T.min(T.max(T.int64(0), lv158[v_ax0, v_ax1]), T.int64(447)), v_ax2]

    @T.prim_func(private=True)
    def layer_norm(lv9: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), p_encoder_layers_0_self_attn_layer_norm_weight: T.Buffer((T.int64(384),), "float32"), p_encoder_layers_0_self_attn_layer_norm_bias: T.Buffer((T.int64(384),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv9_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(1500)))
        lv9_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(1500)))
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(1500), T.int64(384)):
            with T.block("lv9_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(lv9[v_ax0, v_ax1, v_k2])
                T.writes(lv9_red_temp_v0[v_ax0, v_ax1], lv9_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    lv9_red_temp_v0[v_ax0, v_ax1] = T.float32(0.0)
                    lv9_red_temp_v1[v_ax0, v_ax1] = T.float32(0.0)
                v_lv9_red_temp_v0: T.float32 = lv9_red_temp_v0[v_ax0, v_ax1] + lv9[v_ax0, v_ax1, v_k2]
                v_lv9_red_temp_v1: T.float32 = lv9_red_temp_v1[v_ax0, v_ax1] + lv9[v_ax0, v_ax1, v_k2] * lv9[v_ax0, v_ax1, v_k2]
                lv9_red_temp_v0[v_ax0, v_ax1] = v_lv9_red_temp_v0
                lv9_red_temp_v1[v_ax0, v_ax1] = v_lv9_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(384)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv9[v_ax0, v_ax1, v_ax2], lv9_red_temp_v0[v_ax0, v_ax1], lv9_red_temp_v1[v_ax0, v_ax1], p_encoder_layers_0_self_attn_layer_norm_weight[v_ax2], p_encoder_layers_0_self_attn_layer_norm_bias[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = (lv9[v_ax0, v_ax1, v_ax2] - lv9_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0026041666666666665)) * T.rsqrt(lv9_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0026041666666666665) - lv9_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0026041666666666665) * (lv9_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0026041666666666665)) + T.float32(1.0000000000000001e-05)) * p_encoder_layers_0_self_attn_layer_norm_weight[v_ax2] + p_encoder_layers_0_self_attn_layer_norm_bias[v_ax2]

    @T.prim_func(private=True)
    def layer_norm1(lv161: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), p_decoder_layers_0_self_attn_layer_norm_weight: T.Buffer((T.int64(384),), "float32"), p_decoder_layers_0_self_attn_layer_norm_bias: T.Buffer((T.int64(384),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv161_red_temp_v0 = T.alloc_buffer((T.int64(1), T.int64(1)))
        lv161_red_temp_v1 = T.alloc_buffer((T.int64(1), T.int64(1)))
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("lv161_red_temp"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(lv161[v_ax0, v_ax1, v_k2])
                T.writes(lv161_red_temp_v0[v_ax0, v_ax1], lv161_red_temp_v1[v_ax0, v_ax1])
                with T.init():
                    lv161_red_temp_v0[v_ax0, v_ax1] = T.float32(0.0)
                    lv161_red_temp_v1[v_ax0, v_ax1] = T.float32(0.0)
                v_lv161_red_temp_v0: T.float32 = lv161_red_temp_v0[v_ax0, v_ax1] + lv161[v_ax0, v_ax1, v_k2]
                v_lv161_red_temp_v1: T.float32 = lv161_red_temp_v1[v_ax0, v_ax1] + lv161[v_ax0, v_ax1, v_k2] * lv161[v_ax0, v_ax1, v_k2]
                lv161_red_temp_v0[v_ax0, v_ax1] = v_lv161_red_temp_v0
                lv161_red_temp_v1[v_ax0, v_ax1] = v_lv161_red_temp_v1
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("T_layer_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv161[v_ax0, v_ax1, v_ax2], lv161_red_temp_v0[v_ax0, v_ax1], lv161_red_temp_v1[v_ax0, v_ax1], p_decoder_layers_0_self_attn_layer_norm_weight[v_ax2], p_decoder_layers_0_self_attn_layer_norm_bias[v_ax2])
                T.writes(T_layer_norm[v_ax0, v_ax1, v_ax2])
                T_layer_norm[v_ax0, v_ax1, v_ax2] = (lv161[v_ax0, v_ax1, v_ax2] - lv161_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0026041666666666665)) * T.rsqrt(lv161_red_temp_v1[v_ax0, v_ax1] * T.float32(0.0026041666666666665) - lv161_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0026041666666666665) * (lv161_red_temp_v0[v_ax0, v_ax1] * T.float32(0.0026041666666666665)) + T.float32(1.0000000000000001e-05)) * p_decoder_layers_0_self_attn_layer_norm_weight[v_ax2] + p_decoder_layers_0_self_attn_layer_norm_bias[v_ax2]

    @T.prim_func(private=True)
    def matmul(lv10: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), lv11: T.Buffer((T.int64(384), T.int64(384)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1500), T.int64(384), T.int64(384)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv10[v_i0, v_i1, v_k], lv11[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0.0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + lv10[v_i0, v_i1, v_k] * lv11[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul1(lv36: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), lv37: T.Buffer((T.int64(384), T.int64(1536)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1500), T.int64(1536), T.int64(384)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv36[v_i0, v_i1, v_k], lv37[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0.0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + lv36[v_i0, v_i1, v_k] * lv37[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul2(lv40: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32"), lv41: T.Buffer((T.int64(1536), T.int64(384)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1500), T.int64(384), T.int64(1536)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv40[v_i0, v_i1, v_k], lv41[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0.0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + lv40[v_i0, v_i1, v_k] * lv41[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul3(lv162: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), lv163: T.Buffer((T.int64(384), T.int64(384)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(384), T.int64(384)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv162[v_i0, v_i1, v_k], lv163[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0.0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + lv162[v_i0, v_i1, v_k] * lv163[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul4(lv214: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), lv215: T.Buffer((T.int64(384), T.int64(1536)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(1536), T.int64(384)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv214[v_i0, v_i1, v_k], lv215[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0.0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + lv214[v_i0, v_i1, v_k] * lv215[v_k, v_i2]

    @T.prim_func(private=True)
    def matmul5(lv218: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32"), lv219: T.Buffer((T.int64(1536), T.int64(384)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 4, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(384), T.int64(1536)):
            with T.block("matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv218[v_i0, v_i1, v_k], lv219[v_k, v_i2])
                T.writes(matmul[v_i0, v_i1, v_i2])
                with T.init():
                    matmul[v_i0, v_i1, v_i2] = T.float32(0.0)
                matmul[v_i0, v_i1, v_i2] = matmul[v_i0, v_i1, v_i2] + lv218[v_i0, v_i1, v_k] * lv219[v_k, v_i2]

    @T.prim_func(private=True)
    def reshape(p_encoder_conv1_bias: T.Buffer((T.int64(384),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(384), T.int64(1)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(p_encoder_conv1_bias[(v_ax1 + v_ax2) % T.int64(384)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = p_encoder_conv1_bias[(v_ax1 + v_ax2) % T.int64(384)]

    @T.prim_func(private=True)
    def reshape1(lv13: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1500), T.int64(6), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv13[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(384) + v_ax1) % T.int64(1500), (v_ax2 * T.int64(64) + v_ax3) % T.int64(384)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv13[T.int64(0), ((v_ax2 * T.int64(64) + v_ax3) // T.int64(384) + v_ax1) % T.int64(1500), (v_ax2 * T.int64(64) + v_ax3) % T.int64(384)]

    @T.prim_func(private=True)
    def reshape2(lv30: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(384)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv30[T.int64(0), (v_ax2 // T.int64(384) + v_ax1) % T.int64(1500), v_ax2 % T.int64(384) // T.int64(64), v_ax2 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = lv30[T.int64(0), (v_ax2 // T.int64(384) + v_ax1) % T.int64(1500), v_ax2 % T.int64(384) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def reshape5(lv154: T.Buffer((T.int64(1), T.int64(384)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv154[T.int64(0), v_ax2 % T.int64(384)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = lv154[T.int64(0), v_ax2 % T.int64(384)]

    @T.prim_func(private=True)
    def reshape6(lv165: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(6), T.int64(64)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv165[T.int64(0), T.int64(0), (v_ax2 * T.int64(64) + v_ax3) % T.int64(384)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv165[T.int64(0), T.int64(0), (v_ax2 * T.int64(64) + v_ax3) % T.int64(384)]

    @T.prim_func(private=True)
    def reshape7(lv182: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(384)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv182[T.int64(0), T.int64(0), v_ax2 % T.int64(384) // T.int64(64), v_ax2 % T.int64(64)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = lv182[T.int64(0), T.int64(0), v_ax2 % T.int64(384) // T.int64(64), v_ax2 % T.int64(64)]

    @T.prim_func(private=True)
    def take(p_decoder_embed_tokens_weight: T.Buffer((T.int64(51865), T.int64(384)), "float32"), lv153: T.Buffer((T.int64(1),), "int32"), T_take: T.Buffer((T.int64(1), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 8, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(1), T.int64(384)):
            with T.block("T_take"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(p_decoder_embed_tokens_weight[lv153[v_ax0], v_ax1], lv153[v_ax0])
                T.writes(T_take[v_ax0, v_ax1])
                T_take[v_ax0, v_ax1] = p_decoder_embed_tokens_weight[lv153[v_ax0], v_ax1]

    @T.prim_func(private=True)
    def transpose(lv7: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1500), T.int64(384)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv7[v_ax0, v_ax2, v_ax1])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2])
                T_transpose[v_ax0, v_ax1, v_ax2] = lv7[v_ax0, v_ax2, v_ax1]

    @T.prim_func(private=True)
    def transpose1(p_encoder_layers_0_self_attn_q_proj_weight: T.Buffer((T.int64(384), T.int64(384)), "float32"), T_transpose: T.Buffer((T.int64(384), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(384), T.int64(384)):
            with T.block("T_transpose"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(p_encoder_layers_0_self_attn_q_proj_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = p_encoder_layers_0_self_attn_q_proj_weight[v_ax1, v_ax0]

    @T.prim_func(private=True)
    def transpose2(lv14: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1500), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv14[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = lv14[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def transpose3(lv15: T.Buffer((T.int64(1), T.int64(6), T.int64(1500), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1500), T.int64(6), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv15[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = lv15[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def transpose4(p_encoder_layers_0_fc1_weight: T.Buffer((T.int64(1536), T.int64(384)), "float32"), T_transpose: T.Buffer((T.int64(384), T.int64(1536)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(384), T.int64(1536)):
            with T.block("T_transpose"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(p_encoder_layers_0_fc1_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = p_encoder_layers_0_fc1_weight[v_ax1, v_ax0]

    @T.prim_func(private=True)
    def transpose5(p_encoder_layers_0_fc2_weight: T.Buffer((T.int64(384), T.int64(1536)), "float32"), T_transpose: T.Buffer((T.int64(1536), T.int64(384)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1 in T.grid(T.int64(1536), T.int64(384)):
            with T.block("T_transpose"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(p_encoder_layers_0_fc2_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = p_encoder_layers_0_fc2_weight[v_ax1, v_ax0]

    @T.prim_func(private=True)
    def transpose6(lv166: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(6), T.int64(1), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv166[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = lv166[v_ax0, v_ax2, v_ax1, v_ax3]

    @T.prim_func(private=True)
    def transpose7(lv167: T.Buffer((T.int64(1), T.int64(6), T.int64(1), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(6), T.int64(64)):
            with T.block("T_transpose"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv167[v_ax0, v_ax2, v_ax1, v_ax3])
                T.writes(T_transpose[v_ax0, v_ax1, v_ax2, v_ax3])
                T_transpose[v_ax0, v_ax1, v_ax2, v_ax3] = lv167[v_ax0, v_ax2, v_ax1, v_ax3]

    @R.function(private=True)
    def fused_conv1d1_add1_gelu1(lv3: R.Tensor((1, 384, 3000), dtype="float32"), p_encoder_conv2_weight: R.Tensor((384, 384, 3), dtype="float32"), lv5: R.Tensor((1, 384, 1), dtype="float32")) -> R.Tensor((1, 384, 1500), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv4 = R.call_tir(cls.conv1d1, (lv3, p_encoder_conv2_weight), out_sinfo=R.Tensor((1, 384, 1500), dtype="float32"))
            lv6 = R.call_tir(cls.add1, (lv4, lv5), out_sinfo=R.Tensor((1, 384, 1500), dtype="float32"))
            gv = R.call_tir(cls.gelu1, (lv6,), out_sinfo=R.Tensor((1, 384, 1500), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_conv1d_add_gelu(input_features: R.Tensor((1, 80, 3000), dtype="float32"), p_encoder_conv1_weight: R.Tensor((384, 80, 3), dtype="float32"), lv1: R.Tensor((1, 384, 1), dtype="float32")) -> R.Tensor((1, 384, 3000), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.conv1d, (input_features, p_encoder_conv1_weight), out_sinfo=R.Tensor((1, 384, 3000), dtype="float32"))
            lv2 = R.call_tir(cls.add, (lv, lv1), out_sinfo=R.Tensor((1, 384, 3000), dtype="float32"))
            gv = R.call_tir(cls.gelu, (lv2,), out_sinfo=R.Tensor((1, 384, 3000), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul1_add5_gelu2(lv36: R.Tensor((1, 1500, 384), dtype="float32"), lv37: R.Tensor((384, 1536), dtype="float32"), p_encoder_layers_0_fc1_bias: R.Tensor((1536,), dtype="float32")) -> R.Tensor((1, 1500, 1536), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv38 = R.call_tir(cls.matmul1, (lv36, lv37), out_sinfo=R.Tensor((1, 1500, 1536), dtype="float32"))
            lv39 = R.call_tir(cls.add5, (lv38, p_encoder_layers_0_fc1_bias), out_sinfo=R.Tensor((1, 1500, 1536), dtype="float32"))
            gv = R.call_tir(cls.gelu2, (lv39,), out_sinfo=R.Tensor((1, 1500, 1536), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul2_add3_add4(lv40: R.Tensor((1, 1500, 1536), dtype="float32"), lv41: R.Tensor((1536, 384), dtype="float32"), p_encoder_layers_0_fc2_bias: R.Tensor((384,), dtype="float32"), lv35: R.Tensor((1, 1500, 384), dtype="float32")) -> R.Tensor((1, 1500, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv42 = R.call_tir(cls.matmul2, (lv40, lv41), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv43 = R.call_tir(cls.add3, (lv42, p_encoder_layers_0_fc2_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            gv = R.call_tir(cls.add4, (lv35, lv43), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul3_add7(lv162: R.Tensor((1, 1, 384), dtype="float32"), lv163: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32")) -> R.Tensor((1, 1, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv164 = R.call_tir(cls.matmul3, (lv162, lv163), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            gv = R.call_tir(cls.add7, (lv164, p_decoder_layers_0_self_attn_q_proj_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul3_add7_add6(lv183: R.Tensor((1, 1, 384), dtype="float32"), lv184: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), lv161: R.Tensor((1, 1, 384), dtype="float32")) -> R.Tensor((1, 1, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv185 = R.call_tir(cls.matmul3, (lv183, lv184), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv186 = R.call_tir(cls.add7, (lv185, p_decoder_layers_0_self_attn_out_proj_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            gv = R.call_tir(cls.add6, (lv161, lv186), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul4_add8_gelu3(lv214: R.Tensor((1, 1, 384), dtype="float32"), lv215: R.Tensor((384, 1536), dtype="float32"), p_decoder_layers_0_fc1_bias: R.Tensor((1536,), dtype="float32")) -> R.Tensor((1, 1, 1536), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv216 = R.call_tir(cls.matmul4, (lv214, lv215), out_sinfo=R.Tensor((1, 1, 1536), dtype="float32"))
            lv217 = R.call_tir(cls.add8, (lv216, p_decoder_layers_0_fc1_bias), out_sinfo=R.Tensor((1, 1, 1536), dtype="float32"))
            gv = R.call_tir(cls.gelu3, (lv217,), out_sinfo=R.Tensor((1, 1, 1536), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul5_add7_add6(lv218: R.Tensor((1, 1, 1536), dtype="float32"), lv219: R.Tensor((1536, 384), dtype="float32"), p_decoder_layers_0_fc2_bias: R.Tensor((384,), dtype="float32"), lv213: R.Tensor((1, 1, 384), dtype="float32")) -> R.Tensor((1, 1, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv220 = R.call_tir(cls.matmul5, (lv218, lv219), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv221 = R.call_tir(cls.add7, (lv220, p_decoder_layers_0_fc2_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            gv = R.call_tir(cls.add6, (lv213, lv221), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul_add3(lv10: R.Tensor((1, 1500, 384), dtype="float32"), lv11: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_0_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32")) -> R.Tensor((1, 1500, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv12 = R.call_tir(cls.matmul, (lv10, lv11), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            gv = R.call_tir(cls.add3, (lv12, p_encoder_layers_0_self_attn_q_proj_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_matmul_add3_add4(lv31: R.Tensor((1, 1500, 384), dtype="float32"), lv32: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_0_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), lv9: R.Tensor((1, 1500, 384), dtype="float32")) -> R.Tensor((1, 1500, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv33 = R.call_tir(cls.matmul, (lv31, lv32), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv34 = R.call_tir(cls.add3, (lv33, p_encoder_layers_0_self_attn_out_proj_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            gv = R.call_tir(cls.add4, (lv9, lv34), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_reshape1_transpose2_transpose3(lv13: R.Tensor((1, 1500, 384), dtype="float32")) -> R.Tensor((1, 1500, 6, 64), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv14 = R.call_tir(cls.reshape1, (lv13,), out_sinfo=R.Tensor((1, 1500, 6, 64), dtype="float32"))
            lv15 = R.call_tir(cls.transpose2, (lv14,), out_sinfo=R.Tensor((1, 6, 1500, 64), dtype="float32"))
            gv = R.call_tir(cls.transpose3, (lv15,), out_sinfo=R.Tensor((1, 1500, 6, 64), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_reshape5_cast1_add6(lv154: R.Tensor((1, 384), dtype="float32"), lv159: R.Tensor((1, 1, 384), dtype="float32")) -> R.Tensor((1, 1, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv155 = R.call_tir(cls.reshape5, (lv154,), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv160 = R.call_tir(cls.cast1, (lv159,), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            gv = R.call_tir(cls.add6, (lv155, lv160), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_reshape6_transpose6_transpose7(lv165: R.Tensor((1, 1, 384), dtype="float32")) -> R.Tensor((1, 1, 6, 64), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv166 = R.call_tir(cls.reshape6, (lv165,), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv167 = R.call_tir(cls.transpose6, (lv166,), out_sinfo=R.Tensor((1, 6, 1, 64), dtype="float32"))
            gv = R.call_tir(cls.transpose7, (lv167,), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_transpose2_transpose3_reshape2(lv28: R.Tensor((1, 1500, 6, 64), dtype="float32")) -> R.Tensor((1, 1500, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv29 = R.call_tir(cls.transpose2, (lv28,), out_sinfo=R.Tensor((1, 6, 1500, 64), dtype="float32"))
            lv30 = R.call_tir(cls.transpose3, (lv29,), out_sinfo=R.Tensor((1, 1500, 6, 64), dtype="float32"))
            gv = R.call_tir(cls.reshape2, (lv30,), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_transpose6_transpose7_reshape7(lv180: R.Tensor((1, 1, 6, 64), dtype="float32")) -> R.Tensor((1, 1, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv181 = R.call_tir(cls.transpose6, (lv180,), out_sinfo=R.Tensor((1, 6, 1, 64), dtype="float32"))
            lv182 = R.call_tir(cls.transpose7, (lv181,), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            gv = R.call_tir(cls.reshape7, (lv182,), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_transpose_add2(lv7: R.Tensor((1, 384, 1500), dtype="float32"), p_encoder_embed_positions_weight: R.Tensor((1500, 384), dtype="float32")) -> R.Tensor((1, 1500, 384), dtype="float32"):
        R.func_attr({"Primitive": 1})
        cls = Module
        with R.dataflow():
            lv8 = R.call_tir(cls.transpose, (lv7,), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            gv = R.call_tir(cls.add2, (lv8, p_encoder_embed_positions_weight), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            R.output(gv)
        return gv

    @R.function
    def main(input_features: R.Tensor((1, 80, 3000), dtype="float32"), p_encoder_embed_positions_weight: R.Tensor((1500, 384), dtype="float32"), p_decoder_embed_positions_weight: R.Tensor((448, 384), dtype="float32"), p_encoder_conv1_weight: R.Tensor((384, 80, 3), dtype="float32"), p_encoder_conv1_bias: R.Tensor((384,), dtype="float32"), p_encoder_conv2_weight: R.Tensor((384, 384, 3), dtype="float32"), p_encoder_conv2_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_0_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_0_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_0_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_0_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_0_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_encoder_layers_0_fc1_bias: R.Tensor((1536,), dtype="float32"), p_encoder_layers_0_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_encoder_layers_0_fc2_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_1_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_1_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_1_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_1_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_1_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_encoder_layers_1_fc1_bias: R.Tensor((1536,), dtype="float32"), p_encoder_layers_1_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_encoder_layers_1_fc2_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_2_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_2_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_2_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_2_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_2_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_encoder_layers_2_fc1_bias: R.Tensor((1536,), dtype="float32"), p_encoder_layers_2_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_encoder_layers_2_fc2_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_3_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_3_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_3_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_encoder_layers_3_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_encoder_layers_3_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_encoder_layers_3_fc1_bias: R.Tensor((1536,), dtype="float32"), p_encoder_layers_3_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_encoder_layers_3_fc2_bias: R.Tensor((384,), dtype="float32"), p_encoder_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_encoder_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_embed_tokens_weight: R.Tensor((51865, 384), dtype="float32"), p_decoder_layers_0_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_encoder_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_encoder_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_encoder_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_encoder_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_encoder_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_encoder_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_encoder_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_encoder_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_0_encoder_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_0_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_decoder_layers_0_fc1_bias: R.Tensor((1536,), dtype="float32"), p_decoder_layers_0_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_decoder_layers_0_fc2_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_encoder_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_encoder_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_encoder_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_encoder_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_encoder_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_encoder_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_encoder_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_encoder_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_1_encoder_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_1_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_decoder_layers_1_fc1_bias: R.Tensor((1536,), dtype="float32"), p_decoder_layers_1_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_decoder_layers_1_fc2_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_encoder_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_encoder_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_encoder_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_encoder_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_encoder_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_encoder_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_encoder_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_encoder_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_2_encoder_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_2_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_decoder_layers_2_fc1_bias: R.Tensor((1536,), dtype="float32"), p_decoder_layers_2_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_decoder_layers_2_fc2_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_self_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_self_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_self_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_self_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_self_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_self_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_self_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_self_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_self_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_encoder_attn_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_encoder_attn_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_encoder_attn_q_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_encoder_attn_q_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_encoder_attn_k_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_encoder_attn_v_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_encoder_attn_v_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_encoder_attn_out_proj_weight: R.Tensor((384, 384), dtype="float32"), p_decoder_layers_3_encoder_attn_out_proj_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_final_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_final_layer_norm_bias: R.Tensor((384,), dtype="float32"), p_decoder_layers_3_fc1_weight: R.Tensor((1536, 384), dtype="float32"), p_decoder_layers_3_fc1_bias: R.Tensor((1536,), dtype="float32"), p_decoder_layers_3_fc2_weight: R.Tensor((384, 1536), dtype="float32"), p_decoder_layers_3_fc2_bias: R.Tensor((384,), dtype="float32"), p_decoder_layer_norm_weight: R.Tensor((384,), dtype="float32"), p_decoder_layer_norm_bias: R.Tensor((384,), dtype="float32")) -> R.Tuple(R.Tensor((1, 1, 384), dtype="float32")):
        R.func_attr({"num_input": 1})
        cls = Module
        with R.dataflow():
            lv1 = R.call_tir(cls.reshape, (p_encoder_conv1_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv: R.Tensor((1, 384, 3000), dtype="float32") = cls.fused_conv1d_add_gelu(input_features, p_encoder_conv1_weight, lv1)
            lv5 = R.call_tir(cls.reshape, (p_encoder_conv2_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv1_1: R.Tensor((1, 384, 1500), dtype="float32") = cls.fused_conv1d1_add1_gelu1(lv, p_encoder_conv2_weight, lv5)
            lv2: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_transpose_add2(lv1_1, p_encoder_embed_positions_weight)
            lv10 = R.call_tir(cls.layer_norm, (lv2, p_encoder_layers_0_self_attn_layer_norm_weight, p_encoder_layers_0_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv11 = R.call_tir(cls.transpose1, (p_encoder_layers_0_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv3: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv10, lv11, p_encoder_layers_0_self_attn_q_proj_bias)
            lv4: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv3)
            lv16 = R.call_tir(cls.transpose1, (p_encoder_layers_0_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv17 = R.call_tir(cls.matmul, (lv10, lv16), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv5_1: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv17)
            lv20 = R.call_tir(cls.transpose1, (p_encoder_layers_0_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv6: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv10, lv20, p_encoder_layers_0_self_attn_v_proj_bias)
            lv7: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv6)
            lv28 = R.call_tir(cls.attention, (lv4, lv5_1, lv7), out_sinfo=R.Tensor((1, 1500, 6, 64), dtype="float32"))
            lv8: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_transpose2_transpose3_reshape2(lv28)
            lv32 = R.call_tir(cls.transpose1, (p_encoder_layers_0_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv9: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3_add4(lv8, lv32, p_encoder_layers_0_self_attn_out_proj_bias, lv2)
            lv36 = R.call_tir(cls.layer_norm, (lv9, p_encoder_layers_0_final_layer_norm_weight, p_encoder_layers_0_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv37 = R.call_tir(cls.transpose4, (p_encoder_layers_0_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv10_1: R.Tensor((1, 1500, 1536), dtype="float32") = cls.fused_matmul1_add5_gelu2(lv36, lv37, p_encoder_layers_0_fc1_bias)
            lv41 = R.call_tir(cls.transpose5, (p_encoder_layers_0_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv11_1: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul2_add3_add4(lv10_1, lv41, p_encoder_layers_0_fc2_bias, lv9)
            lv45 = R.call_tir(cls.layer_norm, (lv11_1, p_encoder_layers_1_self_attn_layer_norm_weight, p_encoder_layers_1_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv46 = R.call_tir(cls.transpose1, (p_encoder_layers_1_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv12: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv45, lv46, p_encoder_layers_1_self_attn_q_proj_bias)
            lv13: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv12)
            lv51 = R.call_tir(cls.transpose1, (p_encoder_layers_1_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv52 = R.call_tir(cls.matmul, (lv45, lv51), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv14: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv52)
            lv55 = R.call_tir(cls.transpose1, (p_encoder_layers_1_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv15: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv45, lv55, p_encoder_layers_1_self_attn_v_proj_bias)
            lv16_1: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv15)
            lv63 = R.call_tir(cls.attention, (lv13, lv14, lv16_1), out_sinfo=R.Tensor((1, 1500, 6, 64), dtype="float32"))
            lv17_1: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_transpose2_transpose3_reshape2(lv63)
            lv67 = R.call_tir(cls.transpose1, (p_encoder_layers_1_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv18: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3_add4(lv17_1, lv67, p_encoder_layers_1_self_attn_out_proj_bias, lv11_1)
            lv71 = R.call_tir(cls.layer_norm, (lv18, p_encoder_layers_1_final_layer_norm_weight, p_encoder_layers_1_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv72 = R.call_tir(cls.transpose4, (p_encoder_layers_1_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv19: R.Tensor((1, 1500, 1536), dtype="float32") = cls.fused_matmul1_add5_gelu2(lv71, lv72, p_encoder_layers_1_fc1_bias)
            lv76 = R.call_tir(cls.transpose5, (p_encoder_layers_1_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv20_1: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul2_add3_add4(lv19, lv76, p_encoder_layers_1_fc2_bias, lv18)
            lv80 = R.call_tir(cls.layer_norm, (lv20_1, p_encoder_layers_2_self_attn_layer_norm_weight, p_encoder_layers_2_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv81 = R.call_tir(cls.transpose1, (p_encoder_layers_2_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv21: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv80, lv81, p_encoder_layers_2_self_attn_q_proj_bias)
            lv22: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv21)
            lv86 = R.call_tir(cls.transpose1, (p_encoder_layers_2_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv87 = R.call_tir(cls.matmul, (lv80, lv86), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv23: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv87)
            lv90 = R.call_tir(cls.transpose1, (p_encoder_layers_2_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv24: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv80, lv90, p_encoder_layers_2_self_attn_v_proj_bias)
            lv25: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv24)
            lv98 = R.call_tir(cls.attention, (lv22, lv23, lv25), out_sinfo=R.Tensor((1, 1500, 6, 64), dtype="float32"))
            lv26: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_transpose2_transpose3_reshape2(lv98)
            lv102 = R.call_tir(cls.transpose1, (p_encoder_layers_2_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv27: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3_add4(lv26, lv102, p_encoder_layers_2_self_attn_out_proj_bias, lv20_1)
            lv106 = R.call_tir(cls.layer_norm, (lv27, p_encoder_layers_2_final_layer_norm_weight, p_encoder_layers_2_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv107 = R.call_tir(cls.transpose4, (p_encoder_layers_2_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv28_1: R.Tensor((1, 1500, 1536), dtype="float32") = cls.fused_matmul1_add5_gelu2(lv106, lv107, p_encoder_layers_2_fc1_bias)
            lv111 = R.call_tir(cls.transpose5, (p_encoder_layers_2_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv29: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul2_add3_add4(lv28_1, lv111, p_encoder_layers_2_fc2_bias, lv27)
            lv115 = R.call_tir(cls.layer_norm, (lv29, p_encoder_layers_3_self_attn_layer_norm_weight, p_encoder_layers_3_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv116 = R.call_tir(cls.transpose1, (p_encoder_layers_3_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv30: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv115, lv116, p_encoder_layers_3_self_attn_q_proj_bias)
            lv31: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv30)
            lv121 = R.call_tir(cls.transpose1, (p_encoder_layers_3_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv122 = R.call_tir(cls.matmul, (lv115, lv121), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv32_1: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv122)
            lv125 = R.call_tir(cls.transpose1, (p_encoder_layers_3_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv33: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv115, lv125, p_encoder_layers_3_self_attn_v_proj_bias)
            lv34: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv33)
            lv133 = R.call_tir(cls.attention, (lv31, lv32_1, lv34), out_sinfo=R.Tensor((1, 1500, 6, 64), dtype="float32"))
            lv35: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_transpose2_transpose3_reshape2(lv133)
            lv137 = R.call_tir(cls.transpose1, (p_encoder_layers_3_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv36_1: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3_add4(lv35, lv137, p_encoder_layers_3_self_attn_out_proj_bias, lv29)
            lv141 = R.call_tir(cls.layer_norm, (lv36_1, p_encoder_layers_3_final_layer_norm_weight, p_encoder_layers_3_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv142 = R.call_tir(cls.transpose4, (p_encoder_layers_3_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv37_1: R.Tensor((1, 1500, 1536), dtype="float32") = cls.fused_matmul1_add5_gelu2(lv141, lv142, p_encoder_layers_3_fc1_bias)
            lv146 = R.call_tir(cls.transpose5, (p_encoder_layers_3_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv38: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul2_add3_add4(lv37_1, lv146, p_encoder_layers_3_fc2_bias, lv36_1)
            lv150 = R.call_tir(cls.layer_norm, (lv38, p_encoder_layer_norm_weight, p_encoder_layer_norm_bias), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv154 = R.call_tir(cls.take, (p_decoder_embed_tokens_weight, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((1, 384), dtype="float32"))
            lv159 = R.call_tir(cls.index_tensor, (p_decoder_embed_positions_weight, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv39: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_reshape5_cast1_add6(lv154, lv159)
            lv162 = R.call_tir(cls.layer_norm1, (lv39, p_decoder_layers_0_self_attn_layer_norm_weight, p_decoder_layers_0_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv163 = R.call_tir(cls.transpose1, (p_decoder_layers_0_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv40: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv162, lv163, p_decoder_layers_0_self_attn_q_proj_bias)
            lv41_1: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv40)
            lv168 = R.call_tir(cls.transpose1, (p_decoder_layers_0_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv169 = R.call_tir(cls.matmul3, (lv162, lv168), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv42: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv169)
            lv172 = R.call_tir(cls.transpose1, (p_decoder_layers_0_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv43: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv162, lv172, p_decoder_layers_0_self_attn_v_proj_bias)
            lv44: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv43)
            lv180 = R.call_tir(cls.attention1, (lv41_1, lv42, lv44), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv45_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv180)
            lv184 = R.call_tir(cls.transpose1, (p_decoder_layers_0_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv46_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv45_1, lv184, p_decoder_layers_0_self_attn_out_proj_bias, lv39)
            lv188 = R.call_tir(cls.layer_norm1, (lv46_1, p_decoder_layers_0_encoder_attn_layer_norm_weight, p_decoder_layers_0_encoder_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv189 = R.call_tir(cls.transpose1, (p_decoder_layers_0_encoder_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv47: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv188, lv189, p_decoder_layers_0_encoder_attn_q_proj_bias)
            lv48: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv47)
            lv194 = R.call_tir(cls.transpose1, (p_decoder_layers_0_encoder_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv195 = R.call_tir(cls.matmul, (lv150, lv194), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv49: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv195)
            lv198 = R.call_tir(cls.transpose1, (p_decoder_layers_0_encoder_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv50: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv150, lv198, p_decoder_layers_0_encoder_attn_v_proj_bias)
            lv51_1: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv50)
            lv206 = R.call_tir(cls.attention2, (lv48, lv49, lv51_1), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv52_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv206)
            lv210 = R.call_tir(cls.transpose1, (p_decoder_layers_0_encoder_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv53: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv52_1, lv210, p_decoder_layers_0_encoder_attn_out_proj_bias, lv46_1)
            lv214 = R.call_tir(cls.layer_norm1, (lv53, p_decoder_layers_0_final_layer_norm_weight, p_decoder_layers_0_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv215 = R.call_tir(cls.transpose4, (p_decoder_layers_0_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv54: R.Tensor((1, 1, 1536), dtype="float32") = cls.fused_matmul4_add8_gelu3(lv214, lv215, p_decoder_layers_0_fc1_bias)
            lv219 = R.call_tir(cls.transpose5, (p_decoder_layers_0_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv55_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul5_add7_add6(lv54, lv219, p_decoder_layers_0_fc2_bias, lv53)
            lv223 = R.call_tir(cls.layer_norm1, (lv55_1, p_decoder_layers_1_self_attn_layer_norm_weight, p_decoder_layers_1_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv224 = R.call_tir(cls.transpose1, (p_decoder_layers_1_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv56: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv223, lv224, p_decoder_layers_1_self_attn_q_proj_bias)
            lv57: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv56)
            lv229 = R.call_tir(cls.transpose1, (p_decoder_layers_1_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv230 = R.call_tir(cls.matmul3, (lv223, lv229), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv58: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv230)
            lv233 = R.call_tir(cls.transpose1, (p_decoder_layers_1_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv59: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv223, lv233, p_decoder_layers_1_self_attn_v_proj_bias)
            lv60: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv59)
            lv241 = R.call_tir(cls.attention1, (lv57, lv58, lv60), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv61: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv241)
            lv245 = R.call_tir(cls.transpose1, (p_decoder_layers_1_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv62: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv61, lv245, p_decoder_layers_1_self_attn_out_proj_bias, lv55_1)
            lv249 = R.call_tir(cls.layer_norm1, (lv62, p_decoder_layers_1_encoder_attn_layer_norm_weight, p_decoder_layers_1_encoder_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv250 = R.call_tir(cls.transpose1, (p_decoder_layers_1_encoder_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv63_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv249, lv250, p_decoder_layers_1_encoder_attn_q_proj_bias)
            lv64: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv63_1)
            lv255 = R.call_tir(cls.transpose1, (p_decoder_layers_1_encoder_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv256 = R.call_tir(cls.matmul, (lv150, lv255), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv65: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv256)
            lv259 = R.call_tir(cls.transpose1, (p_decoder_layers_1_encoder_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv66: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv150, lv259, p_decoder_layers_1_encoder_attn_v_proj_bias)
            lv67_1: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv66)
            lv267 = R.call_tir(cls.attention2, (lv64, lv65, lv67_1), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv68: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv267)
            lv271 = R.call_tir(cls.transpose1, (p_decoder_layers_1_encoder_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv69: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv68, lv271, p_decoder_layers_1_encoder_attn_out_proj_bias, lv62)
            lv275 = R.call_tir(cls.layer_norm1, (lv69, p_decoder_layers_1_final_layer_norm_weight, p_decoder_layers_1_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv276 = R.call_tir(cls.transpose4, (p_decoder_layers_1_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv70: R.Tensor((1, 1, 1536), dtype="float32") = cls.fused_matmul4_add8_gelu3(lv275, lv276, p_decoder_layers_1_fc1_bias)
            lv280 = R.call_tir(cls.transpose5, (p_decoder_layers_1_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv71_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul5_add7_add6(lv70, lv280, p_decoder_layers_1_fc2_bias, lv69)
            lv284 = R.call_tir(cls.layer_norm1, (lv71_1, p_decoder_layers_2_self_attn_layer_norm_weight, p_decoder_layers_2_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv285 = R.call_tir(cls.transpose1, (p_decoder_layers_2_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv72_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv284, lv285, p_decoder_layers_2_self_attn_q_proj_bias)
            lv73: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv72_1)
            lv290 = R.call_tir(cls.transpose1, (p_decoder_layers_2_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv291 = R.call_tir(cls.matmul3, (lv284, lv290), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv74: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv291)
            lv294 = R.call_tir(cls.transpose1, (p_decoder_layers_2_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv75: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv284, lv294, p_decoder_layers_2_self_attn_v_proj_bias)
            lv76_1: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv75)
            lv302 = R.call_tir(cls.attention1, (lv73, lv74, lv76_1), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv77: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv302)
            lv306 = R.call_tir(cls.transpose1, (p_decoder_layers_2_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv78: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv77, lv306, p_decoder_layers_2_self_attn_out_proj_bias, lv71_1)
            lv310 = R.call_tir(cls.layer_norm1, (lv78, p_decoder_layers_2_encoder_attn_layer_norm_weight, p_decoder_layers_2_encoder_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv311 = R.call_tir(cls.transpose1, (p_decoder_layers_2_encoder_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv79: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv310, lv311, p_decoder_layers_2_encoder_attn_q_proj_bias)
            lv80_1: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv79)
            lv316 = R.call_tir(cls.transpose1, (p_decoder_layers_2_encoder_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv317 = R.call_tir(cls.matmul, (lv150, lv316), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv81_1: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv317)
            lv320 = R.call_tir(cls.transpose1, (p_decoder_layers_2_encoder_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv82: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv150, lv320, p_decoder_layers_2_encoder_attn_v_proj_bias)
            lv83: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv82)
            lv328 = R.call_tir(cls.attention2, (lv80_1, lv81_1, lv83), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv84: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv328)
            lv332 = R.call_tir(cls.transpose1, (p_decoder_layers_2_encoder_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv85: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv84, lv332, p_decoder_layers_2_encoder_attn_out_proj_bias, lv78)
            lv336 = R.call_tir(cls.layer_norm1, (lv85, p_decoder_layers_2_final_layer_norm_weight, p_decoder_layers_2_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv337 = R.call_tir(cls.transpose4, (p_decoder_layers_2_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv86_1: R.Tensor((1, 1, 1536), dtype="float32") = cls.fused_matmul4_add8_gelu3(lv336, lv337, p_decoder_layers_2_fc1_bias)
            lv341 = R.call_tir(cls.transpose5, (p_decoder_layers_2_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv87_1: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul5_add7_add6(lv86_1, lv341, p_decoder_layers_2_fc2_bias, lv85)
            lv345 = R.call_tir(cls.layer_norm1, (lv87_1, p_decoder_layers_3_self_attn_layer_norm_weight, p_decoder_layers_3_self_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv346 = R.call_tir(cls.transpose1, (p_decoder_layers_3_self_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv88: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv345, lv346, p_decoder_layers_3_self_attn_q_proj_bias)
            lv89: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv88)
            lv351 = R.call_tir(cls.transpose1, (p_decoder_layers_3_self_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv352 = R.call_tir(cls.matmul3, (lv345, lv351), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv90_1: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv352)
            lv355 = R.call_tir(cls.transpose1, (p_decoder_layers_3_self_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv91: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv345, lv355, p_decoder_layers_3_self_attn_v_proj_bias)
            lv92: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv91)
            lv363 = R.call_tir(cls.attention1, (lv89, lv90_1, lv92), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv93: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv363)
            lv367 = R.call_tir(cls.transpose1, (p_decoder_layers_3_self_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv94: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv93, lv367, p_decoder_layers_3_self_attn_out_proj_bias, lv87_1)
            lv371 = R.call_tir(cls.layer_norm1, (lv94, p_decoder_layers_3_encoder_attn_layer_norm_weight, p_decoder_layers_3_encoder_attn_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv372 = R.call_tir(cls.transpose1, (p_decoder_layers_3_encoder_attn_q_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv95: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7(lv371, lv372, p_decoder_layers_3_encoder_attn_q_proj_bias)
            lv96: R.Tensor((1, 1, 6, 64), dtype="float32") = cls.fused_reshape6_transpose6_transpose7(lv95)
            lv377 = R.call_tir(cls.transpose1, (p_decoder_layers_3_encoder_attn_k_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv378 = R.call_tir(cls.matmul, (lv150, lv377), out_sinfo=R.Tensor((1, 1500, 384), dtype="float32"))
            lv97: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv378)
            lv381 = R.call_tir(cls.transpose1, (p_decoder_layers_3_encoder_attn_v_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv98_1: R.Tensor((1, 1500, 384), dtype="float32") = cls.fused_matmul_add3(lv150, lv381, p_decoder_layers_3_encoder_attn_v_proj_bias)
            lv99: R.Tensor((1, 1500, 6, 64), dtype="float32") = cls.fused_reshape1_transpose2_transpose3(lv98_1)
            lv389 = R.call_tir(cls.attention2, (lv96, lv97, lv99), out_sinfo=R.Tensor((1, 1, 6, 64), dtype="float32"))
            lv100: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_transpose6_transpose7_reshape7(lv389)
            lv393 = R.call_tir(cls.transpose1, (p_decoder_layers_3_encoder_attn_out_proj_weight,), out_sinfo=R.Tensor((384, 384), dtype="float32"))
            lv101: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul3_add7_add6(lv100, lv393, p_decoder_layers_3_encoder_attn_out_proj_bias, lv94)
            lv397 = R.call_tir(cls.layer_norm1, (lv101, p_decoder_layers_3_final_layer_norm_weight, p_decoder_layers_3_final_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            lv398 = R.call_tir(cls.transpose4, (p_decoder_layers_3_fc1_weight,), out_sinfo=R.Tensor((384, 1536), dtype="float32"))
            lv102_1: R.Tensor((1, 1, 1536), dtype="float32") = cls.fused_matmul4_add8_gelu3(lv397, lv398, p_decoder_layers_3_fc1_bias)
            lv402 = R.call_tir(cls.transpose5, (p_decoder_layers_3_fc2_weight,), out_sinfo=R.Tensor((1536, 384), dtype="float32"))
            lv103: R.Tensor((1, 1, 384), dtype="float32") = cls.fused_matmul5_add7_add6(lv102_1, lv402, p_decoder_layers_3_fc2_bias, lv101)
            lv406 = R.call_tir(cls.layer_norm1, (lv103, p_decoder_layer_norm_weight, p_decoder_layer_norm_bias), out_sinfo=R.Tensor((1, 1, 384), dtype="float32"))
            gv: R.Tuple(R.Tensor((1, 1, 384), dtype="float32")) = (lv406,)
            R.output(gv)
        return gv

# Metadata omitted. Use show_meta=True in script() method to show it.
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul5_add7_add6(lv218: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32"), lv219: T.Buffer((T.int64(1536), T.int64(384)), "float32"), p_decoder_layers_0_fc2_bias: T.Buffer((T.int64(384),), "float32"), lv213: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), T_add_intermediate_1: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 24)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 16)
    matmul_local = T.allocate([1], "float32", "local")
    matmul_rf_local = T.allocate([1], "float32", "local")
    matmul_intermediate_rf_local = T.Buffer((T.int64(1),), data=matmul_rf_local, scope="local")
    matmul_intermediate_rf_local[0] = T.float32(0.0)
    for ax1_fused_0 in range(96):
        lv218_1 = T.Buffer((T.int64(1536),), data=lv218.data)
        lv219_1 = T.Buffer((T.int64(589824),), data=lv219.data)
        matmul_intermediate_rf_local[0] = matmul_intermediate_rf_local[0] + lv218_1[ax1_fused_0 * 16 + threadIdx_y] * lv219_1[ax1_fused_0 * 6144 + threadIdx_y * 384 + blockIdx_x * 16 + threadIdx_x]
    matmul_intermediate_local = T.Buffer((T.int64(1),), data=matmul_local, scope="local")
    with T.allocate([1], "float32", "local") as cross_thread_matmul_intermediate_local:
        cross_thread_matmul_intermediate_local_1 = T.Buffer((1,), data=cross_thread_matmul_intermediate_local, scope="local")
        with T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), matmul_intermediate_rf_local[0], T.bool(True), cross_thread_matmul_intermediate_local_1[0], threadIdx_y)
        matmul_intermediate_local[0] = cross_thread_matmul_intermediate_local_1[0]
    if threadIdx_y == 0:
        T_add_intermediate_1_1 = T.Buffer((T.int64(384),), data=T_add_intermediate_1.data)
        lv213_1 = T.Buffer((T.int64(384),), data=lv213.data)
        p_decoder_layers_0_fc2_bias_1 = T.Buffer((T.int64(384),), data=p_decoder_layers_0_fc2_bias.data)
        T_add_intermediate_1_1[blockIdx_x * 16 + threadIdx_x] = lv213_1[blockIdx_x * 16 + threadIdx_x] + (matmul_intermediate_local[0] + p_decoder_layers_0_fc2_bias_1[blockIdx_x * 16 + threadIdx_x])
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_reshape1_transpose2_transpose3(lv13: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), T_transpose_intermediate_1: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 563)
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if blockIdx_x * 1024 + threadIdx_x < 576000:
        T_transpose_intermediate_1_1 = T.Buffer((T.int64(576000),), data=T_transpose_intermediate_1.data)
        lv13_1 = T.Buffer((T.int64(576000),), data=lv13.data)
        T_transpose_intermediate_1_1[(blockIdx_x * 1024 + threadIdx_x) // 384 * 384 + (blockIdx_x * 256 + threadIdx_x) % 384 // 64 * 64 + threadIdx_x % 64] = lv13_1[(blockIdx_x * 1024 + threadIdx_x) // 384 * 384 + (blockIdx_x * 256 + threadIdx_x) % 384 // 64 * 64 + threadIdx_x % 64]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def layer_norm(lv9: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), p_encoder_layers_0_self_attn_layer_norm_weight: T.Buffer((T.int64(384),), "float32"), p_encoder_layers_0_self_attn_layer_norm_bias: T.Buffer((T.int64(384),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 4, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 1500)
    threadIdx_x = T.launch_thread("threadIdx.x", 256)
    lv9_red_temp_v0_shared = T.allocate([1], "float32", "shared")
    lv9_red_temp_v1_shared = T.allocate([1], "float32", "shared")
    lv9_1 = T.Buffer((T.int64(576000),), data=lv9.data)
    lv9_red_temp_v0_shared_1 = T.Buffer((T.int64(1),), data=lv9_red_temp_v0_shared, scope="shared")
    lv9_red_temp_v1_shared_1 = T.Buffer((T.int64(1),), data=lv9_red_temp_v1_shared, scope="shared")
    with T.allocate([1], "float32", "local") as in_thread_lv9_red_temp_v0_shared:
        in_thread_lv9_red_temp_v1_shared = T.allocate([1], "float32", "local")
        cross_thread_lv9_red_temp_v0_shared = T.allocate([1], "float32", "local")
        cross_thread_lv9_red_temp_v1_shared = T.allocate([1], "float32", "local")
        in_thread_lv9_red_temp_v0_shared_1 = T.Buffer((1,), data=in_thread_lv9_red_temp_v0_shared, scope="local")
        in_thread_lv9_red_temp_v0_shared_1[0] = T.float32(0.0)
        in_thread_lv9_red_temp_v1_shared_1 = T.Buffer((1,), data=in_thread_lv9_red_temp_v1_shared, scope="local")
        in_thread_lv9_red_temp_v1_shared_1[0] = T.float32(0.0)
        ax1_fused_0 = T.int64()
        with T.attr(ax1_fused_0, "pragma_auto_unroll_max_step", 256):
            T.attr(ax1_fused_0, "pragma_unroll_explicit", 1)
            for ax1_fused_0_1 in range(2):
                if ax1_fused_0_1 * 256 + threadIdx_x < 384:
                    v_lv9_red_temp_v0: T.float32 = in_thread_lv9_red_temp_v0_shared_1[0] + lv9_1[blockIdx_x * 384 + (ax1_fused_0_1 * 256 + threadIdx_x)]
                    v_lv9_red_temp_v1: T.float32 = in_thread_lv9_red_temp_v1_shared_1[0] + lv9_1[blockIdx_x * 384 + (ax1_fused_0_1 * 256 + threadIdx_x)] * lv9_1[blockIdx_x * 384 + (ax1_fused_0_1 * 256 + threadIdx_x)]
                    in_thread_lv9_red_temp_v0_shared_1[0] = v_lv9_red_temp_v0
                    in_thread_lv9_red_temp_v1_shared_1[0] = v_lv9_red_temp_v1
        cross_thread_lv9_red_temp_v0_shared_1 = T.Buffer((1,), data=cross_thread_lv9_red_temp_v0_shared, scope="local")
        cross_thread_lv9_red_temp_v1_shared_1 = T.Buffer((1,), data=cross_thread_lv9_red_temp_v1_shared, scope="local")
        with T.attr(T.comm_reducer(lambda x0, x1, y0, y1: (x0 + y0, x1 + y1), [T.float32(0.0), T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(2), in_thread_lv9_red_temp_v0_shared_1[0], in_thread_lv9_red_temp_v1_shared_1[0], T.bool(True), cross_thread_lv9_red_temp_v0_shared_1[0], cross_thread_lv9_red_temp_v1_shared_1[0], threadIdx_x)
        if threadIdx_x == 0:
            lv9_red_temp_v0_shared_1[0] = cross_thread_lv9_red_temp_v0_shared_1[0]
            lv9_red_temp_v1_shared_1[0] = cross_thread_lv9_red_temp_v1_shared_1[0]
    ax1_0 = T.int64()
    T.attr(ax1_0, "pragma_auto_unroll_max_step", 256)
    T.attr(ax1_0, "pragma_unroll_explicit", 1)
    for ax1_0_1 in range(2):
        if ax1_0_1 * 256 + threadIdx_x < 384:
            T_layer_norm_1 = T.Buffer((T.int64(576000),), data=T_layer_norm.data)
            p_encoder_layers_0_self_attn_layer_norm_weight_1 = T.Buffer((T.int64(384),), data=p_encoder_layers_0_self_attn_layer_norm_weight.data)
            p_encoder_layers_0_self_attn_layer_norm_bias_1 = T.Buffer((T.int64(384),), data=p_encoder_layers_0_self_attn_layer_norm_bias.data)
            T_layer_norm_1[blockIdx_x * 384 + (ax1_0_1 * 256 + threadIdx_x)] = (lv9_1[blockIdx_x * 384 + (ax1_0_1 * 256 + threadIdx_x)] - lv9_red_temp_v0_shared_1[0] * T.float32(0.0026041666666666665)) * T.rsqrt(lv9_red_temp_v1_shared_1[0] * T.float32(0.0026041666666666665) - lv9_red_temp_v0_shared_1[0] * T.float32(0.0026041666666666665) * (lv9_red_temp_v0_shared_1[0] * T.float32(0.0026041666666666665)) + T.float32(1.0000000000000001e-05)) * p_encoder_layers_0_self_attn_layer_norm_weight_1[ax1_0_1 * 256 + threadIdx_x] + p_encoder_layers_0_self_attn_layer_norm_bias_1[ax1_0_1 * 256 + threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def index_tensor(p_decoder_embed_positions_weight: T.Buffer((T.int64(448), T.int64(384)), "float32"), lv158: T.Buffer((T.int64(1), T.int64(1)), "int64"), T_take: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 8, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if threadIdx_x < 384:
        T_take_1 = T.Buffer((T.int64(384),), data=T_take.data)
        p_decoder_embed_positions_weight_1 = T.Buffer((T.int64(172032),), data=p_decoder_embed_positions_weight.data)
        lv158_1 = T.Buffer((T.int64(1),), "int64", data=lv158.data)
        T_take_1[threadIdx_x] = p_decoder_embed_positions_weight_1[T.Cast("int64", threadIdx_x) + T.min(T.max(T.int64(0), lv158_1[0]), T.int64(447)) * T.int64(384)]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_reshape5_cast1_add6(lv154: T.Buffer((T.int64(1), T.int64(384)), "float32"), lv159: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), T_add_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if threadIdx_x < 384:
        T_add_intermediate_1 = T.Buffer((T.int64(384),), data=T_add_intermediate.data)
        lv154_1 = T.Buffer((T.int64(384),), data=lv154.data)
        lv159_1 = T.Buffer((T.int64(384),), data=lv159.data)
        T_add_intermediate_1[threadIdx_x] = lv154_1[threadIdx_x] + lv159_1[threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def take(p_decoder_embed_tokens_weight: T.Buffer((T.int64(51865), T.int64(384)), "float32"), lv153: T.Buffer((T.int64(1),), "int32"), T_take: T.Buffer((T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 8, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", T.int64(1024))
    if threadIdx_x < T.int64(384):
        T_take_1 = T.Buffer((T.int64(384),), data=T_take.data)
        p_decoder_embed_tokens_weight_1 = T.Buffer((T.int64(19916160),), data=p_decoder_embed_tokens_weight.data)
        lv153_1 = T.Buffer((T.int64(1),), "int32", data=lv153.data)
        T_take_1[threadIdx_x] = p_decoder_embed_tokens_weight_1[threadIdx_x + T.Cast("int64", lv153_1[0]) * T.int64(384)]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul3_add7_add6(lv183: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), lv184: T.Buffer((T.int64(384), T.int64(384)), "float32"), p_decoder_layers_0_self_attn_out_proj_bias: T.Buffer((T.int64(384),), "float32"), lv161: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), T_add_intermediate_1: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 24)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 16)
    matmul_local = T.allocate([1], "float32", "local")
    matmul_rf_local = T.allocate([1], "float32", "local")
    matmul_intermediate_rf_local = T.Buffer((T.int64(1),), data=matmul_rf_local, scope="local")
    matmul_intermediate_rf_local[0] = T.float32(0.0)
    for ax1_fused_0 in range(24):
        lv183_1 = T.Buffer((T.int64(384),), data=lv183.data)
        lv184_1 = T.Buffer((T.int64(147456),), data=lv184.data)
        matmul_intermediate_rf_local[0] = matmul_intermediate_rf_local[0] + lv183_1[ax1_fused_0 * 16 + threadIdx_y] * lv184_1[ax1_fused_0 * 6144 + threadIdx_y * 384 + blockIdx_x * 16 + threadIdx_x]
    matmul_intermediate_local = T.Buffer((T.int64(1),), data=matmul_local, scope="local")
    with T.allocate([1], "float32", "local") as cross_thread_matmul_intermediate_local:
        cross_thread_matmul_intermediate_local_1 = T.Buffer((1,), data=cross_thread_matmul_intermediate_local, scope="local")
        with T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), matmul_intermediate_rf_local[0], T.bool(True), cross_thread_matmul_intermediate_local_1[0], threadIdx_y)
        matmul_intermediate_local[0] = cross_thread_matmul_intermediate_local_1[0]
    if threadIdx_y == 0:
        T_add_intermediate_1_1 = T.Buffer((T.int64(384),), data=T_add_intermediate_1.data)
        lv161_1 = T.Buffer((T.int64(384),), data=lv161.data)
        p_decoder_layers_0_self_attn_out_proj_bias_1 = T.Buffer((T.int64(384),), data=p_decoder_layers_0_self_attn_out_proj_bias.data)
        T_add_intermediate_1_1[blockIdx_x * 16 + threadIdx_x] = lv161_1[blockIdx_x * 16 + threadIdx_x] + (matmul_intermediate_local[0] + p_decoder_layers_0_self_attn_out_proj_bias_1[blockIdx_x * 16 + threadIdx_x])
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul_add3(lv10: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), lv11: T.Buffer((T.int64(384), T.int64(384)), "float32"), p_encoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(384),), "float32"), T_add_intermediate: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 6)
    blockIdx_x = T.launch_thread("blockIdx.x", 47)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 8)
    var = T.int64()
    T.attr(var, "pragma_auto_unroll_max_step", 256)
    T.attr(var, "pragma_unroll_explicit", 1)
    for var in range(T.int64(1)):
        matmul_intermediate_reindex_pad_local = T.allocate([16], "float32", "local")
        matmul_intermediate_reindex_pad_local_1 = T.Buffer((T.int64(16),), data=matmul_intermediate_reindex_pad_local, scope="local")
        for ax1_3_init, ax2_3_0_init in T.grid(4, 2):
            matmul_intermediate_reindex_pad_local_1[ax1_3_init * 4 + ax2_3_0_init * 2:ax1_3_init * 4 + ax2_3_0_init * 2 + 2] = T.Broadcast(T.float32(0.0), 2)
        for ax3_0 in range(24):
            lv10_reindex_pad_shared = T.allocate([576], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv11_reindex_shared = T.allocate([1152], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv10_reindex_pad_shared_1 = T.Buffer((T.int64(576),), data=lv10_reindex_pad_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(2):
                lv10_1 = T.Buffer((T.int64(576000),), data=lv10.data)
                lv10_reindex_pad_shared_1[T.Broadcast(threadIdx_y * 36 + threadIdx_x // 4 * 18, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = T.if_then_else(blockIdx_x * 16 + threadIdx_y < 750, lv10_1[T.Broadcast(blockIdx_x * 12288 + threadIdx_y * 768 + threadIdx_x // 4 * 384 + ax3_0 * 16, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)], T.Broadcast(T.float32(0.0), 2))
            lv11_reindex_shared_1 = T.Buffer((T.int64(1152),), data=lv11_reindex_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(4):
                lv11_1 = T.Buffer((T.int64(147456),), data=lv11.data)
                lv11_reindex_shared_1[T.Broadcast(threadIdx_y * 72 + threadIdx_x // 2 * 18, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = lv11_1[T.Broadcast(ax3_0 * 6144, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2) * T.Broadcast(384, 2) + T.Broadcast(blockIdx_y * 64, 2) + T.Broadcast(threadIdx_y * 4, 2) + T.Broadcast(threadIdx_x // 2, 2)]
            for ax3_1, ax1_3, ax2_3_0 in T.grid(16, 4, 2):
                matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] = matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] + T.Broadcast(lv10_reindex_pad_shared_1[threadIdx_x * 72 + ax1_3 * 18 + ax3_1], 2) * lv11_reindex_shared_1[threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1:threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1 + 36:18]
        for ax1, ax2_0 in T.grid(4, 2):
            if blockIdx_x * 8 + threadIdx_x < 375:
                T_add_intermediate_1 = T.Buffer((T.int64(576000),), data=T_add_intermediate.data)
                p_encoder_layers_0_self_attn_q_proj_bias_1 = T.Buffer((T.int64(384),), data=p_encoder_layers_0_self_attn_q_proj_bias.data)
                T_add_intermediate_1[(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2] = matmul_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2:ax1 * 4 + ax2_0 * 2 + 2] + p_encoder_layers_0_self_attn_q_proj_bias_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul_add3_add4(lv31: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), lv32: T.Buffer((T.int64(384), T.int64(384)), "float32"), p_encoder_layers_0_self_attn_out_proj_bias: T.Buffer((T.int64(384),), "float32"), lv9: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), T_add_intermediate_1: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 6)
    blockIdx_x = T.launch_thread("blockIdx.x", 47)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 8)
    var = T.int64()
    T.attr(var, "pragma_auto_unroll_max_step", 256)
    T.attr(var, "pragma_unroll_explicit", 1)
    for var in range(T.int64(1)):
        matmul_intermediate_reindex_pad_local = T.allocate([16], "float32", "local")
        matmul_intermediate_reindex_pad_local_1 = T.Buffer((T.int64(16),), data=matmul_intermediate_reindex_pad_local, scope="local")
        for ax1_3_init, ax2_3_0_init in T.grid(4, 2):
            matmul_intermediate_reindex_pad_local_1[ax1_3_init * 4 + ax2_3_0_init * 2:ax1_3_init * 4 + ax2_3_0_init * 2 + 2] = T.Broadcast(T.float32(0.0), 2)
        for ax3_0 in range(24):
            lv31_reindex_pad_shared = T.allocate([576], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv32_reindex_shared = T.allocate([1152], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv31_reindex_pad_shared_1 = T.Buffer((T.int64(576),), data=lv31_reindex_pad_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(2):
                lv31_1 = T.Buffer((T.int64(576000),), data=lv31.data)
                lv31_reindex_pad_shared_1[T.Broadcast(threadIdx_y * 36 + threadIdx_x // 4 * 18, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = T.if_then_else(blockIdx_x * 16 + threadIdx_y < 750, lv31_1[T.Broadcast(blockIdx_x * 12288 + threadIdx_y * 768 + threadIdx_x // 4 * 384 + ax3_0 * 16, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)], T.Broadcast(T.float32(0.0), 2))
            lv32_reindex_shared_1 = T.Buffer((T.int64(1152),), data=lv32_reindex_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(4):
                lv32_1 = T.Buffer((T.int64(147456),), data=lv32.data)
                lv32_reindex_shared_1[T.Broadcast(threadIdx_y * 72 + threadIdx_x // 2 * 18, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = lv32_1[T.Broadcast(ax3_0 * 6144, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2) * T.Broadcast(384, 2) + T.Broadcast(blockIdx_y * 64, 2) + T.Broadcast(threadIdx_y * 4, 2) + T.Broadcast(threadIdx_x // 2, 2)]
            for ax3_1, ax1_3, ax2_3_0 in T.grid(16, 4, 2):
                matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] = matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] + T.Broadcast(lv31_reindex_pad_shared_1[threadIdx_x * 72 + ax1_3 * 18 + ax3_1], 2) * lv32_reindex_shared_1[threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1:threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1 + 36:18]
        for ax1, ax2_0 in T.grid(4, 2):
            if blockIdx_x * 8 + threadIdx_x < 375:
                T_add_intermediate_1_1 = T.Buffer((T.int64(576000),), data=T_add_intermediate_1.data)
                lv9_1 = T.Buffer((T.int64(576000),), data=lv9.data)
                p_encoder_layers_0_self_attn_out_proj_bias_1 = T.Buffer((T.int64(384),), data=p_encoder_layers_0_self_attn_out_proj_bias.data)
                T_add_intermediate_1_1[(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2] = lv9_1[(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2] + (matmul_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2:ax1 * 4 + ax2_0 * 2 + 2] + p_encoder_layers_0_self_attn_out_proj_bias_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2])
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def transpose5(p_encoder_layers_0_fc2_weight: T.Buffer((T.int64(384), T.int64(1536)), "float32"), T_transpose: T.Buffer((T.int64(1536), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 2, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 576)
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    T_transpose_1 = T.Buffer((T.int64(589824),), data=T_transpose.data)
    p_encoder_layers_0_fc2_weight_1 = T.Buffer((T.int64(589824),), data=p_encoder_layers_0_fc2_weight.data)
    T_transpose_1[blockIdx_x * 1024 + threadIdx_x] = p_encoder_layers_0_fc2_weight_1[(blockIdx_x * 256 + threadIdx_x) % 384 * 1536 + (blockIdx_x * 1024 + threadIdx_x) // 384]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def transpose1(p_encoder_layers_0_self_attn_q_proj_weight: T.Buffer((T.int64(384), T.int64(384)), "float32"), T_transpose: T.Buffer((T.int64(384), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 2, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 144)
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    T_transpose_1 = T.Buffer((T.int64(147456),), data=T_transpose.data)
    p_encoder_layers_0_self_attn_q_proj_weight_1 = T.Buffer((T.int64(147456),), data=p_encoder_layers_0_self_attn_q_proj_weight.data)
    T_transpose_1[blockIdx_x * 1024 + threadIdx_x] = p_encoder_layers_0_self_attn_q_proj_weight_1[(blockIdx_x * 256 + threadIdx_x) % 384 * 384 + (blockIdx_x * 1024 + threadIdx_x) // 384]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul2_add3_add4(lv40: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32"), lv41: T.Buffer((T.int64(1536), T.int64(384)), "float32"), p_encoder_layers_0_fc2_bias: T.Buffer((T.int64(384),), "float32"), lv35: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), T_add_intermediate_1: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 6)
    blockIdx_x = T.launch_thread("blockIdx.x", 47)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 8)
    var = T.int64()
    T.attr(var, "pragma_auto_unroll_max_step", 256)
    T.attr(var, "pragma_unroll_explicit", 1)
    for var in range(T.int64(1)):
        matmul_intermediate_reindex_pad_local = T.allocate([16], "float32", "local")
        matmul_intermediate_reindex_pad_local_1 = T.Buffer((T.int64(16),), data=matmul_intermediate_reindex_pad_local, scope="local")
        for ax1_3_init, ax2_3_0_init in T.grid(4, 2):
            matmul_intermediate_reindex_pad_local_1[ax1_3_init * 4 + ax2_3_0_init * 2:ax1_3_init * 4 + ax2_3_0_init * 2 + 2] = T.Broadcast(T.float32(0.0), 2)
        for ax3_0 in range(96):
            lv40_reindex_pad_shared = T.allocate([576], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv41_reindex_shared = T.allocate([1152], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv40_reindex_pad_shared_1 = T.Buffer((T.int64(576),), data=lv40_reindex_pad_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(2):
                lv40_1 = T.Buffer((T.int64(2304000),), data=lv40.data)
                lv40_reindex_pad_shared_1[T.Broadcast(threadIdx_y * 36 + threadIdx_x // 4 * 18, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = T.if_then_else(blockIdx_x * 16 + threadIdx_y < 750, lv40_1[T.Broadcast(blockIdx_x * 49152 + threadIdx_y * 3072 + threadIdx_x // 4 * 1536 + ax3_0 * 16, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)], T.Broadcast(T.float32(0.0), 2))
            lv41_reindex_shared_1 = T.Buffer((T.int64(1152),), data=lv41_reindex_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(4):
                lv41_1 = T.Buffer((T.int64(589824),), data=lv41.data)
                lv41_reindex_shared_1[T.Broadcast(threadIdx_y * 72 + threadIdx_x // 2 * 18, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = lv41_1[T.Broadcast(ax3_0 * 6144, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2) * T.Broadcast(384, 2) + T.Broadcast(blockIdx_y * 64, 2) + T.Broadcast(threadIdx_y * 4, 2) + T.Broadcast(threadIdx_x // 2, 2)]
            for ax3_1, ax1_3, ax2_3_0 in T.grid(16, 4, 2):
                matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] = matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] + T.Broadcast(lv40_reindex_pad_shared_1[threadIdx_x * 72 + ax1_3 * 18 + ax3_1], 2) * lv41_reindex_shared_1[threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1:threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1 + 36:18]
        for ax1, ax2_0 in T.grid(4, 2):
            if blockIdx_x * 8 + threadIdx_x < 375:
                T_add_intermediate_1_1 = T.Buffer((T.int64(576000),), data=T_add_intermediate_1.data)
                lv35_1 = T.Buffer((T.int64(576000),), data=lv35.data)
                p_encoder_layers_0_fc2_bias_1 = T.Buffer((T.int64(384),), data=p_encoder_layers_0_fc2_bias.data)
                T_add_intermediate_1_1[(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2] = lv35_1[(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2] + (matmul_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2:ax1 * 4 + ax2_0 * 2 + 2] + p_encoder_layers_0_fc2_bias_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2])
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul1_add5_gelu2(lv36: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), lv37: T.Buffer((T.int64(384), T.int64(1536)), "float32"), p_encoder_layers_0_fc1_bias: T.Buffer((T.int64(1536),), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1500), T.int64(1536)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 24)
    blockIdx_x = T.launch_thread("blockIdx.x", 47)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 8)
    var = T.int64()
    T.attr(var, "pragma_auto_unroll_max_step", 256)
    T.attr(var, "pragma_unroll_explicit", 1)
    for var in range(T.int64(1)):
        matmul_intermediate_reindex_pad_local = T.allocate([16], "float32", "local")
        matmul_intermediate_reindex_pad_local_1 = T.Buffer((T.int64(16),), data=matmul_intermediate_reindex_pad_local, scope="local")
        for ax1_3_init, ax2_3_0_init in T.grid(4, 2):
            matmul_intermediate_reindex_pad_local_1[ax1_3_init * 4 + ax2_3_0_init * 2:ax1_3_init * 4 + ax2_3_0_init * 2 + 2] = T.Broadcast(T.float32(0.0), 2)
        for ax3_0 in range(24):
            lv36_reindex_pad_shared = T.allocate([576], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv37_reindex_shared = T.allocate([1152], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv36_reindex_pad_shared_1 = T.Buffer((T.int64(576),), data=lv36_reindex_pad_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(2):
                lv36_1 = T.Buffer((T.int64(576000),), data=lv36.data)
                lv36_reindex_pad_shared_1[T.Broadcast(threadIdx_y * 36 + threadIdx_x // 4 * 18, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = T.if_then_else(blockIdx_x * 16 + threadIdx_y < 750, lv36_1[T.Broadcast(blockIdx_x * 12288 + threadIdx_y * 768 + threadIdx_x // 4 * 384 + ax3_0 * 16, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)], T.Broadcast(T.float32(0.0), 2))
            lv37_reindex_shared_1 = T.Buffer((T.int64(1152),), data=lv37_reindex_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(4):
                lv37_1 = T.Buffer((T.int64(589824),), data=lv37.data)
                lv37_reindex_shared_1[T.Broadcast(threadIdx_y * 72 + threadIdx_x // 2 * 18, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = lv37_1[T.Broadcast(ax3_0 * 24576, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2) * T.Broadcast(1536, 2) + T.Broadcast(blockIdx_y * 64, 2) + T.Broadcast(threadIdx_y * 4, 2) + T.Broadcast(threadIdx_x // 2, 2)]
            for ax3_1, ax1_3, ax2_3_0 in T.grid(16, 4, 2):
                matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] = matmul_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] + T.Broadcast(lv36_reindex_pad_shared_1[threadIdx_x * 72 + ax1_3 * 18 + ax3_1], 2) * lv37_reindex_shared_1[threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1:threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1 + 36:18]
        for ax1, ax2_0 in T.grid(4, 2):
            if blockIdx_x * 8 + threadIdx_x < 375:
                T_multiply_intermediate_1 = T.Buffer((T.int64(2304000),), data=T_multiply_intermediate.data)
                p_encoder_layers_0_fc1_bias_1 = T.Buffer((T.int64(1536),), data=p_encoder_layers_0_fc1_bias.data)
                for ax2_1_s in range(2):
                    T_multiply_intermediate_1[(blockIdx_x * 8 + threadIdx_x) * 6144 + ax1 * 1536 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + ax2_1_s] = (matmul_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2 + ax2_1_s] + p_encoder_layers_0_fc1_bias_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + ax2_1_s]) * (T.float32(0.5) + T.erf((matmul_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2 + ax2_1_s] + p_encoder_layers_0_fc1_bias_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + ax2_1_s]) * T.float32(0.70710678118654757)) * T.float32(0.5))
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def matmul(lv10: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32"), lv11: T.Buffer((T.int64(384), T.int64(384)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 4, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 6)
    blockIdx_x = T.launch_thread("blockIdx.x", 47)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 8)
    var = T.int64()
    T.attr(var, "pragma_auto_unroll_max_step", 256)
    T.attr(var, "pragma_unroll_explicit", 1)
    for var in range(T.int64(1)):
        matmul_reindex_pad_local = T.allocate([16], "float32", "local")
        matmul_reindex_pad_local_1 = T.Buffer((T.int64(16),), data=matmul_reindex_pad_local, scope="local")
        for ax1_3_init, ax2_3_0_init in T.grid(4, 2):
            matmul_reindex_pad_local_1[ax1_3_init * 4 + ax2_3_0_init * 2:ax1_3_init * 4 + ax2_3_0_init * 2 + 2] = T.Broadcast(T.float32(0.0), 2)
        for ax3_0 in range(24):
            lv10_reindex_pad_shared = T.allocate([576], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv11_reindex_shared = T.allocate([1152], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            lv10_reindex_pad_shared_1 = T.Buffer((T.int64(576),), data=lv10_reindex_pad_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(2):
                lv10_1 = T.Buffer((T.int64(576000),), data=lv10.data)
                lv10_reindex_pad_shared_1[T.Broadcast(threadIdx_y * 36 + threadIdx_x // 4 * 18, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = T.if_then_else(blockIdx_x * 16 + threadIdx_y < 750, lv10_1[T.Broadcast(blockIdx_x * 12288 + threadIdx_y * 768 + threadIdx_x // 4 * 384 + ax3_0 * 16, 2) + T.Ramp(threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)], T.Broadcast(T.float32(0.0), 2))
            lv11_reindex_shared_1 = T.Buffer((T.int64(1152),), data=lv11_reindex_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(4):
                lv11_1 = T.Buffer((T.int64(147456),), data=lv11.data)
                lv11_reindex_shared_1[T.Broadcast(threadIdx_y * 72 + threadIdx_x // 2 * 18, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = lv11_1[T.Broadcast(ax3_0 * 6144, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2) * T.Broadcast(384, 2) + T.Broadcast(blockIdx_y * 64, 2) + T.Broadcast(threadIdx_y * 4, 2) + T.Broadcast(threadIdx_x // 2, 2)]
            for ax3_1, ax1_3, ax2_3_0 in T.grid(16, 4, 2):
                matmul_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] = matmul_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] + T.Broadcast(lv10_reindex_pad_shared_1[threadIdx_x * 72 + ax1_3 * 18 + ax3_1], 2) * lv11_reindex_shared_1[threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1:threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1 + 36:18]
        for ax1, ax2_0 in T.grid(4, 2):
            if blockIdx_x * 8 + threadIdx_x < 375:
                matmul_1 = T.Buffer((T.int64(576000),), data=matmul.data)
                matmul_1[(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2:(blockIdx_x * 8 + threadIdx_x) * 1536 + ax1 * 384 + blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + 2] = matmul_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2:ax1 * 4 + ax2_0 * 2 + 2]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def reshape(p_encoder_conv1_bias: T.Buffer((T.int64(384),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32")):
    T.func_attr({"op_pattern": 2, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if threadIdx_x < 384:
        T_reshape_1 = T.Buffer((T.int64(384),), data=T_reshape.data)
        p_encoder_conv1_bias_1 = T.Buffer((T.int64(384),), data=p_encoder_conv1_bias.data)
        T_reshape_1[threadIdx_x] = p_encoder_conv1_bias_1[threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def layer_norm1(lv161: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), p_decoder_layers_0_self_attn_layer_norm_weight: T.Buffer((T.int64(384),), "float32"), p_decoder_layers_0_self_attn_layer_norm_bias: T.Buffer((T.int64(384),), "float32"), T_layer_norm: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 4, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 256)
    lv161_red_temp_v0_shared = T.allocate([1], "float32", "shared")
    lv161_red_temp_v1_shared = T.allocate([1], "float32", "shared")
    lv161_1 = T.Buffer((T.int64(384),), data=lv161.data)
    lv161_red_temp_v0_shared_1 = T.Buffer((T.int64(1),), data=lv161_red_temp_v0_shared, scope="shared")
    lv161_red_temp_v1_shared_1 = T.Buffer((T.int64(1),), data=lv161_red_temp_v1_shared, scope="shared")
    with T.allocate([1], "float32", "local") as in_thread_lv161_red_temp_v0_shared:
        in_thread_lv161_red_temp_v1_shared = T.allocate([1], "float32", "local")
        cross_thread_lv161_red_temp_v0_shared = T.allocate([1], "float32", "local")
        cross_thread_lv161_red_temp_v1_shared = T.allocate([1], "float32", "local")
        in_thread_lv161_red_temp_v0_shared_1 = T.Buffer((1,), data=in_thread_lv161_red_temp_v0_shared, scope="local")
        in_thread_lv161_red_temp_v0_shared_1[0] = T.float32(0.0)
        in_thread_lv161_red_temp_v1_shared_1 = T.Buffer((1,), data=in_thread_lv161_red_temp_v1_shared, scope="local")
        in_thread_lv161_red_temp_v1_shared_1[0] = T.float32(0.0)
        ax1_fused_0 = T.int64()
        with T.attr(ax1_fused_0, "pragma_auto_unroll_max_step", 256):
            T.attr(ax1_fused_0, "pragma_unroll_explicit", 1)
            for ax1_fused_0_1 in range(2):
                if ax1_fused_0_1 * 256 + threadIdx_x < 384:
                    v_lv161_red_temp_v0: T.float32 = in_thread_lv161_red_temp_v0_shared_1[0] + lv161_1[ax1_fused_0_1 * 256 + threadIdx_x]
                    v_lv161_red_temp_v1: T.float32 = in_thread_lv161_red_temp_v1_shared_1[0] + lv161_1[ax1_fused_0_1 * 256 + threadIdx_x] * lv161_1[ax1_fused_0_1 * 256 + threadIdx_x]
                    in_thread_lv161_red_temp_v0_shared_1[0] = v_lv161_red_temp_v0
                    in_thread_lv161_red_temp_v1_shared_1[0] = v_lv161_red_temp_v1
        cross_thread_lv161_red_temp_v0_shared_1 = T.Buffer((1,), data=cross_thread_lv161_red_temp_v0_shared, scope="local")
        cross_thread_lv161_red_temp_v1_shared_1 = T.Buffer((1,), data=cross_thread_lv161_red_temp_v1_shared, scope="local")
        with T.attr(T.comm_reducer(lambda x0, x1, y0, y1: (x0 + y0, x1 + y1), [T.float32(0.0), T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(2), in_thread_lv161_red_temp_v0_shared_1[0], in_thread_lv161_red_temp_v1_shared_1[0], T.bool(True), cross_thread_lv161_red_temp_v0_shared_1[0], cross_thread_lv161_red_temp_v1_shared_1[0], threadIdx_x)
        if threadIdx_x == 0:
            lv161_red_temp_v0_shared_1[0] = cross_thread_lv161_red_temp_v0_shared_1[0]
            lv161_red_temp_v1_shared_1[0] = cross_thread_lv161_red_temp_v1_shared_1[0]
    ax1_0 = T.int64()
    T.attr(ax1_0, "pragma_auto_unroll_max_step", 256)
    T.attr(ax1_0, "pragma_unroll_explicit", 1)
    for ax1_0_1 in range(2):
        if ax1_0_1 * 256 + threadIdx_x < 384:
            T_layer_norm_1 = T.Buffer((T.int64(384),), data=T_layer_norm.data)
            p_decoder_layers_0_self_attn_layer_norm_weight_1 = T.Buffer((T.int64(384),), data=p_decoder_layers_0_self_attn_layer_norm_weight.data)
            p_decoder_layers_0_self_attn_layer_norm_bias_1 = T.Buffer((T.int64(384),), data=p_decoder_layers_0_self_attn_layer_norm_bias.data)
            T_layer_norm_1[ax1_0_1 * 256 + threadIdx_x] = (lv161_1[ax1_0_1 * 256 + threadIdx_x] - lv161_red_temp_v0_shared_1[0] * T.float32(0.0026041666666666665)) * T.rsqrt(lv161_red_temp_v1_shared_1[0] * T.float32(0.0026041666666666665) - lv161_red_temp_v0_shared_1[0] * T.float32(0.0026041666666666665) * (lv161_red_temp_v0_shared_1[0] * T.float32(0.0026041666666666665)) + T.float32(1.0000000000000001e-05)) * p_decoder_layers_0_self_attn_layer_norm_weight_1[ax1_0_1 * 256 + threadIdx_x] + p_decoder_layers_0_self_attn_layer_norm_bias_1[ax1_0_1 * 256 + threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def attention2(lv203: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), lv204: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), lv205: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32")):
    T.func_attr({"op_pattern": 4, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    T_batch_matmul_NT = T.allocate([9000], "float32", "global")
    T_softmax_maxelem = T.allocate([6], "float32", "global")
    T_softmax_expsum = T.allocate([6], "float32", "global")
    T_batch_matmul_NN = T.allocate([384], "float32", "global")
    T_batch_matmul_NT_1 = T.Buffer((T.int64(9000),), data=T_batch_matmul_NT)
    with T.launch_thread("blockIdx.x", 9) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if blockIdx_x * 1024 + threadIdx_x < 9000:
            T_batch_matmul_NT_1[blockIdx_x * 1024 + threadIdx_x] = T.float32(0.0)
        for ax2 in range(64):
            if blockIdx_x * 1024 + threadIdx_x < 9000:
                lv203_1 = T.Buffer((T.int64(384),), data=lv203.data)
                lv204_1 = T.Buffer((T.int64(576000),), data=lv204.data)
                T_batch_matmul_NT_1[blockIdx_x * 1024 + threadIdx_x] = T_batch_matmul_NT_1[blockIdx_x * 1024 + threadIdx_x] + lv203_1[(blockIdx_x * 1024 + threadIdx_x) // 1500 * 64 + ax2] * lv204_1[(blockIdx_x * 1024 + threadIdx_x) % 1500 * 384 + (blockIdx_x * 1024 + threadIdx_x) // 1500 * 64 + ax2]
    T_softmax_maxelem_1 = T.Buffer((T.int64(6),), data=T_softmax_maxelem)
    with T.launch_thread("blockIdx.x", T.int64(1)) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if threadIdx_x < 6:
            T_softmax_maxelem_1[threadIdx_x] = T.float32(-340282346638528859811704183484516925440.0)
        for ax1 in range(1500):
            if threadIdx_x < 6:
                T_softmax_maxelem_1[threadIdx_x] = T.max(T_softmax_maxelem_1[threadIdx_x], T_batch_matmul_NT_1[threadIdx_x * 1500 + ax1] / T.sqrt(T.float32(64.0)))
    T_softmax_expsum_1 = T.Buffer((T.int64(6),), data=T_softmax_expsum)
    with T.launch_thread("blockIdx.x", T.int64(1)) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if threadIdx_x < 6:
            T_softmax_expsum_1[threadIdx_x] = T.float32(0.0)
        for ax1 in range(1500):
            if threadIdx_x < 6:
                T_softmax_expsum_1[threadIdx_x] = T_softmax_expsum_1[threadIdx_x] + T.exp(T_batch_matmul_NT_1[threadIdx_x * 1500 + ax1] / T.sqrt(T.float32(64.0)) - T_softmax_maxelem_1[threadIdx_x])
    T_batch_matmul_NN_1 = T.Buffer((T.int64(384),), data=T_batch_matmul_NN)
    with T.launch_thread("blockIdx.x", T.int64(1)) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if threadIdx_x < 384:
            T_batch_matmul_NN_1[threadIdx_x] = T.float32(0.0)
        for ax2 in range(1500):
            if threadIdx_x < 384:
                lv205_1 = T.Buffer((T.int64(576000),), data=lv205.data)
                T_batch_matmul_NN_1[threadIdx_x] = T_batch_matmul_NN_1[threadIdx_x] + T.exp(T_batch_matmul_NT_1[threadIdx_x // 64 * 1500 + ax2] / T.sqrt(T.float32(64.0)) - T_softmax_maxelem_1[threadIdx_x // 64]) / T_softmax_expsum_1[threadIdx_x // 64] * lv205_1[ax2 * 384 + threadIdx_x]
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if threadIdx_x < 384:
        T_transpose_1 = T.Buffer((T.int64(384),), data=T_transpose.data)
        T_transpose_1[threadIdx_x] = T_batch_matmul_NN_1[threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def transpose4(p_encoder_layers_0_fc1_weight: T.Buffer((T.int64(1536), T.int64(384)), "float32"), T_transpose: T.Buffer((T.int64(384), T.int64(1536)), "float32")):
    T.func_attr({"op_pattern": 2, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 576)
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    T_transpose_1 = T.Buffer((T.int64(589824),), data=T_transpose.data)
    p_encoder_layers_0_fc1_weight_1 = T.Buffer((T.int64(589824),), data=p_encoder_layers_0_fc1_weight.data)
    T_transpose_1[blockIdx_x * 1024 + threadIdx_x] = p_encoder_layers_0_fc1_weight_1[(blockIdx_x * 1024 + threadIdx_x) % 1536 * 384 + (blockIdx_x * 1024 + threadIdx_x) // 1536]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def attention(lv25: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), lv26: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), lv27: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32")):
    T.func_attr({"op_pattern": 4, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    T_batch_matmul_NT = T.allocate([13500000], "float32", "global")
    T_softmax_maxelem = T.allocate([9000], "float32", "global")
    T_softmax_expsum = T.allocate([9000], "float32", "global")
    T_batch_matmul_NN = T.allocate([576000], "float32", "global")
    T_batch_matmul_NT_1 = T.Buffer((T.int64(13500000),), data=T_batch_matmul_NT)
    with T.launch_thread("blockIdx.x", 13184) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if blockIdx_x * 1024 + threadIdx_x < 13500000:
            T_batch_matmul_NT_1[blockIdx_x * 1024 + threadIdx_x] = T.float32(0.0)
        for ax3 in range(64):
            if blockIdx_x * 1024 + threadIdx_x < 13500000:
                lv25_1 = T.Buffer((T.int64(576000),), data=lv25.data)
                lv26_1 = T.Buffer((T.int64(576000),), data=lv26.data)
                T_batch_matmul_NT_1[blockIdx_x * 1024 + threadIdx_x] = T_batch_matmul_NT_1[blockIdx_x * 1024 + threadIdx_x] + lv25_1[(blockIdx_x * 1024 + threadIdx_x) % 2250000 // 1500 * 384 + (blockIdx_x * 1024 + threadIdx_x) // 2250000 * 64 + ax3] * lv26_1[(blockIdx_x * 1024 + threadIdx_x) % 1500 * 384 + (blockIdx_x * 1024 + threadIdx_x) // 2250000 * 64 + ax3]
    T_softmax_maxelem_1 = T.Buffer((T.int64(9000),), data=T_softmax_maxelem)
    with T.launch_thread("blockIdx.x", 9) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if blockIdx_x * 1024 + threadIdx_x < 9000:
            T_softmax_maxelem_1[blockIdx_x * 1024 + threadIdx_x] = T.float32(-340282346638528859811704183484516925440.0)
        for ax2 in range(1500):
            if blockIdx_x * 1024 + threadIdx_x < 9000:
                T_softmax_maxelem_1[blockIdx_x * 1024 + threadIdx_x] = T.max(T_softmax_maxelem_1[blockIdx_x * 1024 + threadIdx_x], T_batch_matmul_NT_1[(blockIdx_x * 1024 + threadIdx_x) * 1500 + ax2] / T.sqrt(T.float32(64.0)))
    T_softmax_expsum_1 = T.Buffer((T.int64(9000),), data=T_softmax_expsum)
    with T.launch_thread("blockIdx.x", 9) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if blockIdx_x * 1024 + threadIdx_x < 9000:
            T_softmax_expsum_1[blockIdx_x * 1024 + threadIdx_x] = T.float32(0.0)
        for ax2 in range(1500):
            if blockIdx_x * 1024 + threadIdx_x < 9000:
                T_softmax_expsum_1[blockIdx_x * 1024 + threadIdx_x] = T_softmax_expsum_1[blockIdx_x * 1024 + threadIdx_x] + T.exp(T_batch_matmul_NT_1[(blockIdx_x * 1024 + threadIdx_x) * 1500 + ax2] / T.sqrt(T.float32(64.0)) - T_softmax_maxelem_1[blockIdx_x * 1024 + threadIdx_x])
    T_batch_matmul_NN_1 = T.Buffer((T.int64(576000),), data=T_batch_matmul_NN)
    with T.launch_thread("blockIdx.x", 563) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if blockIdx_x * 1024 + threadIdx_x < 576000:
            T_batch_matmul_NN_1[blockIdx_x * 1024 + threadIdx_x] = T.float32(0.0)
        for ax3 in range(1500):
            if blockIdx_x * 1024 + threadIdx_x < 576000:
                lv27_1 = T.Buffer((T.int64(576000),), data=lv27.data)
                T_batch_matmul_NN_1[blockIdx_x * 1024 + threadIdx_x] = T_batch_matmul_NN_1[blockIdx_x * 1024 + threadIdx_x] + T.exp(T_batch_matmul_NT_1[blockIdx_x * 24000 + threadIdx_x // 64 * 1500 + ax3] / T.sqrt(T.float32(64.0)) - T_softmax_maxelem_1[blockIdx_x * 16 + threadIdx_x // 64]) / T_softmax_expsum_1[blockIdx_x * 16 + threadIdx_x // 64] * lv27_1[ax3 * 384 + (blockIdx_x * 1024 + threadIdx_x) // 96000 * 64 + threadIdx_x % 64]
    blockIdx_x = T.launch_thread("blockIdx.x", 563)
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if blockIdx_x * 1024 + threadIdx_x < 576000:
        T_transpose_1 = T.Buffer((T.int64(576000),), data=T_transpose.data)
        T_transpose_1[(blockIdx_x * 1024 + threadIdx_x) // 384 * 384 + (blockIdx_x * 256 + threadIdx_x) % 384 // 64 * 64 + threadIdx_x % 64] = T_batch_matmul_NN_1[(blockIdx_x * 256 + threadIdx_x) % 384 // 64 * 96000 + (blockIdx_x * 1024 + threadIdx_x) // 384 * 64 + threadIdx_x % 64]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_conv1d1_add1_gelu1(lv3: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32"), p_encoder_conv2_weight: T.Buffer((T.int64(384), T.int64(384), T.int64(3)), "float32"), lv5: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 6)
    blockIdx_x = T.launch_thread("blockIdx.x", 47)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 8)
    var = T.int64()
    T.attr(var, "pragma_auto_unroll_max_step", 256)
    T.attr(var, "pragma_unroll_explicit", 1)
    for var in range(T.int64(1)):
        conv1d_ncw_intermediate_reindex_pad_local = T.allocate([16], "float32", "local")
        conv1d_ncw_intermediate_reindex_pad_local_1 = T.Buffer((T.int64(16),), data=conv1d_ncw_intermediate_reindex_pad_local, scope="local")
        for ax1_3_init, ax2_3_0_init in T.grid(4, 2):
            conv1d_ncw_intermediate_reindex_pad_local_1[ax1_3_init * 4 + ax2_3_0_init * 2:ax1_3_init * 4 + ax2_3_0_init * 2 + 2] = T.Broadcast(T.float32(0.0), 2)
        for ax3_0 in range(72):
            pad_temp_reindex_pad_shared = T.allocate([576], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            p_encoder_conv2_weight_reindex_shared = T.allocate([1152], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            pad_temp_reindex_pad_shared_1 = T.Buffer((T.int64(576),), data=pad_temp_reindex_pad_shared, scope="shared")
            for ax0_ax1_ax2_fused_2, ax0_ax1_ax2_fused_3_s in T.grid(2, 2):
                lv3_1 = T.Buffer((T.int64(1152000),), data=lv3.data)
                pad_temp_reindex_pad_shared_1[threadIdx_y * 36 + threadIdx_x // 4 * 18 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16] = T.if_then_else(blockIdx_x * 16 + threadIdx_y < 750 and 1 <= blockIdx_x * 64 + threadIdx_y * 4 + threadIdx_x // 4 * 2 + (ax3_0 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) % 3 and blockIdx_x * 64 + threadIdx_y * 4 + threadIdx_x // 4 * 2 + (ax3_0 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) % 3 < 3001, lv3_1[(ax3_0 * 16 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) // 3 * 3000 + blockIdx_x * 64 + threadIdx_y * 4 + threadIdx_x // 4 * 2 + (ax3_0 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) % 3 - 1], T.float32(0.0))
            p_encoder_conv2_weight_reindex_shared_1 = T.Buffer((T.int64(1152),), data=p_encoder_conv2_weight_reindex_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(4):
                p_encoder_conv2_weight_1 = T.Buffer((T.int64(442368),), data=p_encoder_conv2_weight.data)
                p_encoder_conv2_weight_reindex_shared_1[T.Broadcast(threadIdx_y * 72 + threadIdx_x // 2 * 18, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = p_encoder_conv2_weight_1[T.Broadcast(blockIdx_y * 73728 + threadIdx_y * 4608 + threadIdx_x // 2 * 1152 + ax3_0 * 16, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)]
            for ax3_1, ax1_3, ax2_3_0 in T.grid(16, 4, 2):
                conv1d_ncw_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] = conv1d_ncw_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] + T.Broadcast(pad_temp_reindex_pad_shared_1[threadIdx_x * 72 + ax1_3 * 18 + ax3_1], 2) * p_encoder_conv2_weight_reindex_shared_1[threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1:threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1 + 36:18]
        for ax1, ax2_0 in T.grid(4, 2):
            if blockIdx_x * 8 + threadIdx_x < 375:
                T_multiply_intermediate_1 = T.Buffer((T.int64(576000),), data=T_multiply_intermediate.data)
                lv5_1 = T.Buffer((T.int64(384),), data=lv5.data)
                for ax2_1_s in range(2):
                    T_multiply_intermediate_1[blockIdx_y * 96000 + threadIdx_y * 6000 + ax2_0 * 3000 + ax2_1_s * 1500 + (blockIdx_x * 8 + threadIdx_x) * 4 + ax1] = (conv1d_ncw_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2 + ax2_1_s] + lv5_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + ax2_1_s]) * (T.float32(0.5) + T.erf((conv1d_ncw_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2 + ax2_1_s] + lv5_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + ax2_1_s]) * T.float32(0.70710678118654757)) * T.float32(0.5))
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul3_add7(lv162: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), lv163: T.Buffer((T.int64(384), T.int64(384)), "float32"), p_decoder_layers_0_self_attn_q_proj_bias: T.Buffer((T.int64(384),), "float32"), T_add_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 24)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 16)
    matmul_local = T.allocate([1], "float32", "local")
    matmul_rf_local = T.allocate([1], "float32", "local")
    matmul_intermediate_rf_local = T.Buffer((T.int64(1),), data=matmul_rf_local, scope="local")
    matmul_intermediate_rf_local[0] = T.float32(0.0)
    for ax1_fused_0 in range(24):
        lv162_1 = T.Buffer((T.int64(384),), data=lv162.data)
        lv163_1 = T.Buffer((T.int64(147456),), data=lv163.data)
        matmul_intermediate_rf_local[0] = matmul_intermediate_rf_local[0] + lv162_1[ax1_fused_0 * 16 + threadIdx_y] * lv163_1[ax1_fused_0 * 6144 + threadIdx_y * 384 + blockIdx_x * 16 + threadIdx_x]
    matmul_intermediate_local = T.Buffer((T.int64(1),), data=matmul_local, scope="local")
    with T.allocate([1], "float32", "local") as cross_thread_matmul_intermediate_local:
        cross_thread_matmul_intermediate_local_1 = T.Buffer((1,), data=cross_thread_matmul_intermediate_local, scope="local")
        with T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), matmul_intermediate_rf_local[0], T.bool(True), cross_thread_matmul_intermediate_local_1[0], threadIdx_y)
        matmul_intermediate_local[0] = cross_thread_matmul_intermediate_local_1[0]
    if threadIdx_y == 0:
        T_add_intermediate_1 = T.Buffer((T.int64(384),), data=T_add_intermediate.data)
        p_decoder_layers_0_self_attn_q_proj_bias_1 = T.Buffer((T.int64(384),), data=p_decoder_layers_0_self_attn_q_proj_bias.data)
        T_add_intermediate_1[blockIdx_x * 16 + threadIdx_x] = matmul_intermediate_local[0] + p_decoder_layers_0_self_attn_q_proj_bias_1[blockIdx_x * 16 + threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def matmul3(lv162: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), lv163: T.Buffer((T.int64(384), T.int64(384)), "float32"), matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"op_pattern": 4, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 24)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 16)
    matmul_rf_local = T.allocate([1], "float32", "local")
    matmul_rf_local_1 = T.Buffer((T.int64(1),), data=matmul_rf_local, scope="local")
    matmul_rf_local_1[0] = T.float32(0.0)
    for ax1_fused_0 in range(24):
        lv162_1 = T.Buffer((T.int64(384),), data=lv162.data)
        lv163_1 = T.Buffer((T.int64(147456),), data=lv163.data)
        matmul_rf_local_1[0] = matmul_rf_local_1[0] + lv162_1[ax1_fused_0 * 16 + threadIdx_y] * lv163_1[ax1_fused_0 * 6144 + threadIdx_y * 384 + blockIdx_x * 16 + threadIdx_x]
    cross_thread_matmul = T.allocate([1], "float32", "local")
    cross_thread_matmul_1 = T.Buffer((1,), data=cross_thread_matmul, scope="local")
    with T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
        T.tvm_thread_allreduce(T.uint32(1), matmul_rf_local_1[0], T.bool(True), cross_thread_matmul_1[0], threadIdx_y)
    if threadIdx_y == 0:
        matmul_1 = T.Buffer((T.int64(384),), data=matmul.data)
        matmul_1[blockIdx_x * 16 + threadIdx_x] = cross_thread_matmul_1[0]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_conv1d_add_gelu(input_features: T.Buffer((T.int64(1), T.int64(80), T.int64(3000)), "float32"), p_encoder_conv1_weight: T.Buffer((T.int64(384), T.int64(80), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(384), T.int64(3000)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_y = T.launch_thread("blockIdx.y", 6)
    blockIdx_x = T.launch_thread("blockIdx.x", 94)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 8)
    var = T.int64()
    T.attr(var, "pragma_auto_unroll_max_step", 256)
    T.attr(var, "pragma_unroll_explicit", 1)
    for var in range(T.int64(1)):
        conv1d_ncw_intermediate_reindex_pad_local = T.allocate([16], "float32", "local")
        conv1d_ncw_intermediate_reindex_pad_local_1 = T.Buffer((T.int64(16),), data=conv1d_ncw_intermediate_reindex_pad_local, scope="local")
        for ax1_3_init, ax2_3_0_init in T.grid(4, 2):
            conv1d_ncw_intermediate_reindex_pad_local_1[ax1_3_init * 4 + ax2_3_0_init * 2:ax1_3_init * 4 + ax2_3_0_init * 2 + 2] = T.Broadcast(T.float32(0.0), 2)
        for ax3_0 in range(15):
            pad_temp_reindex_pad_shared = T.allocate([576], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            p_encoder_conv1_weight_reindex_shared = T.allocate([1152], "float32", "shared", annotations={"buffer_dim_align": [[-1, 1, 8, 2]]})
            pad_temp_reindex_pad_shared_1 = T.Buffer((T.int64(576),), data=pad_temp_reindex_pad_shared, scope="shared")
            for ax0_ax1_ax2_fused_2, ax0_ax1_ax2_fused_3_s in T.grid(2, 2):
                input_features_1 = T.Buffer((T.int64(240000),), data=input_features.data)
                pad_temp_reindex_pad_shared_1[threadIdx_y * 36 + threadIdx_x // 4 * 18 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16] = T.if_then_else(blockIdx_x * 16 + threadIdx_y < 1500 and 1 <= blockIdx_x * 32 + threadIdx_y * 2 + threadIdx_x // 4 + (ax3_0 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) % 3 and blockIdx_x * 32 + threadIdx_y * 2 + threadIdx_x // 4 + (ax3_0 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) % 3 < 3001, input_features_1[(ax3_0 * 16 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) // 3 * 3000 + blockIdx_x * 32 + threadIdx_y * 2 + threadIdx_x // 4 + (ax3_0 + (threadIdx_x * 4 + ax0_ax1_ax2_fused_2 * 2 + ax0_ax1_ax2_fused_3_s) % 16) % 3 - 1], T.float32(0.0))
            p_encoder_conv1_weight_reindex_shared_1 = T.Buffer((T.int64(1152),), data=p_encoder_conv1_weight_reindex_shared, scope="shared")
            for ax0_ax1_ax2_fused_2 in range(4):
                p_encoder_conv1_weight_1 = T.Buffer((T.int64(92160),), data=p_encoder_conv1_weight.data)
                p_encoder_conv1_weight_reindex_shared_1[T.Broadcast(threadIdx_y * 72 + threadIdx_x // 2 * 18, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)] = p_encoder_conv1_weight_1[T.Broadcast(blockIdx_y * 15360 + threadIdx_y * 960 + threadIdx_x // 2 * 240 + ax3_0 * 16, 2) + T.Ramp(threadIdx_x * 8 + ax0_ax1_ax2_fused_2 * 2, 1, 2) % T.Broadcast(16, 2)]
            for ax3_1, ax1_3, ax2_3_0 in T.grid(16, 4, 2):
                conv1d_ncw_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] = conv1d_ncw_intermediate_reindex_pad_local_1[ax1_3 * 4 + ax2_3_0 * 2:ax1_3 * 4 + ax2_3_0 * 2 + 2] + T.Broadcast(pad_temp_reindex_pad_shared_1[threadIdx_x * 72 + ax1_3 * 18 + ax3_1], 2) * p_encoder_conv1_weight_reindex_shared_1[threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1:threadIdx_y * 72 + ax2_3_0 * 36 + ax3_1 + 36:18]
        for ax1, ax2_0 in T.grid(4, 2):
            if blockIdx_x * 8 + threadIdx_x < 750:
                T_multiply_intermediate_1 = T.Buffer((T.int64(1152000),), data=T_multiply_intermediate.data)
                lv1_1 = T.Buffer((T.int64(384),), data=lv1.data)
                for ax2_1_s in range(2):
                    T_multiply_intermediate_1[blockIdx_y * 192000 + threadIdx_y * 12000 + ax2_0 * 6000 + ax2_1_s * 3000 + (blockIdx_x * 8 + threadIdx_x) * 4 + ax1] = (conv1d_ncw_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2 + ax2_1_s] + lv1_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + ax2_1_s]) * (T.float32(0.5) + T.erf((conv1d_ncw_intermediate_reindex_pad_local_1[ax1 * 4 + ax2_0 * 2 + ax2_1_s] + lv1_1[blockIdx_y * 64 + threadIdx_y * 4 + ax2_0 * 2 + ax2_1_s]) * T.float32(0.70710678118654757)) * T.float32(0.5))
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_transpose2_transpose3_reshape2(lv28: T.Buffer((T.int64(1), T.int64(1500), T.int64(6), T.int64(64)), "float32"), T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 563)
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if blockIdx_x * 1024 + threadIdx_x < 576000:
        T_reshape_intermediate_1 = T.Buffer((T.int64(576000),), data=T_reshape_intermediate.data)
        lv28_1 = T.Buffer((T.int64(576000),), data=lv28.data)
        T_reshape_intermediate_1[blockIdx_x * 1024 + threadIdx_x] = lv28_1[(blockIdx_x * 1024 + threadIdx_x) // 384 * 384 + (blockIdx_x * 256 + threadIdx_x) % 384 // 64 * 64 + threadIdx_x % 64]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_matmul4_add8_gelu3(lv214: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), lv215: T.Buffer((T.int64(384), T.int64(1536)), "float32"), p_decoder_layers_0_fc1_bias: T.Buffer((T.int64(1536),), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(1536)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 96)
    threadIdx_y = T.launch_thread("threadIdx.y", 16)
    threadIdx_x = T.launch_thread("threadIdx.x", 16)
    matmul_local = T.allocate([1], "float32", "local")
    matmul_rf_local = T.allocate([1], "float32", "local")
    matmul_intermediate_rf_local = T.Buffer((T.int64(1),), data=matmul_rf_local, scope="local")
    matmul_intermediate_rf_local[0] = T.float32(0.0)
    for ax1_fused_0 in range(24):
        lv214_1 = T.Buffer((T.int64(384),), data=lv214.data)
        lv215_1 = T.Buffer((T.int64(589824),), data=lv215.data)
        matmul_intermediate_rf_local[0] = matmul_intermediate_rf_local[0] + lv214_1[ax1_fused_0 * 16 + threadIdx_y] * lv215_1[ax1_fused_0 * 24576 + threadIdx_y * 1536 + blockIdx_x * 16 + threadIdx_x]
    matmul_intermediate_local = T.Buffer((T.int64(1),), data=matmul_local, scope="local")
    with T.allocate([1], "float32", "local") as cross_thread_matmul_intermediate_local:
        cross_thread_matmul_intermediate_local_1 = T.Buffer((1,), data=cross_thread_matmul_intermediate_local, scope="local")
        with T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0))):
            T.tvm_thread_allreduce(T.uint32(1), matmul_intermediate_rf_local[0], T.bool(True), cross_thread_matmul_intermediate_local_1[0], threadIdx_y)
        matmul_intermediate_local[0] = cross_thread_matmul_intermediate_local_1[0]
    if threadIdx_y == 0:
        T_multiply_intermediate_1 = T.Buffer((T.int64(1536),), data=T_multiply_intermediate.data)
        p_decoder_layers_0_fc1_bias_1 = T.Buffer((T.int64(1536),), data=p_decoder_layers_0_fc1_bias.data)
        T_multiply_intermediate_1[blockIdx_x * 16 + threadIdx_x] = (matmul_intermediate_local[0] + p_decoder_layers_0_fc1_bias_1[blockIdx_x * 16 + threadIdx_x]) * (T.float32(0.5) + T.erf((matmul_intermediate_local[0] + p_decoder_layers_0_fc1_bias_1[blockIdx_x * 16 + threadIdx_x]) * T.float32(0.70710678118654757)) * T.float32(0.5))
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_transpose6_transpose7_reshape7(lv180: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), T_reshape_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if threadIdx_x < 384:
        T_reshape_intermediate_1 = T.Buffer((T.int64(384),), data=T_reshape_intermediate.data)
        lv180_1 = T.Buffer((T.int64(384),), data=lv180.data)
        T_reshape_intermediate_1[threadIdx_x] = lv180_1[threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_reshape6_transpose6_transpose7(lv165: T.Buffer((T.int64(1), T.int64(1), T.int64(384)), "float32"), T_transpose_intermediate_1: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if threadIdx_x < 384:
        T_transpose_intermediate_1_1 = T.Buffer((T.int64(384),), data=T_transpose_intermediate_1.data)
        lv165_1 = T.Buffer((T.int64(384),), data=lv165.data)
        T_transpose_intermediate_1_1[threadIdx_x] = lv165_1[threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def fused_transpose_add2(lv7: T.Buffer((T.int64(1), T.int64(384), T.int64(1500)), "float32"), p_encoder_embed_positions_weight: T.Buffer((T.int64(1500), T.int64(384)), "float32"), T_add_intermediate: T.Buffer((T.int64(1), T.int64(1500), T.int64(384)), "float32")):
    T.func_attr({"target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    blockIdx_x = T.launch_thread("blockIdx.x", 563)
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if blockIdx_x * 1024 + threadIdx_x < 576000:
        T_add_intermediate_1 = T.Buffer((T.int64(576000),), data=T_add_intermediate.data)
        lv7_1 = T.Buffer((T.int64(576000),), data=lv7.data)
        p_encoder_embed_positions_weight_1 = T.Buffer((T.int64(576000),), data=p_encoder_embed_positions_weight.data)
        T_add_intermediate_1[blockIdx_x * 1024 + threadIdx_x] = lv7_1[(blockIdx_x * 256 + threadIdx_x) % 384 * 1500 + (blockIdx_x * 1024 + threadIdx_x) // 384] + p_encoder_embed_positions_weight_1[blockIdx_x * 1024 + threadIdx_x]
[16:57:20] /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1733: # from tvm.script import tir as T

@T.prim_func
def attention1(lv177: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), lv178: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), lv179: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1), T.int64(6), T.int64(64)), "float32")):
    T.func_attr({"op_pattern": 4, "target": T.target({"arch": "sm_89", "host": {"keys": ["cpu"], "kind": "llvm", "mtriple": "x86_64-pc-linux-gnu", "tag": ""}, "keys": ["cuda", "gpu"], "kind": "cuda", "max_num_threads": 1024, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "tag": "", "thread_warp_size": 32}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
    T_batch_matmul_NT = T.allocate([6], "float32", "global")
    T_softmax_maxelem = T.allocate([6], "float32", "global")
    T_softmax_expsum = T.allocate([6], "float32", "global")
    T_batch_matmul_NT_1 = T.Buffer((T.int64(6),), data=T_batch_matmul_NT)
    with T.launch_thread("blockIdx.x", T.int64(1)) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if threadIdx_x < 6:
            T_batch_matmul_NT_1[threadIdx_x] = T.float32(0.0)
        for ax1 in range(64):
            if threadIdx_x < 6:
                lv177_1 = T.Buffer((T.int64(384),), data=lv177.data)
                lv178_1 = T.Buffer((T.int64(384),), data=lv178.data)
                T_batch_matmul_NT_1[threadIdx_x] = T_batch_matmul_NT_1[threadIdx_x] + lv177_1[threadIdx_x * 64 + ax1] * lv178_1[threadIdx_x * 64 + ax1]
    T_softmax_maxelem_1 = T.Buffer((T.int64(6),), data=T_softmax_maxelem)
    with T.launch_thread("blockIdx.x", T.int64(1)) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if threadIdx_x < 6:
            T_softmax_maxelem_1[threadIdx_x] = T.float32(-340282346638528859811704183484516925440.0)
            T_softmax_maxelem_1[threadIdx_x] = T.max(T_softmax_maxelem_1[threadIdx_x], T_batch_matmul_NT_1[threadIdx_x] / T.sqrt(T.float32(64.0)))
    T_softmax_expsum_1 = T.Buffer((T.int64(6),), data=T_softmax_expsum)
    with T.launch_thread("blockIdx.x", T.int64(1)) as blockIdx_x:
        threadIdx_x = T.launch_thread("threadIdx.x", 1024)
        if threadIdx_x < 6:
            T_softmax_expsum_1[threadIdx_x] = T.float32(0.0)
            T_softmax_expsum_1[threadIdx_x] = T_softmax_expsum_1[threadIdx_x] + T.exp(T_batch_matmul_NT_1[threadIdx_x] / T.sqrt(T.float32(64.0)) - T_softmax_maxelem_1[threadIdx_x])
    blockIdx_x = T.launch_thread("blockIdx.x", T.int64(1))
    threadIdx_x = T.launch_thread("threadIdx.x", 1024)
    if threadIdx_x < 384:
        T_transpose_1 = T.Buffer((T.int64(384),), data=T_transpose.data)
        T_transpose_1[threadIdx_x] = T.float32(0.0)
        T_batch_matmul_NN = T.Buffer((T.int64(384),))
        lv179_1 = T.Buffer((T.int64(384),), data=lv179.data)
        T_transpose_1[threadIdx_x] = T_batch_matmul_NN[threadIdx_x] + T.exp(T_batch_matmul_NT_1[threadIdx_x // 64] / T.sqrt(T.float32(64.0)) - T_softmax_maxelem_1[threadIdx_x // 64]) / T_softmax_expsum_1[threadIdx_x // 64] * lv179_1[threadIdx_x]
!!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function view.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 4
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function contiguous.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function scaled_dot_product_attention.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function transpose.int !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
found function reshape.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, list) of length 3
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function gelu.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function linear.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 3
isinstance(node, fx.Node)
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function dropout.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
found function add.Tensor !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 2
isinstance(node, fx.Node)
isinstance(node, fx.Node)
found function layer_norm.default !!!!!!!!!!!!!!!!!!!!!!!!!!!!
isinstance(node, tuple) of length 1
isinstance(node, tuple) of length 1
isinstance(node, fx.Node)
Traceback (most recent call last):
  File "/ssd1/htalendr/whisper/whisper_inlined.py", line 2230, in <module>
    test_export_and_cuda(raw_data, torch_model, show=False)
  File "/ssd1/htalendr/hl_utils/hlutils/test_export_and_cuda.py", line 40, in test_export_and_cuda
    ex = relax.build(tvm_mod, target=target, relax_pipeline=relax.get_default_pipeline(target))
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py", line 259, in build
    return _vmlink(
           ^^^^^^^^
  File "/ssd1/htalendr/tvm/python/tvm/relax/vm_build.py", line 154, in _vmlink
    lib = tvm.tir.build(tir_mod, target=target, pipeline=tir_pipeline)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/python/tvm/tir/build.py", line 173, in build
    mod = pipeline(mod)
          ^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/python/tvm/ir/transform.py", line 238, in __call__
    return _ffi_transform_api.RunPass(self, mod)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "tvm/_ffi/_cython/./packed_func.pxi", line 339, in tvm._ffi._cy3.core.PackedFuncBase.__call__
  File "tvm/_ffi/_cython/./packed_func.pxi", line 270, in tvm._ffi._cy3.core.FuncCall
  File "tvm/_ffi/_cython/./packed_func.pxi", line 259, in tvm._ffi._cy3.core.FuncCall3
  File "tvm/_ffi/_cython/./base.pxi", line 185, in tvm._ffi._cy3.core.CHECK_CALL
  File "/ssd1/htalendr/tvm/python/tvm/_ffi/base.py", line 468, in raise_last_ffi_error
    raise py_err
  File "tvm/_ffi/_cython/./packed_func.pxi", line 56, in tvm._ffi._cy3.core.tvm_callback
  File "/ssd1/htalendr/tvm/python/tvm/tir/pipeline.py", line 122, in _pipeline
    mod = tvm.ir.transform.Sequential(passes)(mod)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/python/tvm/ir/transform.py", line 238, in __call__
    return _ffi_transform_api.RunPass(self, mod)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "tvm/_ffi/_cython/./packed_func.pxi", line 339, in tvm._ffi._cy3.core.PackedFuncBase.__call__
  File "tvm/_ffi/_cython/./packed_func.pxi", line 270, in tvm._ffi._cy3.core.FuncCall
  File "tvm/_ffi/_cython/./packed_func.pxi", line 259, in tvm._ffi._cy3.core.FuncCall3
  File "tvm/_ffi/_cython/./base.pxi", line 185, in tvm._ffi._cy3.core.CHECK_CALL
  File "/ssd1/htalendr/tvm/src/tir/ir/transform.cc", line 121, in tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
    func = pass_func(std::move(func), mod, pass_ctx);
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc", line 1760, in operator()
    false);
^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc", line 1717, in tvm::tir::PointerValueTypeRewrite(tvm::tir::PrimFunc, bool, bool, bool, bool, bool, bool, bool, bool)
    checker(f->body);
                    ^^
  File "/ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc", line 1202, in tvm::tir::VectorTypeAccessChecker::VisitStmt_(tvm::tir::AllocateNode const*)
    StmtExprVisitor::VisitStmt_(op);
                  ^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 58, in tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::AllocateNode const*)
    this->VisitStmt(op->body);
                    ^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc", line 1202, in tvm::tir::VectorTypeAccessChecker::VisitStmt_(tvm::tir::AllocateNode const*)
    StmtExprVisitor::VisitStmt_(op);
                  ^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 58, in tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::AllocateNode const*)
    this->VisitStmt(op->body);
                    ^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc", line 1202, in tvm::tir::VectorTypeAccessChecker::VisitStmt_(tvm::tir::AllocateNode const*)
    StmtExprVisitor::VisitStmt_(op);
                  ^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 58, in tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::AllocateNode const*)
    this->VisitStmt(op->body);
                    ^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 119, in tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::SeqStmtNode const*)
    VisitArray(op->seq, [this](const Stmt& s) { this->VisitStmt(s); });
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/functor_common.h", line 35, in VisitArray<tvm::tir::Stmt, tvm::tir::StmtVisitor::VisitStmt_(const tvm::tir::SeqStmtNode*)::<lambda(const tvm::tir::Stmt&)> >
    fvisit(arr[i]);
                  ^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 119, in operator()
    VisitArray(op->seq, [this](const Stmt& s) { this->VisitStmt(s); });
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 85, in tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::IfThenElseNode const*)
    this->VisitStmt(op->then_case);
                    ^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 119, in tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::SeqStmtNode const*)
    VisitArray(op->seq, [this](const Stmt& s) { this->VisitStmt(s); });
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/functor_common.h", line 35, in VisitArray<tvm::tir::Stmt, tvm::tir::StmtVisitor::VisitStmt_(const tvm::tir::SeqStmtNode*)::<lambda(const tvm::tir::Stmt&)> >
    fvisit(arr[i]);
                  ^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 119, in operator()
    VisitArray(op->seq, [this](const Stmt& s) { this->VisitStmt(s); });
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc", line 70, in tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::BufferStoreNode const*)
    this->VisitExpr(op->value);
                    ^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/ir/expr_functor.cc", line 58, in tvm::tir::ExprVisitor::VisitExpr_(tvm::tir::AddNode const*)
    DEFINE_BINOP_VISIT_(AddNode);
                      ^^^^^^^^^^^^
  File "/ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc", line 1298, in tvm::tir::VectorTypeAccessChecker::OnArrayAccess(tvm::runtime::DataType, tvm::tir::VarNode const*, tvm::runtime::Array<tvm::PrimExpr, void> const&, bool)
    ICHECK(indices[i].dtype().is_scalar())
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
tvm.error.InternalError: Traceback (most recent call last):
  20: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const
        at /ssd1/htalendr/tvm/src/tir/ir/transform.cc:121
  19: operator()
        at /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1760
  18: tvm::tir::PointerValueTypeRewrite(tvm::tir::PrimFunc, bool, bool, bool, bool, bool, bool, bool, bool)
        at /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1717
  17: tvm::tir::VectorTypeAccessChecker::VisitStmt_(tvm::tir::AllocateNode const*)
        at /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1202
  16: tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::AllocateNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:58
  15: tvm::tir::VectorTypeAccessChecker::VisitStmt_(tvm::tir::AllocateNode const*)
        at /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1202
  14: tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::AllocateNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:58
  13: tvm::tir::VectorTypeAccessChecker::VisitStmt_(tvm::tir::AllocateNode const*)
        at /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1202
  12: tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::AllocateNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:58
  11: tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::SeqStmtNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:119
  10: VisitArray<tvm::tir::Stmt, tvm::tir::StmtVisitor::VisitStmt_(const tvm::tir::SeqStmtNode*)::<lambda(const tvm::tir::Stmt&)> >
        at /ssd1/htalendr/tvm/src/tir/ir/functor_common.h:35
  9: operator()
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:119
  8: tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::IfThenElseNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:85
  7: tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::SeqStmtNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:119
  6: VisitArray<tvm::tir::Stmt, tvm::tir::StmtVisitor::VisitStmt_(const tvm::tir::SeqStmtNode*)::<lambda(const tvm::tir::Stmt&)> >
        at /ssd1/htalendr/tvm/src/tir/ir/functor_common.h:35
  5: operator()
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:119
  4: tvm::tir::StmtVisitor::VisitStmt_(tvm::tir::BufferStoreNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/stmt_functor.cc:70
  3: tvm::tir::ExprVisitor::VisitExpr_(tvm::tir::AddNode const*)
        at /ssd1/htalendr/tvm/src/tir/ir/expr_functor.cc:58
  2: non-virtual thunk to tvm::tir::StmtExprVisitor::VisitExpr(tvm::PrimExpr const&)
  1: _ZThn8_N3tvm3tir23VectorTypeAccessChecker10VisitExpr_EPKNS0_14Buf
  0: tvm::tir::VectorTypeAccessChecker::OnArrayAccess(tvm::runtime::DataType, tvm::tir::VarNode const*, tvm::runtime::Array<tvm::PrimExpr, void> const&, bool)
        at /ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc:1298
  File "/ssd1/htalendr/tvm/src/tir/transforms/storage_rewrite.cc", line 1276
InternalError: Check failed: (it != info_map_.end()) is false: Load/Store of buffer T_batch_matmul_NN (0x5591ad2c4fd0) occurred before its declaration.
